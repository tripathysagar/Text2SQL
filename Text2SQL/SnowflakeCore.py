"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_SnowflakeCore.ipynb.

# %% auto 0
__all__ = ['model_name', 'UNSAFE_QUERIES', 'FIELD_TO_FILTER', 'connect_to_snowflake', 'QueryResult', 'SnowflakeAgent',
           'TableAttr', 'ParentSchema', 'get_fk', 'SnowMetadata']

# %% ../nbs/00_SnowflakeCore.ipynb 4
from dotenv import load_dotenv
import os
import json
import pandas as pd
from tqdm import tqdm
from fastcore.utils import *
import regex as re
from lisette import *

# Load environment variables from the .env file
load_dotenv()

assert os.getenv("SPIDER2_SNOWFLAKE_USERNAME")
assert os.getenv("SPIDER2_SNOWFLAKE_PASSWORD")
assert os.getenv("SPIDER2_SNOWFLAKE_ACCOUNT")
assert os.getenv("COMPUTE_WH_PARTICIPANT")

# %% ../nbs/00_SnowflakeCore.ipynb 6
assert os.getenv("LM_STUDIO_API_BASE")
assert os.getenv("LM_STUDIO_MODEL_NAME")
model_name = os.getenv("LM_STUDIO_MODEL_NAME")
model_name

# %% ../nbs/00_SnowflakeCore.ipynb 7
import litellm

litellm.register_model(
    {
        model_name:{
        "max_tokens": 8192, # put the model’s real context limit
        "input_cost_per_token": 0.0,
        "output_cost_per_token": 0.0,
        "supports_assistant_prefill": False
    }})

# %% ../nbs/00_SnowflakeCore.ipynb 10
import snowflake.connector

def connect_to_snowflake():
    conn = snowflake.connector.connect(
        user=os.getenv("SPIDER2_SNOWFLAKE_USERNAME"),
        password=os.getenv("SPIDER2_SNOWFLAKE_PASSWORD"),
        account=os.getenv("SPIDER2_SNOWFLAKE_ACCOUNT"),
        warehouse=os.getenv("COMPUTE_WH_PARTICIPANT"),
        session_parameters={
            "STATEMENT_TIMEOUT_IN_SECONDS": 300,   # 5 min timeout
            "QUERY_TAG": "Text2SQL",              # optional tag for tracing
        }
    )
    cursor = conn.cursor()
    assert not cursor.is_closed()

    cursor.arraysize = 10_000   # default is usually 1

    return conn, cursor

# %% ../nbs/00_SnowflakeCore.ipynb 26
from pydantic import BaseModel
from typing import Optional

class QueryResult(BaseModel):
    query: str
    success: bool
    data: Optional[list[dict]] = None
    error: Optional[str] = None
    row_count: int = 0
    execution_time: float = 0.0

# %% ../nbs/00_SnowflakeCore.ipynb 27
import time
UNSAFE_QUERIES = ['DROP', 'UPDATE', 'DELETE', 'INSERT', 'TRUNCATE', 'ALTER']

class SnowflakeAgent:
    def __init__(self):
        self.connect_to_snowflake = connect_to_snowflake

    def connect(self): 
        self.conn, self.cursor = self.connect_to_snowflake()
        self.check_connection

    @property
    def check_connection(self):
        try:
            return not self.cursor.is_closed()
        except Exception as e:
            return False
    
    def execute_query(
        self,                       # SnowflakeAgent instance for toolloop ignore this part
        query: str,                 # SQL query with to execute
        max_rows: int = 10,         # Maximum rows to fetch
        fetch_all: bool = False,    # Fetch all rows if True
    ) -> QueryResult:
        """
        Execute a SQL query and return results as a QueryResult object.
        
        This function provides safe SQL execution with protection against:
        - Multiple statements (via semicolon detection)
        - Unsafe operations (DROP, DELETE, UPDATE, etc.)
        - Memory issues (via row limiting)
        
        Args:
            query (str): The SQL query to execute. Must be a single SELECT statement.
            max_rows (int, optional): Maximum number of rows to fetch. Defaults to 10.
                Only applies when fetch_all=False.
            fetch_all (bool, optional): If True, fetch all rows regardless of max_rows.
                Defaults to False for safety.
        
        Returns:
            QueryResult: A Pydantic model containing:
                - query: The executed query
                - success: Whether execution succeeded
                - data: List of dictionaries (rows) if successful, None otherwise
                - error: Error message if failed, None otherwise
                - row_count: Number of rows returned
                - execution_time: Time taken to execute the query in seconds
        
        Raises:
            ValueError: If query contains multiple statements or unsafe operations.
            Exception: Any database errors are caught and returned in QueryResult.error
        
        Examples:
            >>> result = sql_df("SELECT * FROM users")
            >>> result.success
            True
            >>> result.row_count
            10
            
            >>> result = sql_df("SELECT * FROM users", fetch_all=True)
            >>> result.row_count
            1000
        """
        if not self.check_connection:
            self.connect()

        try:
            query = query.strip()

            # Check for multiple statements via semicolon detection
            if ';' in query:
                total_semicolons = query.count(';')
                safe_semicolons = len(re.findall(r';', ''.join(re.findall(r'"[^"]*"|\'[^\']*\'', query))))
                
                if total_semicolons != safe_semicolons:
                    raise ValueError("Multiple statements or unsafe semicolons detected!")

            # Prevent unsafe data modification queries
            if any([query.upper().startswith(i) for i in UNSAFE_QUERIES]):
                raise ValueError("Trying Data Update, Not allowed!!!")
            
            # Execute query and measure time
            start_time = time.time()
            self.cursor.execute(query)
            execution_time = time.time() - start_time
            
            # Fetch results
            column_names = [desc[0] for desc in self.cursor.description]
            if fetch_all:
                results = self.cursor.fetchall()
            else:
                results = self.cursor.fetchmany(max_rows)
                
            df = pd.DataFrame(results, columns=column_names)
            
            return QueryResult(
                query=query,
                success=True,
                data=df.to_dict('records'),
                error=None,
                row_count=len(results),
                execution_time=execution_time
            )
            
        except Exception as e:
            return QueryResult(
                query=query,
                success=False,
                data=None,
                error=str(e),
                row_count=0,
                execution_time=0.0
            )

# %% ../nbs/00_SnowflakeCore.ipynb 34
# indivisual table
class TableAttr(BaseModel):
    name: str
    column_names: Optional[list[dict]] = None
    sample_rows: Optional[list[dict]] = None
    row_count : int 

# complete 
class ParentSchema(BaseModel):
    dialect: str
    database: str
    Schema: str
    tables: list[TableAttr]
    relationships: Optional[list[dict]] = None  # For foreign key

# %% ../nbs/00_SnowflakeCore.ipynb 69
def get_fk(agent, schema: ParentSchema, model_name: str ="gemini/gemini-2.5-flash") -> ParentSchema:
    try:
        # Query for column information
        # before any heavy query
        agent.cursor.execute("ALTER SESSION SET STATEMENT_TIMEOUT_IN_SECONDS = 600")
        r = agent.execute_query(f"""
SELECT COLUMN_NAME, TABLE_NAME
FROM {schema.database}.INFORMATION_SCHEMA.COLUMNS 
WHERE TABLE_SCHEMA = '{schema.Schema}'
ORDER BY COLUMN_NAME, TABLE_NAME
""", fetch_all=True)
        
        if not r.success:
            raise Exception(f"Failed to query schema: {r.error}")
        
        df = pd.DataFrame(r.data)
        
        # Build schema summary
        schema_summary = []
        for table in schema.tables:
            cols = [col['name'] for col in table.column_names]
            schema_summary.append({
                "table": table.name,
                "columns": cols,
                "row_count": table.row_count,
                "sample": table.sample_rows[0] if table.sample_rows else {}
            })
        
        # Build prompt
        prompt = f"""Given this database schema, identify the foreign key relationships.

Schema: {json.dumps(schema_summary, indent=2)}

Foreign Keys relations db column name:
{json.dumps(df.groupby('COLUMN_NAME')['TABLE_NAME'].apply(list).to_dict(), indent=2)}

Return ONLY a JSON array of relationships in this exact format:
[
  {{
    "from_table": "FLIGHTS",
    "from_column": "aircraft_code",
    "to_table": "AIRCRAFTS_DATA",
    "to_column": "aircraft_code"
  }}
]

Rules:
- Only include relationships where a column in one table references a primary key in another
- Use row counts as hints (parent tables typically have fewer rows)
- Consider naming patterns (e.g., aircraft_code likely references AIRCRAFTS_DATA)
"""
        
        # Call LLM
        chat = Chat(model_name)
        resp = chat(prompt)
        content = resp.choices[0].message.content
        
        # Extract JSON more robustly
        # Try to find JSON array in the response
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if not json_match:
            raise ValueError("Could not find JSON array in LLM response")
        
        json_str = json_match.group(0)
        relationships = json.loads(json_str)
        
        # Validate it's a list
        if not isinstance(relationships, list):
            raise ValueError("Expected list of relationships from LLM")
        
        schema.relationships = relationships
        return schema
        
    except json.JSONDecodeError as e:
        raise Exception(f"Failed to parse LLM response as JSON: {e}")
    except Exception as e:
        raise Exception(f"Error inferring foreign keys: {e}")


# %% ../nbs/00_SnowflakeCore.ipynb 72
FIELD_TO_FILTER = ['name', 'type', 'null?', 'default', 'primary key', 'unique key', 'comment', ] #"check", 'expression']

class SnowMetadata:
    def __init__(self, agent, db_name: str, schema_name: str, row_limit: int = 1, model_name:str = "gemini/gemini-2.5-flash"):
        self.agent = agent
        self.db_name = db_name
        self.schema_name = schema_name
        self.row_limit = row_limit
        self.model_name = model_name
    
    def _get_all_tables_metadata(self):
        if self.row_limit <= 0: self.row_limit = 1 # if negative row count is provided

        result = self.agent.execute_query(f"SHOW TABLES IN {self.db_name}.{self.schema_name}", fetch_all=True)
        assert result.success, f"Not able to fetch the `SHOW TABLES IN {self.db_name}.{self.schema_name}`"

        if len(result.data) == 0 :
            raise ValueError("Empty schema")

        all_tables_df = pd.DataFrame(result.data)

        print("Reading Tables....")
        all_table_atrs = []

        for tn in tqdm(all_tables_df['name']):
            # get table info
            tn = f"{self.db_name}.{self.schema_name}.{tn}"
            result = self.agent.execute_query(f"DESCRIBE TABLE {tn}", fetch_all=True)
            assert result.success, f"Not able to fetch the `SHOW TABLES IN {tn}`"

            table_df = pd.DataFrame(result.data)

            # get column related data
            # this needs to be updated for the new db setup ie oracle and others
            column_names = []
            for _, row in table_df.iterrows():
                column_names.append({k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v  and v != 'N'})
            
            # fectch a single row of data from the table
            table_data_example = self.agent.execute_query(f"select * from {tn} limit {self.row_limit}", fetch_all=True)
            assert table_data_example.success

            # fectch a single row of data from the table
            row_count = self.agent.execute_query(f"select count(*)  as count from {tn}", fetch_all=True)
            assert row_count.success

            all_table_atrs.append(
                TableAttr(
                    name=tn,
                    column_names=column_names,
                    sample_rows=table_data_example.data,
                    row_count=next(iter(row_count.data[0].values())) # for ignoring the key if upper or lower type
                    ))

        self.schema_metadata =  ParentSchema(
            dialect="snowflake", # for now it is fixed to snowflake
            database=self.db_name,
            Schema=self.schema_name,
            tables=all_table_atrs
        )

    def _get_fk_metadata(self):
        print("Building up FKs....")
        new_s = get_fk(self.agent, self.schema_metadata, self.model_name)
        
        
    def get_metadata(self):
        self._get_all_tables_metadata()
        self._get_fk_metadata()
        return self.schema_metadata
