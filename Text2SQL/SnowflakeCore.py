"""Secure Snowflake database interface with automated schema extraction and LLM-based foreign key inference for Text-to-SQL systems."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_SnowflakeCore.ipynb.

# %% auto 0
__all__ = ['model_name', 'UNSAFE_QUERIES', 'FIELD_TO_FILTER', 'CHACHE_DIR', 'connect_to_snowflake', 'QueryResult',
           'SnowflakeAgent', 'TableAttr', 'ParentSchema', 'SnowMetadata']

# %% ../nbs/00_SnowflakeCore.ipynb 4
from dotenv import load_dotenv
import os
import json
import pandas as pd
from tqdm import tqdm
from fastcore.utils import *
from fastcore.all import *  
import regex as re
from lisette import *
from fastcore.all import *


# Load environment variables from the .env file
load_dotenv()

assert os.getenv("SPIDER2_SNOWFLAKE_USERNAME")
assert os.getenv("SPIDER2_SNOWFLAKE_PASSWORD")
assert os.getenv("SPIDER2_SNOWFLAKE_ACCOUNT")
assert os.getenv("COMPUTE_WH_PARTICIPANT")

# %% ../nbs/00_SnowflakeCore.ipynb 8
assert os.getenv("LM_STUDIO_API_BASE")
assert os.getenv("LM_STUDIO_MODEL_NAME")
model_name = os.getenv("LM_STUDIO_MODEL_NAME")
model_name

# %% ../nbs/00_SnowflakeCore.ipynb 9
import litellm

litellm.register_model(
    {
        model_name:{
        "max_tokens": 8192, # put the model’s real context limit
        "input_cost_per_token": 0.0,
        "output_cost_per_token": 0.0,
        "supports_assistant_prefill": False
    }})

# %% ../nbs/00_SnowflakeCore.ipynb 12
import snowflake.connector

def connect_to_snowflake():
    conn = snowflake.connector.connect(
        user=os.getenv("SPIDER2_SNOWFLAKE_USERNAME"),
        password=os.getenv("SPIDER2_SNOWFLAKE_PASSWORD"),
        account=os.getenv("SPIDER2_SNOWFLAKE_ACCOUNT"),
        warehouse=os.getenv("COMPUTE_WH_PARTICIPANT"),
        session_parameters={
            "STATEMENT_TIMEOUT_IN_SECONDS": 300,   # 5 min timeout
            "QUERY_TAG": "Text2SQL",              # optional tag for tracing
        }
    )
    cursor = conn.cursor()
    assert not cursor.is_closed()

    cursor.arraysize = 10_000   # default is usually 1

    return conn, cursor

# %% ../nbs/00_SnowflakeCore.ipynb 26
from pydantic import BaseModel
from typing import Optional

class QueryResult(BaseModel):
    query: str
    success: bool
    data: Optional[list[dict]] = None
    error: Optional[str] = None
    row_count: int = 0
    execution_time: float = 0.0

# %% ../nbs/00_SnowflakeCore.ipynb 27
import time
UNSAFE_QUERIES = ['DROP', 'UPDATE', 'DELETE', 'INSERT', 'TRUNCATE', 'ALTER']

class SnowflakeAgent:
    @property
    def dialect_name(self):
        return "snowflake"
        
    def __init__(self):
        self.connect_to_snowflake = connect_to_snowflake

    def connect(self): 
        self.conn, self.cursor = self.connect_to_snowflake()
        self.check_connection

    @property
    def check_connection(self):
        try:
            return not self.cursor.is_closed()
        except Exception as e:
            return False
    
    def execute_query(
        self,                       # SnowflakeAgent instance for toolloop ignore this part
        query: str,                 # SQL query with to execute
        max_rows: int = 10,         # Maximum rows to fetch
        fetch_all: bool = False,    # Fetch all rows if True
    ) -> QueryResult:
        """
        Execute a SQL query and return results as a QueryResult object.
        
        This function provides safe SQL execution with protection against:
        - Multiple statements (via semicolon detection)
        - Unsafe operations (DROP, DELETE, UPDATE, etc.)
        - Memory issues (via row limiting)
        
        Args:
            query (str): The SQL query to execute. Must be a single SELECT statement.
            max_rows (int, optional): Maximum number of rows to fetch. Defaults to 10.
                Only applies when fetch_all=False.
            fetch_all (bool, optional): If True, fetch all rows regardless of max_rows.
                Defaults to False for safety.
        
        Returns:
            QueryResult: A Pydantic model containing:
                - query: The executed query
                - success: Whether execution succeeded
                - data: List of dictionaries (rows) if successful, None otherwise
                - error: Error message if failed, None otherwise
                - row_count: Number of rows returned
                - execution_time: Time taken to execute the query in seconds
        
        Raises:
            ValueError: If query contains multiple statements or unsafe operations.
            Exception: Any database errors are caught and returned in QueryResult.error
        
        Examples:
            >>> result = sql_df("SELECT * FROM users")
            >>> result.success
            True
            >>> result.row_count
            10
            
            >>> result = sql_df("SELECT * FROM users", fetch_all=True)
            >>> result.row_count
            1000
        """
        if not self.check_connection:
            self.connect()

        try:
            query = query.strip()

            # Check for multiple statements via semicolon detection
            if ';' in query:
                total_semicolons = query.count(';')
                safe_semicolons = len(re.findall(r';', ''.join(re.findall(r'"[^"]*"|\'[^\']*\'', query))))
                
                if total_semicolons != safe_semicolons:
                    raise ValueError("Multiple statements or unsafe semicolons detected!")

            # Prevent unsafe data modification queries
            if any([query.upper().startswith(i) for i in UNSAFE_QUERIES]):
                raise ValueError("Trying Data Update, Not allowed!!!")
            
            # Execute query and measure time
            start_time = time.time()
            self.cursor.execute(query)
            execution_time = time.time() - start_time
            
            # Fetch results
            column_names = [desc[0] for desc in self.cursor.description]
            if fetch_all:
                results = self.cursor.fetchall()
            else:
                results = self.cursor.fetchmany(max_rows)
                
            df = pd.DataFrame(results, columns=column_names)
            
            return QueryResult(
                query=query,
                success=True,
                data=df.to_dict('records'),
                error=None,
                row_count=len(results),
                execution_time=execution_time
            )
            
        except Exception as e:
            return QueryResult(
                query=query,
                success=False,
                data=None,
                error=str(e),
                row_count=0,
                execution_time=0.0
            )

# %% ../nbs/00_SnowflakeCore.ipynb 35
# indivisual table
class TableAttr(BaseModel):
    name: str
    column_names: Optional[list[dict]] = None
    sample_rows: Optional[list[dict]] = None
    row_count : int 

# complete 
class ParentSchema(BaseModel):
    dialect: str
    database: str
    Schema: str
    tables: list[TableAttr]
    relationships: Optional[list[dict]] = None  # For foreign key

# %% ../nbs/00_SnowflakeCore.ipynb 56
@patch  
def _get_all_tables_metadata(self:SnowflakeAgent, db_name: str, schema_name: str, row_limit: int = 1):
        """
        This function is used to fetch the metadata of all the tables in the given database and schema.
        """
        if row_limit <= 0: row_limit = 1 # if negative row count is provided

        result = self.execute_query(f"SHOW TABLES IN {db_name}.{schema_name}", fetch_all=True)
        assert result.success, f"Not able to fetch the `DESCRIBE TABLE {tn}`"


        if len(result.data) == 0 :
            raise ValueError("Empty schema")

        all_tables_df = pd.DataFrame(result.data)

        print("Reading Tables....")
        all_table_atrs = []

        for tn in tqdm(all_tables_df['name']):
            # get table info
            tn = f"{db_name}.{schema_name}.{tn}"
            result = self.execute_query(f"DESCRIBE TABLE {tn}", fetch_all=True)
            assert result.success, f"Not able to fetch the `SHOW TABLES IN {tn}`"

            table_df = pd.DataFrame(result.data)

            # get column related data
            # this needs to be updated for the new db setup ie oracle and others
            column_names = []
            for _, row in table_df.iterrows():
                column_names.append({k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v  and v != 'N'})
            
            # fectch a single row of data from the table
            table_data_example = self.execute_query(f"select * from {tn} limit {row_limit}", fetch_all=True)
            assert table_data_example.success

            # fectchin row count data from the table
            row_count = self.execute_query(f"select count(*) as count from {tn}", fetch_all=True)
            assert row_count.success, f"select count(*) as count from {tn}"

            all_table_atrs.append(
                TableAttr(
                    name=tn,
                    column_names=column_names,
                    sample_rows=table_data_example.data,
                    row_count=next(iter(row_count.data[0].values())) # for ignoring the key if upper or lower type
                    ))

        return ParentSchema(
            dialect=self.dialect_name, # for now it is fixed to snowflake
            database=db_name,
            Schema=schema_name,
            tables=all_table_atrs
        )

# %% ../nbs/00_SnowflakeCore.ipynb 72
@patch
def _get_fk_metadata(
    self: SnowflakeAgent, 
    schema: ParentSchema, 
    model_name: str ="gemini/gemini-2.5-flash"
    ) -> ParentSchema:
    """
    Infer foreign key relationships between tables in a database schema.
    """
    try:
        # Query for column information
        # before any heavy query
        self.cursor.execute("ALTER SESSION SET STATEMENT_TIMEOUT_IN_SECONDS = 600")
        
        r = self.execute_query(f"""SELECT COLUMN_NAME, TABLE_NAME
FROM {schema.database}.INFORMATION_SCHEMA.COLUMNS 
WHERE TABLE_SCHEMA = '{schema.Schema}'
ORDER BY COLUMN_NAME, TABLE_NAME
""", fetch_all=True)
        
        if not r.success:
            raise Exception(f"Failed to query schema: {r.error}")
        
        df = pd.DataFrame(r.data)
        
        # Build schema summary
        schema_summary = []
        for table in schema.tables:
            cols = [col['name'] for col in table.column_names]
            schema_summary.append({
                "table": table.name,
                "columns": cols,
                "row_count": table.row_count,
                "sample": str(table.sample_rows[0] if table.sample_rows else {})
            })
        
        # Build prompt
        prompt = f"""Given this database schema, identify the foreign key relationships.

Schema: {json.dumps(schema_summary, indent=2)}

Foreign Keys relations db column name:
{json.dumps(df.groupby('COLUMN_NAME')['TABLE_NAME'].apply(list).to_dict(), indent=2)}

Return ONLY a JSON array of relationships in this exact format:
[
  {{
    "from_table": "FLIGHTS",
    "from_column": "aircraft_code",
    "to_table": "AIRCRAFTS_DATA",
    "to_column": "aircraft_code"
  }}
]

Rules:
- Only include relationships where a column in one table references a primary key in another
- Use row counts as hints (parent tables typically have fewer rows)
- Consider naming patterns (e.g., aircraft_code likely references AIRCRAFTS_DATA)
"""
        
        # Call LLM
        chat = Chat(model_name)
        resp = chat(prompt)
        content = resp.choices[0].message.content
        
        # Extract JSON more robustly
        # Try to find JSON array in the response
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if not json_match:
            raise ValueError("Could not find JSON array in LLM response")
        
        json_str = json_match.group(0)
        relationships = json.loads(json_str)
        
        # Validate it's a list
        if not isinstance(relationships, list):
            raise ValueError("Expected list of relationships from LLM")
        
        return relationships
        
    except json.JSONDecodeError as e:
        raise Exception(f"Failed to parse LLM response as JSON: {e}")
    except Exception as e:
        raise Exception(f"Error inferring foreign keys: {e}")

# %% ../nbs/00_SnowflakeCore.ipynb 76
FIELD_TO_FILTER = ['name', 'type', 'null?', 'default', 'primary key', 'unique key', 'comment', ] #"check", 'expression']
import hashlib

class SnowMetadata:
    def __init__(
        self, 
        agent : SnowflakeAgent, 
        db_name: str, 
        schema_name: str, 
        row_limit: int = 1, 
        model_name:str = "gemini/gemini-2.5-flash"
        ):

        self.agent = agent
        self.db_name = db_name
        self.schema_name = schema_name
        self.row_limit = row_limit
        self.model_name = model_name
    
    # for caching hashable by implementing __hash__  __eq__ for unique name
    # based only on the cacheable parameters (db_name, schema_name, etc.)
    # so that the searching ans saving would be done easyly
    def __hash__(self):
        # Use a tuple; Python’s built‑in hash on a tuple is also salted,
        # so we convert it to a string and then to an int via a stable hash.
        raw = (self.db_name, self.schema_name, self.row_limit, self.model_name)
        # Convert to a reproducible integer with hashlib
        return int(hashlib.md5(str(raw).encode()).hexdigest(), 16)
    
    def __eq__(self, other):
        if not isinstance(other, SnowMetadata):
            return False
        return (self.db_name == other.db_name and 
                self.schema_name == other.schema_name and
                self.row_limit == other.row_limit and
                self.model_name == other.model_name)

    def __call__(self): 
        pass
        # Checke the blow monkey patched

# %% ../nbs/00_SnowflakeCore.ipynb 85
CHACHE_DIR = Path('../cache')
CHACHE_DIR.mkdir(exist_ok=True)

# %% ../nbs/00_SnowflakeCore.ipynb 94
@patch(as_prop=True)
def fn(self:SnowMetadata):
    return CHACHE_DIR / f"{self.__hash__()}.json"

# %% ../nbs/00_SnowflakeCore.ipynb 97
@patch(as_prop=True)
def metadata2json_str(self:SnowMetadata):
    return self.metadata.model_dump_json(indent=2)   # or .dict() + json.dumps(...)

# %% ../nbs/00_SnowflakeCore.ipynb 99
@patch
def load_metadata(self:SnowMetadata, json_str):
    metadata_dict = json.loads(json_str)
    self.metadata = ParentSchema(**metadata_dict)

# %% ../nbs/00_SnowflakeCore.ipynb 102
@patch
def metadata2disk(self:SnowMetadata):
    self.fn.write_text(self.metadata2json_str, "utf-8")

@patch
def disk2metadata(self:SnowMetadata):
    data = self.fn.read_text("utf-8")
    self.load_metadata(data)

# %% ../nbs/00_SnowflakeCore.ipynb 108
@patch
def __call__(self:SnowMetadata, use_cache:bool = True):
    if use_cache and self.fn.exists():
        self.disk2metadata()
        assert hasattr(self, "metadata")

    else:
        # get metadata from snowflake
        self.metadata = self.agent._get_all_tables_metadata(
            db_name=self.db_name,
            schema_name=self.schema_name,
            row_limit=self.row_limit,
        )

        # get fk relationships
        self.metadata.relationships = self.agent._get_fk_metadata(
            self.metadata,
            model_name=self.model_name,
        )
        
        # save metadata to disk if the file does not exist
        if not self.fn.exists():
            self.metadata2disk()

    return True
