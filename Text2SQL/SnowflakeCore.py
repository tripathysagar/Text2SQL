"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_SnowflakeCore.ipynb.

# %% auto 0
__all__ = ['model_name', 'conn', 'cursor', 'UNSAFE_QUERIES', 'FIELD_TO_FILTER', 'QueryResult', 'execute_query', 'TableAttr',
           'ParentSchema', 'get_fk', 'get_schema_context']

# %% ../nbs/00_SnowflakeCore.ipynb 4
from dotenv import load_dotenv
import os
import json
import pandas as pd
from tqdm import tqdm
from fastcore.utils import *
import regex as re
from lisette import *

# Load environment variables from the .env file
load_dotenv()

assert os.getenv("SPIDER2_SNOWFLAKE_USERNAME")
assert os.getenv("SPIDER2_SNOWFLAKE_PASSWORD")
assert os.getenv("SPIDER2_SNOWFLAKE_ACCOUNT")
assert os.getenv("COMPUTE_WH_PARTICIPANT")

# %% ../nbs/00_SnowflakeCore.ipynb 6
assert os.getenv("LM_STUDIO_API_BASE")
assert os.getenv("LM_STUDIO_MODEL_NAME")
model_name = os.getenv("LM_STUDIO_MODEL_NAME")
model_name

# %% ../nbs/00_SnowflakeCore.ipynb 7
import litellm

litellm.register_model(
    {
        model_name:{
        "max_tokens": 8192, # put the modelâ€™s real context limit
        "input_cost_per_token": 0.0,
        "output_cost_per_token": 0.0,
        "supports_assistant_prefill": False
    }})

# %% ../nbs/00_SnowflakeCore.ipynb 10
import snowflake.connector

conn = snowflake.connector.connect(
    user=os.getenv("SPIDER2_SNOWFLAKE_USERNAME"),
    password=os.getenv("SPIDER2_SNOWFLAKE_PASSWORD"),
    account=os.getenv("SPIDER2_SNOWFLAKE_ACCOUNT"),
    warehouse=os.getenv("COMPUTE_WH_PARTICIPANT"),
)
cursor = conn.cursor()

assert not cursor.is_closed()

# %% ../nbs/00_SnowflakeCore.ipynb 22
from pydantic import BaseModel
from typing import Optional

class QueryResult(BaseModel):
    query: str
    success: bool
    data: Optional[list[dict]] = None
    error: Optional[str] = None
    row_count: int = 0
    execution_time: float = 0.0

# %% ../nbs/00_SnowflakeCore.ipynb 23
import time

UNSAFE_QUERIES = ['DROP', 'UPDATE', 'DELETE', 'INSERT', 'TRUNCATE', 'ALTER']

def execute_query(
    query: str,                 # SQL query with to execute
    max_rows: int = 10,         # Maximum rows to fetch
    fetch_all: bool = False,    # Fetch all rows if True
) -> QueryResult:
    """
    Execute a SQL query and return results as a QueryResult object.
    
    This function provides safe SQL execution with protection against:
    - Multiple statements (via semicolon detection)
    - Unsafe operations (DROP, DELETE, UPDATE, etc.)
    - Memory issues (via row limiting)
    
    Args:
        query (str): The SQL query to execute. Must be a single SELECT statement.
        max_rows (int, optional): Maximum number of rows to fetch. Defaults to 10.
            Only applies when fetch_all=False.
        fetch_all (bool, optional): If True, fetch all rows regardless of max_rows.
            Defaults to False for safety.
    
    Returns:
        QueryResult: A Pydantic model containing:
            - query: The executed query
            - success: Whether execution succeeded
            - data: List of dictionaries (rows) if successful, None otherwise
            - error: Error message if failed, None otherwise
            - row_count: Number of rows returned
            - execution_time: Time taken to execute the query in seconds
    
    Raises:
        ValueError: If query contains multiple statements or unsafe operations.
        Exception: Any database errors are caught and returned in QueryResult.error
    
    Examples:
        >>> result = sql_df("SELECT * FROM users")
        >>> result.success
        True
        >>> result.row_count
        10
        
        >>> result = sql_df("SELECT * FROM users", fetch_all=True)
        >>> result.row_count
        1000
    """
    try:
        query = query.strip()

        # Check for multiple statements via semicolon detection
        if ';' in query:
            total_semicolons = query.count(';')
            safe_semicolons = len(re.findall(r';', ''.join(re.findall(r'"[^"]*"|\'[^\']*\'', query))))
            
            if total_semicolons != safe_semicolons:
                raise ValueError("Multiple statements or unsafe semicolons detected!")

        # Prevent unsafe data modification queries
        if any([query.upper().startswith(i) for i in UNSAFE_QUERIES]):
            raise ValueError("Trying Data Update, Not allowed!!!")
        
        # Execute query and measure time
        start_time = time.time()
        cursor.execute(query)
        execution_time = time.time() - start_time
        
        # Fetch results
        column_names = [desc[0] for desc in cursor.description]
        if fetch_all:
            results = cursor.fetchall()
        else:
            results = cursor.fetchmany(max_rows)
            
        df = pd.DataFrame(results, columns=column_names)
        
        return QueryResult(
            query=query,
            success=True,
            data=df.to_dict('records'),
            error=None,
            row_count=len(results),
            execution_time=execution_time
        )
        
    except Exception as e:
        return QueryResult(
            query=query,
            success=False,
            data=None,
            error=str(e),
            row_count=0,
            execution_time=0.0
        )


# %% ../nbs/00_SnowflakeCore.ipynb 29
# indivisual table
class TableAttr(BaseModel):
    name: str
    column_names: Optional[list[dict]] = None
    sample_rows: Optional[list[dict]] = None
    row_count : int 

# complete 
class ParentSchema(BaseModel):
    dialect: str
    database: str
    Schema: str
    tables: list[TableAttr]
    relationships: Optional[list[dict]] = None  # For foreign key

# %% ../nbs/00_SnowflakeCore.ipynb 63
def get_fk(schema: ParentSchema, model_name: str ="gemini/gemini-2.5-flash") -> ParentSchema:
    try:
        # Query for column information
        r = execute_query(f"""
SELECT COLUMN_NAME, TABLE_NAME
FROM {schema.database}.INFORMATION_SCHEMA.COLUMNS 
WHERE TABLE_SCHEMA = '{schema.Schema}'
ORDER BY COLUMN_NAME, TABLE_NAME
""", fetch_all=True)
        
        if not r.success:
            raise Exception(f"Failed to query schema: {r.error}")
        
        df = pd.DataFrame(r.data)
        
        # Build schema summary
        schema_summary = []
        for table in schema.tables:
            cols = [col['name'] for col in table.column_names]
            schema_summary.append({
                "table": table.name,
                "columns": cols,
                "row_count": table.row_count,
                "sample": table.sample_rows[0] if table.sample_rows else {}
            })
        
        # Build prompt
        prompt = f"""Given this database schema, identify the foreign key relationships.

Schema: {json.dumps(schema_summary, indent=2)}

Foreign Keys relations db column name:
{json.dumps(df.groupby('COLUMN_NAME')['TABLE_NAME'].apply(list).to_dict(), indent=2)}

Return ONLY a JSON array of relationships in this exact format:
[
  {{
    "from_table": "FLIGHTS",
    "from_column": "aircraft_code",
    "to_table": "AIRCRAFTS_DATA",
    "to_column": "aircraft_code"
  }}
]

Rules:
- Only include relationships where a column in one table references a primary key in another
- Use row counts as hints (parent tables typically have fewer rows)
- Consider naming patterns (e.g., aircraft_code likely references AIRCRAFTS_DATA)
"""
        
        # Call LLM
        chat = Chat(model_name)
        resp = chat(prompt)
        content = resp.choices[0].message.content
        
        # Extract JSON more robustly
        # Try to find JSON array in the response
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if not json_match:
            raise ValueError("Could not find JSON array in LLM response")
        
        json_str = json_match.group(0)
        relationships = json.loads(json_str)
        
        # Validate it's a list
        if not isinstance(relationships, list):
            raise ValueError("Expected list of relationships from LLM")
        
        schema.relationships = relationships
        return schema
        
    except json.JSONDecodeError as e:
        raise Exception(f"Failed to parse LLM response as JSON: {e}")
    except Exception as e:
        raise Exception(f"Error inferring foreign keys: {e}")


# %% ../nbs/00_SnowflakeCore.ipynb 66
FIELD_TO_FILTER = ['name', 'type', 'null?', 'default', 'primary key', 'unique key', 'comment', ] #"check", 'expression']


def get_schema_context(db_name: str, schema_name: str, row_limit: int = 1, model_name:str = "gemini/gemini-2.5-flash" ) -> ParentSchema:
    """
    Extract complete schema context from a Snowflake database for LLM text-to-SQL generation.
    
    This function retrieves comprehensive metadata about all tables in a schema, including:
    - Table and column names with data types
    - Column constraints (nullable, primary keys, unique keys)
    - Sample rows to show data format
    - Row counts for each table
    
    Args:
        db_name (str): Name of the Snowflake database
        schema_name (str): Name of the schema within the database
        row_limit (int, optional): Number of sample rows to fetch per table. Defaults to 1.
    
    Returns:
        ParentSchema: Pydantic model containing complete schema context with:
            - dialect: Database dialect (always 'snowflake')
            - database: Database name
            - Schema: Schema name
            - tables: List of TableAttr objects with full metadata
    
    Raises:
        AssertionError: If any database query fails during schema extraction
    
    Example:
        >>> schema = get_schema_context("AIRLINES", "AIRLINES", row_limit=3)
        >>> print(f"Found {len(schema.tables)} tables")
        >>> print(schema.model_dump_json(indent=2))
    """
    if row_limit <= 0: row_limit = 1 # if negative row count is provided

    result = execute_query(f"SHOW TABLES IN {db_name}.{schema_name}", fetch_all=True)
    assert result.success, f"Not able to fetch the `SHOW TABLES IN {db_name}.{schema_name}`"

    if len(result.data) == 0 :
        raise ValueError("Empty schema")

    all_tables_df = pd.DataFrame(result.data)

    print("Reading Tables....")
    all_table_atrs = []
    for tn in tqdm(all_tables_df['name']):
        # get table info
        result = execute_query(f"DESCRIBE TABLE {db_name}.{schema_name}.{tn}", fetch_all=True)
        assert result.success, f"Not able to fetch the `SHOW TABLES IN {db_name}.{schema_name}.{tn}`"

        table_df = pd.DataFrame(result.data)

        # get column related data
        # this needs to be updated for the new db setup ie oracle and others
        column_names = []
        for _, row in table_df.iterrows():
            column_names.append({k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v  and v != 'N'})
        
        # fectch a single row of data from the table
        table_data_example = execute_query(f"select * from {db_name}.{schema_name}.{tn} limit {row_limit}", fetch_all=True)
        assert table_data_example.success

        # fectch a single row of data from the table
        row_count = execute_query(f"select count(*)  as count from {db_name}.{schema_name}.{tn}", fetch_all=True)
        assert row_count.success

        all_table_atrs.append(
            TableAttr(
                name=tn,
                column_names=column_names,
                sample_rows=table_data_example.data,
                row_count=next(iter(row_count.data[0].values())) # for ignoring the key if upper or lower type
                ))

    base_s =  ParentSchema(
        dialect="snowflake", # for now it is fixed to snowflake
        database=db_name,
        Schema=schema_name,
        tables=all_table_atrs
    )
    print("Building up FKs....")
    new_s = get_fk(base_s, model_name)

    return new_s


