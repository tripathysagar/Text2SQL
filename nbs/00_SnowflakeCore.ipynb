{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowflakeCore\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp SnowflakeCore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q snowflake-connector-python  lisette  pandas tqdm fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from fastcore.utils import *\n",
    "import regex as re\n",
    "from lisette import *\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "assert os.getenv(\"SPIDER2_SNOWFLAKE_USERNAME\")\n",
    "assert os.getenv(\"SPIDER2_SNOWFLAKE_PASSWORD\")\n",
    "assert os.getenv(\"SPIDER2_SNOWFLAKE_ACCOUNT\")\n",
    "assert os.getenv(\"COMPUTE_WH_PARTICIPANT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM_STUDIO calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lm_studio/openai/gpt-oss-20b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "assert os.getenv(\"LM_STUDIO_API_BASE\")\n",
    "assert os.getenv(\"LM_STUDIO_MODEL_NAME\")\n",
    "model_name = os.getenv(\"LM_STUDIO_MODEL_NAME\")\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_studio/openai/gpt-oss-20b': {'max_tokens': 8192,\n",
       "  'input_cost_per_token': 0.0,\n",
       "  'output_cost_per_token': 0.0,\n",
       "  'supports_assistant_prefill': False}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "import litellm\n",
    "\n",
    "litellm.register_model(\n",
    "    {\n",
    "        model_name:{\n",
    "        \"max_tokens\": 8192, # put the modelâ€™s real context limit\n",
    "        \"input_cost_per_token\": 0.0,\n",
    "        \"output_cost_per_token\": 0.0,\n",
    "        \"supports_assistant_prefill\": False\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey! ðŸ‘‹ Howâ€™s it going?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-owuv2mmdrxs069q4sgmkpxt`\n",
       "- model: `lm_studio/openai/gpt-oss-20b`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=22, prompt_tokens=70, total_tokens=92, completion_tokens_details=None, prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-owuv2mmdrxs069q4sgmkpxt', created=1763695923, model='lm_studio/openai/gpt-oss-20b', object='chat.completion', system_fingerprint='openai/gpt-oss-20b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey! ðŸ‘‹ Howâ€™s it going?', role='assistant', tool_calls=None, function_call=None, reasoning_content='Need friendly greeting.', provider_specific_fields={'refusal': None, 'reasoning': 'Need friendly greeting.', 'reasoning_content': 'Need friendly greeting.'}), provider_specific_fields={})], usage=Usage(completion_tokens=22, prompt_tokens=70, total_tokens=92, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, stats={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model_name)\n",
    "chat(\"Hello there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowflake setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import snowflake.connector\n",
    "\n",
    "def connect_to_snowflake():\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv(\"SPIDER2_SNOWFLAKE_USERNAME\"),\n",
    "        password=os.getenv(\"SPIDER2_SNOWFLAKE_PASSWORD\"),\n",
    "        account=os.getenv(\"SPIDER2_SNOWFLAKE_ACCOUNT\"),\n",
    "        warehouse=os.getenv(\"COMPUTE_WH_PARTICIPANT\"),\n",
    "        session_parameters={\n",
    "            \"STATEMENT_TIMEOUT_IN_SECONDS\": 300,   # 5â€¯min timeout\n",
    "            \"QUERY_TAG\": \"Text2SQL\",              # optional tag for tracing\n",
    "        }\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    assert not cursor.is_closed()\n",
    "\n",
    "    cursor.arraysize = 10_000   # default is usually 1\n",
    "\n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cursor = connect_to_snowflake()\n",
    "assert not cursor.is_closed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.arraysize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m           SnowflakeCursor\n",
      "\u001b[31mString form:\u001b[39m    <snowflake.connector.cursor.SnowflakeCursor object at 0x7f18a5083620>\n",
      "\u001b[31mFile:\u001b[39m           ~/miniconda3/lib/python3.13/site-packages/snowflake/connector/cursor.py\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m SnowflakeCursor(SnowflakeCursorBase[tuple[Any, ...]]):\n",
      "    \u001b[33m\"\"\"Implementation of Cursor object that is returned from Connection.cursor() method.\u001b[39m\n",
      "\n",
      "\u001b[33m    Attributes:\u001b[39m\n",
      "\u001b[33m        description: A list of namedtuples about metadata for all columns.\u001b[39m\n",
      "\u001b[33m        rowcount: The number of records updated or selected. If not clear, -1 is returned.\u001b[39m\n",
      "\u001b[33m        rownumber: The current 0-based index of the cursor in the result set or None if the index cannot be\u001b[39m\n",
      "\u001b[33m            determined.\u001b[39m\n",
      "\u001b[33m        sfqid: Snowflake query id in UUID form. Include this in the problem report to the customer support.\u001b[39m\n",
      "\u001b[33m        sqlstate: Snowflake SQL State code.\u001b[39m\n",
      "\u001b[33m        timestamp_output_format: Snowflake timestamp_output_format for timestamps.\u001b[39m\n",
      "\u001b[33m        timestamp_ltz_output_format: Snowflake output format for LTZ timestamps.\u001b[39m\n",
      "\u001b[33m        timestamp_tz_output_format: Snowflake output format for TZ timestamps.\u001b[39m\n",
      "\u001b[33m        timestamp_ntz_output_format: Snowflake output format for NTZ timestamps.\u001b[39m\n",
      "\u001b[33m        date_output_format: Snowflake output format for dates.\u001b[39m\n",
      "\u001b[33m        time_output_format: Snowflake output format for times.\u001b[39m\n",
      "\u001b[33m        timezone: Snowflake timezone.\u001b[39m\n",
      "\u001b[33m        binary_output_format: Snowflake output format for binary fields.\u001b[39m\n",
      "\u001b[33m        arraysize: The default number of rows fetched by fetchmany.\u001b[39m\n",
      "\u001b[33m        connection: The connection object by which the cursor was created.\u001b[39m\n",
      "\u001b[33m        errorhandle: The class that handles error handling.\u001b[39m\n",
      "\u001b[33m        is_file_transfer: Whether, or not the current command is a put, or get.\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _use_dict_result(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m fetchone(self) -> tuple[Any, ...] | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        row = self._fetchone()\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m (row \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m isinstance(row, tuple)):\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(f\"fetchone got unexpected result: {row}\")\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "\u001b[31mInit docstring:\u001b[39m\n",
      "Inits a SnowflakeCursor with a connection.\n",
      "\n",
      "Args:\n",
      "    connection: The connection that created this cursor."
     ]
    }
   ],
   "source": [
    "cursor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m           SnowflakeConnection\n",
      "\u001b[31mString form:\u001b[39m    <snowflake.connector.connection.SnowflakeConnection object at 0x7f18a50834d0>\n",
      "\u001b[31mFile:\u001b[39m           ~/miniconda3/lib/python3.13/site-packages/snowflake/connector/connection.py\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m SnowflakeConnection:\n",
      "    \u001b[33m\"\"\"Implementation of the connection object for the Snowflake Database.\u001b[39m\n",
      "\n",
      "\u001b[33m    Use connect(..) to get the object.\u001b[39m\n",
      "\n",
      "\u001b[33m    Attributes:\u001b[39m\n",
      "\u001b[33m        insecure_mode (deprecated): Whether or not the connection is in OCSP disabled mode. It means that the connection\u001b[39m\n",
      "\u001b[33m            validates the TLS certificate but doesn't check revocation status with OCSP provider.\u001b[39m\n",
      "\u001b[33m        disable_ocsp_checks: Whether or not the connection is in OCSP disabled mode. It means that the connection\u001b[39m\n",
      "\u001b[33m            validates the TLS certificate but doesn't check revocation status with OCSP provider.\u001b[39m\n",
      "\u001b[33m        ocsp_fail_open: Whether or not the connection is in fail open mode. Fail open mode decides if TLS certificates\u001b[39m\n",
      "\u001b[33m            continue to be validated. Revoked certificates are blocked. Any other exceptions are disregarded.\u001b[39m\n",
      "\u001b[33m        ocsp_root_certs_dict_lock_timeout: Timeout for the OCSP root certs dict lock in seconds. Default value is -1, which means no timeout.\u001b[39m\n",
      "\u001b[33m        session_id: The session ID of the connection.\u001b[39m\n",
      "\u001b[33m        user: The user name used in the connection.\u001b[39m\n",
      "\u001b[33m        host: The host name the connection attempts to connect to.\u001b[39m\n",
      "\u001b[33m        port: The port to communicate with on the host.\u001b[39m\n",
      "\u001b[33m        region: Region name if not the default Snowflake Database deployment.\u001b[39m\n",
      "\u001b[33m        proxy_host: The hostname used proxy server.\u001b[39m\n",
      "\u001b[33m        proxy_port: Port on proxy server to communicate with.\u001b[39m\n",
      "\u001b[33m        proxy_user: User name to login with on the proxy sever.\u001b[39m\n",
      "\u001b[33m        proxy_password: Password to be used to authenticate with proxy server.\u001b[39m\n",
      "\u001b[33m        account: Account name to be used to authenticate with Snowflake.\u001b[39m\n",
      "\u001b[33m        database: Database to use on Snowflake.\u001b[39m\n",
      "\u001b[33m        schema: Schema in use on Snowflake.\u001b[39m\n",
      "\u001b[33m        warehouse: Warehouse to be used on Snowflake.\u001b[39m\n",
      "\u001b[33m        role: Role in use on Snowflake.\u001b[39m\n",
      "\u001b[33m        login_timeout: Login timeout in seconds. Login requests will not be retried after this timeout expires.\u001b[39m\n",
      "\u001b[33m            Note that the login attempt may still take more than login_timeout seconds as an ongoing login request\u001b[39m\n",
      "\u001b[33m            cannot be canceled even upon login timeout expiry. The login timeout only prevents further retries.\u001b[39m\n",
      "\u001b[33m            If not specified, login_timeout is set to `snowflake.connector.auth.by_plugin.DEFAULT_AUTH_CLASS_TIMEOUT`.\u001b[39m\n",
      "\u001b[33m            Note that the number of retries on login requests is still limited by\u001b[39m\n",
      "\u001b[33m            `snowflake.connector.auth.by_plugin.DEFAULT_MAX_CON_RETRY_ATTEMPTS`.\u001b[39m\n",
      "\u001b[33m        network_timeout: Network timeout in seconds. Network requests besides login requests will not be retried\u001b[39m\n",
      "\u001b[33m            after this timeout expires. Overriden in cursor query execution if timeout is passed to cursor.execute.\u001b[39m\n",
      "\u001b[33m            Note that an operation may still take more than network_timeout seconds for the same reason as above.\u001b[39m\n",
      "\u001b[33m            If not specified, network_timeout is infinite.\u001b[39m\n",
      "\u001b[33m        socket_timeout: Socket timeout in seconds. Sets both socket connect and read timeout.\u001b[39m\n",
      "\u001b[33m        backoff_policy: Backoff policy to use for login and network requests. Must be a callable generator function.\u001b[39m\n",
      "\u001b[33m            Standard linear and exponential backoff implementations are included in `snowflake.connector.backoff_policies`\u001b[39m\n",
      "\u001b[33m            See the backoff_policies module for details and implementation examples.\u001b[39m\n",
      "\u001b[33m        client_session_keep_alive_heartbeat_frequency: Heartbeat frequency to keep connection alive in seconds.\u001b[39m\n",
      "\u001b[33m        client_prefetch_threads: Number of threads to download the result set.\u001b[39m\n",
      "\u001b[33m        client_fetch_threads: Number of threads (or processes) to fetch staged query results.\u001b[39m\n",
      "\u001b[33m            If not specified, reuses client_prefetch_threads value.\u001b[39m\n",
      "\u001b[33m        client_fetch_use_mp: Enables multiprocessing for fetching query results in parallel.\u001b[39m\n",
      "\u001b[33m        rest: Snowflake REST API object. Internal use only. Maybe removed in a later release.\u001b[39m\n",
      "\u001b[33m        application: Application name to communicate with Snowflake as. By default, this is \"PythonConnector\".\u001b[39m\n",
      "\u001b[33m        errorhandler: Handler used with errors. By default, an exception will be raised on error.\u001b[39m\n",
      "\u001b[33m        converter_class: Handler used to convert data to Python native objects.\u001b[39m\n",
      "\u001b[33m        validate_default_parameters: Validate database, schema, role and warehouse used on Snowflake.\u001b[39m\n",
      "\u001b[33m        is_pyformat: Whether the current argument binding is pyformat or format.\u001b[39m\n",
      "\u001b[33m        consent_cache_id_token: Consented cache ID token.\u001b[39m\n",
      "\u001b[33m        enable_stage_s3_privatelink_for_us_east_1: when true, clients use regional s3 url to upload files.\u001b[39m\n",
      "\u001b[33m        enable_connection_diag: when true, clients will generate a connectivity diagnostic report.\u001b[39m\n",
      "\u001b[33m        connection_diag_log_path: path to location to create diag report with enable_connection_diag.\u001b[39m\n",
      "\u001b[33m        connection_diag_whitelist_path: path to a whitelist.json file to test with enable_connection_diag - deprecated remove in future\u001b[39m\n",
      "\u001b[33m        connection_diag_allowlist_path: path to a allowlist.json file to test with enable_connection_diag.\u001b[39m\n",
      "\u001b[33m        json_result_force_utf8_decoding: When true, json result will be decoded in utf-8,\u001b[39m\n",
      "\u001b[33m          when false, the encoding of the content is auto-detected. Default value is false.\u001b[39m\n",
      "\u001b[33m          This parameter is only effective when the result format is JSON.\u001b[39m\n",
      "\u001b[33m        server_session_keep_alive: When true, the connector does not destroy the session on the Snowflake server side\u001b[39m\n",
      "\u001b[33m          before the connector shuts down. Default value is false.\u001b[39m\n",
      "\u001b[33m        token_file_path: The file path of the token file. If both token and token_file_path are provided, the token in token_file_path will be used.\u001b[39m\n",
      "\u001b[33m        unsafe_file_write: When true, files downloaded by GET will be saved with 644 permissions. Otherwise, files will be saved with safe - owner-only permissions: 600.\u001b[39m\n",
      "\u001b[33m        check_arrow_conversion_error_on_every_column: When true, the error check after the conversion from arrow to python types will happen for every column in the row. This is a new behaviour which fixes the bug that caused the type errors to trigger silently when occurring at any place other than last column in a row. To revert the previous (faulty) behaviour, please set this flag to false.\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    OCSP_ENV_LOCK = Lock()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        connection_name: str | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        connections_file_path: pathlib.Path | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        **kwargs,\n",
      "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Create a new SnowflakeConnection.\u001b[39m\n",
      "\n",
      "\u001b[33m        Connections can be loaded from the TOML file located at\u001b[39m\n",
      "\u001b[33m        snowflake.connector.constants.CONNECTIONS_FILE.\u001b[39m\n",
      "\n",
      "\u001b[33m        When connection_name is supplied we will first load that connection\u001b[39m\n",
      "\u001b[33m        and then override any other values supplied.\u001b[39m\n",
      "\n",
      "\u001b[33m        When no arguments are given (other than connection_file_path) the\u001b[39m\n",
      "\u001b[33m        default connection will be loaded first. Note that no overwriting is\u001b[39m\n",
      "\u001b[33m        supported in this case.\u001b[39m\n",
      "\n",
      "\u001b[33m        If overwriting values from the default connection is desirable, supply\u001b[39m\n",
      "\u001b[33m        the name explicitly.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        self._unsafe_skip_file_permissions_check = kwargs.get(\n",
      "            \u001b[33m\"unsafe_skip_file_permissions_check\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "        )\n",
      "        \u001b[38;5;66;03m# initiate easy logging during every connection\u001b[39;00m\n",
      "        easy_logging = EasyLoggingConfigPython(\n",
      "            skip_config_file_permissions_check=self._unsafe_skip_file_permissions_check\n",
      "        )\n",
      "        easy_logging.create_log()\n",
      "        self._lock_sequence_counter = Lock()\n",
      "        self.sequence_counter = \u001b[32m0\u001b[39m\n",
      "        self._errorhandler = Error.default_errorhandler\n",
      "        self._lock_converter = Lock()\n",
      "        self.messages = []\n",
      "        self._async_sfqids: dict[str, \u001b[38;5;28;01mNone\u001b[39;00m] = {}\n",
      "        self._done_async_sfqids: dict[str, \u001b[38;5;28;01mNone\u001b[39;00m] = {}\n",
      "        self._client_param_telemetry_enabled = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "        self._server_param_telemetry_enabled = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "        self._session_parameters: dict[str, str | int | bool] = {}\n",
      "        logger.info(\n",
      "            \u001b[33m\"Snowflake Connector for Python Version: %s, \"\u001b[39m\n",
      "            \u001b[33m\"Python Version: %s, Platform: %s\"\u001b[39m,\n",
      "            SNOWFLAKE_CONNECTOR_VERSION,\n",
      "            PYTHON_VERSION,\n",
      "            PLATFORM,\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;66;03m# Placeholder attributes; will be initialized in connect()\u001b[39;00m\n",
      "        self._http_config: HttpConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        self._crl_config: CRLConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        self._session_manager: SessionManager | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        self._rest: SnowflakeRestful | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m name, (value, _) \u001b[38;5;28;01min\u001b[39;00m DEFAULT_CONFIGURATION.items():\n",
      "            setattr(self, f\"_{name}\", value)\n",
      "\n",
      "        self.heartbeat_thread = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        is_kwargs_empty = \u001b[38;5;28;01mnot\u001b[39;00m kwargs\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"application\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            app = self._detect_application()\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m app:\n",
      "                kwargs[\u001b[33m\"application\"\u001b[39m] = app\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"insecure_mode\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            warn_message = \u001b[33m\"The 'insecure_mode' connection property is deprecated. Please use 'disable_ocsp_checks' instead\"\u001b[39m\n",
      "            warnings.warn(\n",
      "                warn_message,\n",
      "                DeprecationWarning,\n",
      "                stacklevel=\u001b[32m2\u001b[39m,\n",
      "            )\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                \u001b[33m\"disable_ocsp_checks\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m kwargs[\u001b[33m\"disable_ocsp_checks\"\u001b[39m] != kwargs[\u001b[33m\"insecure_mode\"\u001b[39m]\n",
      "            ):\n",
      "                logger.warning(\n",
      "                    \u001b[33m\"The values for 'disable_ocsp_checks' and 'insecure_mode' differ. \"\u001b[39m\n",
      "                    \u001b[33m\"Using the value of 'disable_ocsp_checks.\"\u001b[39m\n",
      "                )\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                self._disable_ocsp_checks = kwargs[\u001b[33m\"insecure_mode\"\u001b[39m]\n",
      "\n",
      "        self.converter = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        self.query_context_cache: QueryContextCache | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        self.query_context_cache_size = \u001b[32m5\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m connections_file_path \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# Change config file path and force update cache\u001b[39;00m\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;28;01min\u001b[39;00m enumerate(CONFIG_MANAGER._slices):\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m s.section == \u001b[33m\"connections\"\u001b[39m:\n",
      "                    CONFIG_MANAGER._slices[i] = s._replace(path=connections_file_path)\n",
      "                    CONFIG_MANAGER.read_config(\n",
      "                        skip_file_permissions_check=self._unsafe_skip_file_permissions_check\n",
      "                    )\n",
      "                    \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m connection_name \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            connections = CONFIG_MANAGER[\u001b[33m\"connections\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m connection_name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m connections:\n",
      "                \u001b[38;5;28;01mraise\u001b[39;00m Error(\n",
      "                    f\"Invalid connection_name '{connection_name}',\"\n",
      "                    f\" known ones are {list(connections.keys())}\"\n",
      "                )\n",
      "            kwargs = {**connections[connection_name], **kwargs}\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m is_kwargs_empty:\n",
      "            \u001b[38;5;66;03m# connection_name is None and kwargs was empty when called\u001b[39;00m\n",
      "            kwargs = _get_default_connection_params()\n",
      "        self.__set_error_attributes()\n",
      "        self.connect(**kwargs)\n",
      "        self._telemetry = TelemetryClient(self._rest)\n",
      "        self.expired = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "        \u001b[38;5;66;03m# get the imported modules from sys.modules\u001b[39;00m\n",
      "        self._log_telemetry_imported_packages()\n",
      "        \u001b[38;5;66;03m# check SNOW-1218851 for long term improvement plan to refactor ocsp code\u001b[39;00m\n",
      "        atexit.register(self._close_at_exit)\n",
      "\n",
      "        \u001b[38;5;66;03m# Set up the file operation parser and stream downloader.\u001b[39;00m\n",
      "        self._file_operation_parser = FileOperationParser(self)\n",
      "        self._stream_downloader = StreamDownloader(self)\n",
      "\n",
      "    \u001b[38;5;66;03m# Deprecated\u001b[39;00m\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m insecure_mode(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._disable_ocsp_checks\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m disable_ocsp_checks(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._disable_ocsp_checks\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m ocsp_fail_open(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._ocsp_fail_open\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _ocsp_mode(self) -> OCSPMode:\n",
      "        \u001b[33m\"\"\"OCSP mode. DISABLE_OCSP_CHECKS, FAIL_OPEN or FAIL_CLOSED.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.disable_ocsp_checks:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m OCSPMode.DISABLE_OCSP_CHECKS\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.ocsp_fail_open:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m OCSPMode.FAIL_OPEN\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m OCSPMode.FAIL_CLOSED\n",
      "\n",
      "    \u001b[38;5;66;03m# CRL (Certificate Revocation List) configuration properties\u001b[39;00m\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m cert_revocation_check_mode(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Certificate revocation check mode: DISABLED, ENABLED, or ADVISORY.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._cert_revocation_check_mode\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.cert_revocation_check_mode.value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m allow_certificates_without_crl_url(self) -> bool | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Whether to allow certificates without CRL distribution points.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._allow_certificates_without_crl_url\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.allow_certificates_without_crl_url\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_connection_timeout_ms(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Connection timeout for CRL downloads in milliseconds.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_connection_timeout_ms\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.connection_timeout_ms\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_read_timeout_ms(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Read timeout for CRL downloads in milliseconds.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_read_timeout_ms\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.read_timeout_ms\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_cache_validity_hours(self) -> float | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"CRL cache validity time in hours.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_cache_validity_hours\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.cache_validity_time.total_seconds() / \u001b[32m3600\u001b[39m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m enable_crl_cache(self) -> bool | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Whether CRL caching is enabled.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._enable_crl_cache\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.enable_crl_cache\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m enable_crl_file_cache(self) -> bool | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Whether file-based CRL cache is enabled.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._enable_crl_file_cache\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.enable_crl_file_cache\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_cache_dir(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Directory for CRL file cache.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_cache_dir\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config.crl_cache_dir:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m str(self._crl_config.crl_cache_dir)\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_cache_removal_delay_days(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Days to keep expired CRL files before removal.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_cache_removal_delay_days\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.crl_cache_removal_delay_days\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_cache_cleanup_interval_hours(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"CRL cache cleanup interval in hours.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_cache_cleanup_interval_hours\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.crl_cache_cleanup_interval_hours\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m crl_cache_start_cleanup(self) -> bool | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Whether to start CRL cache cleanup immediately.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._crl_config:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_cache_start_cleanup\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._crl_config.crl_cache_start_cleanup\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m session_id(self) -> int:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._session_id\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m user(self) -> str:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._user\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m host(self) -> str:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._host\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m port(self) -> int:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m int(self._port)\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m region(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        warnings.warn(\n",
      "            \u001b[33m\"Region has been deprecated and will be removed in the near future\"\u001b[39m,\n",
      "            PendingDeprecationWarning,\n",
      "            \u001b[38;5;66;03m# Raise warning from where this property was called from\u001b[39;00m\n",
      "            stacklevel=\u001b[32m2\u001b[39m,\n",
      "        )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._region\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m proxy_host(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._proxy_host\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m proxy_port(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._proxy_port\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m proxy_user(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._proxy_user\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m proxy_password(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._proxy_password\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m no_proxy(self) -> str | Iterable | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._no_proxy\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m account(self) -> str:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._account\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m database(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._database\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m schema(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._schema\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m warehouse(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._warehouse\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m role(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._role\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m login_timeout(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m int(self._login_timeout) \u001b[38;5;28;01mif\u001b[39;00m self._login_timeout \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m network_timeout(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m int(self._network_timeout) \u001b[38;5;28;01mif\u001b[39;00m self._network_timeout \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m socket_timeout(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m int(self._socket_timeout) \u001b[38;5;28;01mif\u001b[39;00m self._socket_timeout \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _backoff_generator(self) -> Iterator:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._backoff_policy()\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_session_keep_alive(self) -> bool | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._client_session_keep_alive\n",
      "\n",
      "    @client_session_keep_alive.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_session_keep_alive(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._client_session_keep_alive = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_session_keep_alive_heartbeat_frequency(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._client_session_keep_alive_heartbeat_frequency\n",
      "\n",
      "    @client_session_keep_alive_heartbeat_frequency.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_session_keep_alive_heartbeat_frequency(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._client_session_keep_alive_heartbeat_frequency = value\n",
      "        self._validate_client_session_keep_alive_heartbeat_frequency()\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m platform_detection_timeout_seconds(self) -> float | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._platform_detection_timeout_seconds\n",
      "\n",
      "    @platform_detection_timeout_seconds.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m platform_detection_timeout_seconds(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._platform_detection_timeout_seconds = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_prefetch_threads(self) -> int:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m (\n",
      "            self._client_prefetch_threads\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self._client_prefetch_threads\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_CLIENT_PREFETCH_THREADS\n",
      "        )\n",
      "\n",
      "    @client_prefetch_threads.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_prefetch_threads(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._client_prefetch_threads = value\n",
      "        self._validate_client_prefetch_threads()\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_fetch_threads(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._client_fetch_threads\n",
      "\n",
      "    @client_fetch_threads.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_fetch_threads(self, value: \u001b[38;5;28;01mNone\u001b[39;00m | int) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            value = min(max(\u001b[32m1\u001b[39m, value), MAX_CLIENT_FETCH_THREADS)\n",
      "        self._client_fetch_threads = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m client_fetch_use_mp(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._client_fetch_use_mp\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m rest(self) -> SnowflakeRestful | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._rest\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m application(self) -> str:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._application\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m errorhandler(self) -> Callable:  \u001b[38;5;66;03m# TODO: callable args\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._errorhandler\n",
      "\n",
      "    @errorhandler.setter\n",
      "    \u001b[38;5;66;03m# Note: Callable doesn't implement operator|\u001b[39;00m\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m errorhandler(self, value: Callable | \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[33m\"None errorhandler is specified\"\u001b[39m)\n",
      "        self._errorhandler = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m converter_class(self) -> type[SnowflakeConverter]:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._converter_class\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m validate_default_parameters(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._validate_default_parameters\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_pyformat(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._paramstyle \u001b[38;5;28;01min\u001b[39;00m (\u001b[33m\"pyformat\"\u001b[39m, \u001b[33m\"format\"\u001b[39m)\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m consent_cache_id_token(self):\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._consent_cache_id_token\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m telemetry_enabled(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m bool(\n",
      "            self._client_param_telemetry_enabled\n",
      "            \u001b[38;5;28;01mand\u001b[39;00m self._server_param_telemetry_enabled\n",
      "        )\n",
      "\n",
      "    @telemetry_enabled.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m telemetry_enabled(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._client_param_telemetry_enabled = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "            self._client_param_telemetry_enabled\n",
      "            \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._server_param_telemetry_enabled\n",
      "        ):\n",
      "            logger.info(\n",
      "                \u001b[33m\"Telemetry has been disabled by the session parameter CLIENT_TELEMETRY_ENABLED.\"\u001b[39m\n",
      "                \u001b[33m\" Set session parameter CLIENT_TELEMETRY_ENABLED to true to enable telemetry.\"\u001b[39m\n",
      "            )\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m service_name(self) -> str | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._service_name\n",
      "\n",
      "    @service_name.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m service_name(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._service_name = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m log_max_query_length(self) -> int:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._log_max_query_length\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m disable_request_pooling(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._disable_request_pooling\n",
      "\n",
      "    @disable_request_pooling.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m disable_request_pooling(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._disable_request_pooling = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m use_openssl_only(self) -> bool:\n",
      "        \u001b[38;5;66;03m# Deprecated, kept for backwards compatibility\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m arrow_number_to_decimal(self):\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._arrow_number_to_decimal\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m enable_stage_s3_privatelink_for_us_east_1(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._enable_stage_s3_privatelink_for_us_east_1\n",
      "\n",
      "    @enable_stage_s3_privatelink_for_us_east_1.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m enable_stage_s3_privatelink_for_us_east_1(self, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._enable_stage_s3_privatelink_for_us_east_1 = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m enable_connection_diag(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._enable_connection_diag\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m connection_diag_log_path(self):\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._connection_diag_log_path\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m connection_diag_whitelist_path(self):\n",
      "        \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m        Old version of ``connection_diag_allowlist_path``.\u001b[39m\n",
      "\u001b[33m        This used to be the original name, but snowflake backend\u001b[39m\n",
      "\u001b[33m        deprecated whitelist for allowlist. This name will be\u001b[39m\n",
      "\u001b[33m        deprecated in the future.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        warnings.warn(\n",
      "            \u001b[33m\"connection_diag_whitelist_path has been deprecated, use connection_diag_allowlist_path instead\"\u001b[39m,\n",
      "            DeprecationWarning,\n",
      "            stacklevel=\u001b[32m2\u001b[39m,\n",
      "        )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._connection_diag_whitelist_path\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m connection_diag_allowlist_path(self):\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._connection_diag_allowlist_path\n",
      "\n",
      "    @arrow_number_to_decimal.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m arrow_number_to_decimal_setter(self, value: bool) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._arrow_number_to_decimal = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m auth_class(self) -> AuthByPlugin | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._auth_class\n",
      "\n",
      "    @auth_class.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m auth_class(self, value: AuthByPlugin) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(value, AuthByPlugin):\n",
      "            self._auth_class = value\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\u001b[33m\"auth_class must subclass AuthByPlugin\"\u001b[39m)\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_query_context_cache_disabled(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._disable_query_context_cache\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m iobound_tpe_limit(self) -> int | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._iobound_tpe_limit\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m unsafe_file_write(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._unsafe_file_write\n",
      "\n",
      "    @unsafe_file_write.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m unsafe_file_write(self, value: bool) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        self._unsafe_file_write = value\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m check_arrow_conversion_error_on_every_column(self) -> bool:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._check_arrow_conversion_error_on_every_column\n",
      "\n",
      "    @cached_property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m snowflake_version(self) -> str:\n",
      "        \u001b[38;5;66;03m# The result from SELECT CURRENT_VERSION() is `<version> <internal hash>`,\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# and we only need the first part\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m str(\n",
      "            self.cursor().execute(\u001b[33m\"SELECT CURRENT_VERSION()\"\u001b[39m).fetchall()[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "        ).split(\u001b[33m\" \"\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "\n",
      "    @check_arrow_conversion_error_on_every_column.setter\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m check_arrow_conversion_error_on_every_column(self, value: bool) -> bool:\n",
      "        self._check_arrow_conversion_error_on_every_column = value\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m connect(self, **kwargs) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Establishes connection to Snowflake.\"\"\"\u001b[39m\n",
      "        logger.debug(\u001b[33m\"connect\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m len(kwargs) > \u001b[32m0\u001b[39m:\n",
      "            self.__config(**kwargs)\n",
      "\n",
      "        self._crl_config: CRLConfig = CRLConfig.from_connection(self)\n",
      "\n",
      "        no_proxy_csv_str = (\n",
      "            \u001b[33m\",\"\u001b[39m.join(str(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m self.no_proxy)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                self.no_proxy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m isinstance(self.no_proxy, Iterable)\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(self.no_proxy, (str, bytes))\n",
      "            )\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m self.no_proxy\n",
      "        )\n",
      "        self._http_config = HttpConfig(\n",
      "            adapter_factory=ProxySupportAdapterFactory(),\n",
      "            use_pooling=(\u001b[38;5;28;01mnot\u001b[39;00m self.disable_request_pooling),\n",
      "            proxy_host=self.proxy_host,\n",
      "            proxy_port=self.proxy_port,\n",
      "            proxy_user=self.proxy_user,\n",
      "            proxy_password=self.proxy_password,\n",
      "            no_proxy=no_proxy_csv_str,\n",
      "        )\n",
      "        self._session_manager = SessionManagerFactory.get_manager(self._http_config)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.enable_connection_diag:\n",
      "            exceptions_dict = {}\n",
      "            connection_diag = ConnectionDiagnostic(\n",
      "                account=self.account,\n",
      "                host=self.host,\n",
      "                connection_diag_log_path=self.connection_diag_log_path,\n",
      "                connection_diag_allowlist_path=(\n",
      "                    self.connection_diag_allowlist_path\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m self.connection_diag_allowlist_path \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                    \u001b[38;5;28;01melse\u001b[39;00m self.connection_diag_whitelist_path\n",
      "                ),\n",
      "                proxy_host=self.proxy_host,\n",
      "                proxy_port=self.proxy_port,\n",
      "                proxy_user=self.proxy_user,\n",
      "                proxy_password=self.proxy_password,\n",
      "                session_manager=self._session_manager.clone(use_pooling=\u001b[38;5;28;01mFalse\u001b[39;00m),\n",
      "            )\n",
      "            \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "                connection_diag.run_test()\n",
      "                self.__open_connection()\n",
      "                connection_diag.cursor = self.cursor()\n",
      "            \u001b[38;5;28;01mexcept\u001b[39;00m Exception:\n",
      "                exceptions_dict[\u001b[33m\"connection_test\"\u001b[39m] = traceback.format_exc()\n",
      "                logger.warning(\n",
      "                    f\"\"\"Exception during connection test:\\n{exceptions_dict[\u001b[33m\"connection_test\"\u001b[39m]} \"\"\"\n",
      "                )\n",
      "            \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "                connection_diag.run_post_test()\n",
      "            \u001b[38;5;28;01mexcept\u001b[39;00m Exception:\n",
      "                exceptions_dict[\u001b[33m\"post_test\"\u001b[39m] = traceback.format_exc()\n",
      "                logger.warning(\n",
      "                    f\"\"\"Exception during post connection test:\\n{exceptions_dict[\u001b[33m\"post_test\"\u001b[39m]} \"\"\"\n",
      "                )\n",
      "            \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "                connection_diag.generate_report()\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m exceptions_dict:\n",
      "                    \u001b[38;5;28;01mraise\u001b[39;00m Exception(str(exceptions_dict))\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            self.__open_connection()\n",
      "\n",
      "        \u001b[38;5;66;03m# Register the connection in the pool after successful connection\u001b[39;00m\n",
      "        _connections_registry.add_connection(self)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m close(self, retry: bool = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Closes the connection.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;66;03m# unregister to dereference connection object as it's already closed after the execution\u001b[39;00m\n",
      "        atexit.unregister(self._close_at_exit)\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# Remove connection from the pool\u001b[39;00m\n",
      "            _connections_registry.remove_connection(self)\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.rest:\n",
      "                logger.debug(\u001b[33m\"Rest object has been destroyed, cannot close session\"\u001b[39m)\n",
      "                \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\n",
      "            \u001b[38;5;66;03m# will hang if the application doesn't close the connection and\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# CLIENT_SESSION_KEEP_ALIVE is set, because the heartbeat runs on\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# a separate thread.\u001b[39;00m\n",
      "            self._cancel_heartbeat()\n",
      "\n",
      "            \u001b[38;5;66;03m# close telemetry first, since it needs rest to send remaining data\u001b[39;00m\n",
      "            logger.debug(\u001b[33m\"closed\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.telemetry_enabled:\n",
      "                self._telemetry.close(retry=retry)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                self._all_async_queries_finished()\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._server_session_keep_alive\n",
      "            ):\n",
      "                logger.debug(\u001b[33m\"No async queries seem to be running, deleting session\"\u001b[39m)\n",
      "                self.rest.delete_session(retry=retry)\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                logger.debug(\n",
      "                    \u001b[33m\"There are {} async queries still running, not deleting session\"\u001b[39m.format(\n",
      "                        len(self._async_sfqids)\n",
      "                    )\n",
      "                )\n",
      "            self.rest.close()\n",
      "            self._rest = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.query_context_cache:\n",
      "                self.query_context_cache.clear_cache()\n",
      "            \u001b[38;5;28;01mdel\u001b[39;00m self.messages[:]\n",
      "            logger.debug(\u001b[33m\"Session is closed\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            logger.debug(\n",
      "                \u001b[33m\"Exception encountered in closing connection. ignoring...: %s\"\u001b[39m, e\n",
      "            )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_closed(self) -> bool:\n",
      "        \u001b[33m\"\"\"Checks whether the connection has been closed.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self.rest \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m autocommit(self, mode) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Sets autocommit mode to True, or False. Defaults to True.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.rest:\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                DatabaseError,\n",
      "                {\n",
      "                    \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Connection is closed\"\u001b[39m,\n",
      "                    \u001b[33m\"errno\"\u001b[39m: ER_CONNECTION_IS_CLOSED,\n",
      "                    \u001b[33m\"sqlstate\"\u001b[39m: SQLSTATE_CONNECTION_NOT_EXISTS,\n",
      "                },\n",
      "            )\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(mode, bool):\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                ProgrammingError,\n",
      "                {\n",
      "                    \u001b[33m\"msg\"\u001b[39m: f\"Invalid parameter: {mode}\",\n",
      "                    \u001b[33m\"errno\"\u001b[39m: ER_INVALID_VALUE,\n",
      "                },\n",
      "            )\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            self.cursor().execute(f\"ALTER SESSION SET autocommit={mode}\")\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m Error \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m e.sqlstate == SQLSTATE_FEATURE_NOT_SUPPORTED:\n",
      "                logger.debug(\n",
      "                    \u001b[33m\"Autocommit feature is not enabled for this \"\u001b[39m \u001b[33m\"connection. Ignored\"\u001b[39m\n",
      "                )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m commit(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Commits the current transaction.\"\"\"\u001b[39m\n",
      "        self.cursor().execute(\u001b[33m\"COMMIT\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m rollback(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Rolls back the current transaction.\"\"\"\u001b[39m\n",
      "        self.cursor().execute(\u001b[33m\"ROLLBACK\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m cursor(self, cursor_class: type[CursorCls] = SnowflakeCursor) -> CursorCls:\n",
      "        \u001b[33m\"\"\"Creates a cursor object. Each statement will be executed in a new cursor object.\"\"\"\u001b[39m\n",
      "        logger.debug(\u001b[33m\"cursor\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.rest:\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                DatabaseError,\n",
      "                {\n",
      "                    \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Connection is closed\"\u001b[39m,\n",
      "                    \u001b[33m\"errno\"\u001b[39m: ER_CONNECTION_IS_CLOSED,\n",
      "                    \u001b[33m\"sqlstate\"\u001b[39m: SQLSTATE_CONNECTION_NOT_EXISTS,\n",
      "                },\n",
      "            )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m cursor_class(self)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m execute_string(\n",
      "        self,\n",
      "        sql_text: str,\n",
      "        remove_comments: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        return_cursors: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        cursor_class: SnowflakeCursor = SnowflakeCursor,\n",
      "        **kwargs,\n",
      "    ) -> Iterable[SnowflakeCursor]:\n",
      "        \u001b[33m\"\"\"Executes a SQL text including multiple statements. This is a non-standard convenience method.\"\"\"\u001b[39m\n",
      "        stream = StringIO(sql_text)\n",
      "        stream_generator = self.execute_stream(\n",
      "            stream, remove_comments=remove_comments, cursor_class=cursor_class, **kwargs\n",
      "        )\n",
      "        ret = list(stream_generator)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m ret \u001b[38;5;28;01mif\u001b[39;00m return_cursors \u001b[38;5;28;01melse\u001b[39;00m list()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m execute_stream(\n",
      "        self,\n",
      "        stream: StringIO,\n",
      "        remove_comments: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        cursor_class: SnowflakeCursor = SnowflakeCursor,\n",
      "        **kwargs,\n",
      "    ) -> Generator[SnowflakeCursor]:\n",
      "        \u001b[33m\"\"\"Executes a stream of SQL statements. This is a non-standard convenient method.\"\"\"\u001b[39m\n",
      "        split_statements_list = split_statements(\n",
      "            stream, remove_comments=remove_comments\n",
      "        )\n",
      "        \u001b[38;5;66;03m# Note: split_statements_list is a list of tuples of sql statements and whether they are put/get\u001b[39;00m\n",
      "        non_empty_statements = [e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;28;01min\u001b[39;00m split_statements_list \u001b[38;5;28;01mif\u001b[39;00m e[\u001b[32m0\u001b[39m]]\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m sql, is_put_or_get \u001b[38;5;28;01min\u001b[39;00m non_empty_statements:\n",
      "            cur = self.cursor(cursor_class=cursor_class)\n",
      "            cur.execute(sql, _is_put_get=is_put_or_get, **kwargs)\n",
      "            \u001b[38;5;28;01myield\u001b[39;00m cur\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __set_error_attributes(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;28;01min\u001b[39;00m [\n",
      "            method \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;28;01min\u001b[39;00m dir(errors) \u001b[38;5;28;01mif\u001b[39;00m callable(getattr(errors, method))\n",
      "        ]:\n",
      "            \u001b[38;5;66;03m# If name starts with _ then ignore that\u001b[39;00m\n",
      "            name = m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m m.startswith(\u001b[33m\"_\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m m[\u001b[32m1\u001b[39m:]\n",
      "            setattr(self, name, getattr(errors, m))\n",
      "\n",
      "    @staticmethod\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m setup_ocsp_privatelink(app, hostname) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        hostname = hostname.lower()\n",
      "        SnowflakeConnection.OCSP_ENV_LOCK.acquire()\n",
      "        ocsp_cache_server = f\"http://ocsp.{hostname}/ocsp_response_cache.json\"\n",
      "        os.environ[\u001b[33m\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\"\u001b[39m] = ocsp_cache_server\n",
      "        logger.debug(\u001b[33m\"OCSP Cache Server is updated: %s\"\u001b[39m, ocsp_cache_server)\n",
      "        SnowflakeConnection.OCSP_ENV_LOCK.release()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __open_connection(self):\n",
      "        \u001b[33m\"\"\"Opens a new network connection.\"\"\"\u001b[39m\n",
      "        self.converter = self._converter_class(\n",
      "            use_numpy=self._numpy, support_negative_year=self._support_negative_year\n",
      "        )\n",
      "\n",
      "        self._rest = SnowflakeRestful(\n",
      "            host=self.host,\n",
      "            port=self.port,\n",
      "            protocol=self._protocol,\n",
      "            inject_client_pause=self._inject_client_pause,\n",
      "            connection=self,\n",
      "            session_manager=self._session_manager,  \u001b[38;5;66;03m# connection shares the session pool used for making Backend related requests\u001b[39;00m\n",
      "        )\n",
      "        logger.debug(\u001b[33m\"REST API object was created: %s:%s\"\u001b[39m, self.host, self.port)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m os.environ:\n",
      "            logger.debug(\n",
      "                \u001b[33m\"Custom OCSP Cache Server URL found in environment - %s\"\u001b[39m,\n",
      "                os.environ[\u001b[33m\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\"\u001b[39m],\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\".privatelink.snowflakecomputing.\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m self.host.lower():\n",
      "            SnowflakeConnection.setup_ocsp_privatelink(self.application, self.host)\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m os.environ:\n",
      "                \u001b[38;5;28;01mdel\u001b[39;00m os.environ[\u001b[33m\"SF_OCSP_RESPONSE_CACHE_SERVER_URL\"\u001b[39m]\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._session_parameters \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self._session_parameters = {}\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._autocommit \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self._session_parameters[PARAMETER_AUTOCOMMIT] = self._autocommit\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._timezone \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self._session_parameters[PARAMETER_TIMEZONE] = self._timezone\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._validate_default_parameters:\n",
      "            \u001b[38;5;66;03m# Snowflake will validate the requested database, schema, and warehouse\u001b[39;00m\n",
      "            self._session_parameters[PARAMETER_CLIENT_VALIDATE_DEFAULT_PARAMETERS] = (\n",
      "                \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.client_session_keep_alive \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self._session_parameters[PARAMETER_CLIENT_SESSION_KEEP_ALIVE] = (\n",
      "                self._client_session_keep_alive\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.client_session_keep_alive_heartbeat_frequency \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self._session_parameters[\n",
      "                PARAMETER_CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY\n",
      "            ] = self._validate_client_session_keep_alive_heartbeat_frequency()\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.client_prefetch_threads:\n",
      "            self._session_parameters[PARAMETER_CLIENT_PREFETCH_THREADS] = (\n",
      "                self._validate_client_prefetch_threads()\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;66;03m# Setup authenticator\u001b[39;00m\n",
      "        auth = Auth(self.rest)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._session_token \u001b[38;5;28;01mand\u001b[39;00m self._master_token:\n",
      "            auth._rest.update_tokens(\n",
      "                self._session_token,\n",
      "                self._master_token,\n",
      "                self._master_validity_in_seconds,\n",
      "            )\n",
      "            heartbeat_ret = auth._rest._heartbeat()\n",
      "            logger.debug(heartbeat_ret)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m heartbeat_ret \u001b[38;5;28;01mor\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m heartbeat_ret.get(\u001b[33m\"success\"\u001b[39m):\n",
      "                Error.errorhandler_wrapper(\n",
      "                    self,\n",
      "                    \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                    ProgrammingError,\n",
      "                    {\n",
      "                        \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Session and master tokens invalid\"\u001b[39m,\n",
      "                        \u001b[33m\"errno\"\u001b[39m: ER_INVALID_VALUE,\n",
      "                    },\n",
      "                )\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                logger.debug(\u001b[33m\"Session and master token validation successful.\"\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.auth_class \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m type(\n",
      "                    self.auth_class\n",
      "                ) \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m FIRST_PARTY_AUTHENTICATORS \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m issubclass(\n",
      "                    type(self.auth_class), AuthByKeyPair\n",
      "                ):\n",
      "                    \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\u001b[33m\"auth_class must be a child class of AuthByKeyPair\"\u001b[39m)\n",
      "                    \u001b[38;5;66;03m# TODO: add telemetry for custom auth\u001b[39;00m\n",
      "                self.auth_class = self.auth_class\n",
      "            \u001b[38;5;66;03m# match authentivator - validation happens in __config\u001b[39;00m\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == DEFAULT_AUTHENTICATOR:\n",
      "                self.auth_class = AuthByDefault(\n",
      "                    password=self._password,\n",
      "                    timeout=self.login_timeout,\n",
      "                    backoff_generator=self._backoff_generator,\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == EXTERNAL_BROWSER_AUTHENTICATOR:\n",
      "                \u001b[38;5;66;03m# Enable SSO credential caching\u001b[39;00m\n",
      "                self._session_parameters[\n",
      "                    PARAMETER_CLIENT_STORE_TEMPORARY_CREDENTIAL\n",
      "                ] = (self._client_store_temporary_credential \u001b[38;5;28;01mif\u001b[39;00m IS_LINUX \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "                \u001b[38;5;66;03m# Try to load cached ID token to avoid browser popup\u001b[39;00m\n",
      "                auth.read_temporary_credentials(\n",
      "                    self.host,\n",
      "                    self.user,\n",
      "                    self._session_parameters,\n",
      "                )\n",
      "                \u001b[38;5;66;03m# Depending on whether self._rest.id_token is available we do different\u001b[39;00m\n",
      "                \u001b[38;5;66;03m#  auth_instance\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self._rest.id_token \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    self.auth_class = AuthByWebBrowser(\n",
      "                        application=self.application,\n",
      "                        protocol=self._protocol,\n",
      "                        host=self.host,  \u001b[38;5;66;03m# TODO: delete this?\u001b[39;00m\n",
      "                        port=self.port,\n",
      "                        timeout=self.login_timeout,\n",
      "                        backoff_generator=self._backoff_generator,\n",
      "                    )\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    self.auth_class = AuthByIdToken(\n",
      "                        id_token=self._rest.id_token,\n",
      "                        application=self.application,\n",
      "                        protocol=self._protocol,\n",
      "                        host=self.host,\n",
      "                        port=self.port,\n",
      "                        timeout=self.login_timeout,\n",
      "                        backoff_generator=self._backoff_generator,\n",
      "                    )\n",
      "\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == KEY_PAIR_AUTHENTICATOR:\n",
      "                private_key = self._private_key\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self._private_key_file:\n",
      "                    private_key = _get_private_bytes_from_file(\n",
      "                        self._private_key_file,\n",
      "                        self._private_key_file_pwd,\n",
      "                    )\n",
      "\n",
      "                self.auth_class = AuthByKeyPair(\n",
      "                    private_key=private_key,\n",
      "                    timeout=self.login_timeout,\n",
      "                    backoff_generator=self._backoff_generator,\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == OAUTH_AUTHENTICATOR:\n",
      "                self.auth_class = AuthByOAuth(\n",
      "                    oauth_token=self._token,\n",
      "                    timeout=self.login_timeout,\n",
      "                    backoff_generator=self._backoff_generator,\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == OAUTH_AUTHORIZATION_CODE:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self._role \u001b[38;5;28;01mand\u001b[39;00m (self._oauth_scope == \u001b[33m\"\"\u001b[39m):\n",
      "                    \u001b[38;5;66;03m# if role is known then let's inject it into scope\u001b[39;00m\n",
      "                    self._oauth_scope = _OAUTH_DEFAULT_SCOPE.format(role=self._role)\n",
      "                self.auth_class = AuthByOauthCode(\n",
      "                    application=self.application,\n",
      "                    client_id=self._oauth_client_id,\n",
      "                    client_secret=self._oauth_client_secret,\n",
      "                    host=self.host,\n",
      "                    authentication_url=self._oauth_authorization_url.format(\n",
      "                        host=self.host, port=self.port\n",
      "                    ),\n",
      "                    token_request_url=self._oauth_token_request_url.format(\n",
      "                        host=self.host, port=self.port\n",
      "                    ),\n",
      "                    redirect_uri=self._oauth_redirect_uri,\n",
      "                    uri=self._oauth_socket_uri,\n",
      "                    scope=self._oauth_scope,\n",
      "                    pkce_enabled=\u001b[38;5;28;01mnot\u001b[39;00m self._oauth_disable_pkce,\n",
      "                    token_cache=(\n",
      "                        auth.get_token_cache()\n",
      "                        \u001b[38;5;28;01mif\u001b[39;00m self._client_store_temporary_credential\n",
      "                        \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                    ),\n",
      "                    refresh_token_enabled=self._oauth_enable_refresh_tokens,\n",
      "                    external_browser_timeout=self._external_browser_timeout,\n",
      "                    enable_single_use_refresh_tokens=self._oauth_enable_single_use_refresh_tokens,\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == OAUTH_CLIENT_CREDENTIALS:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self._role \u001b[38;5;28;01mand\u001b[39;00m (self._oauth_scope == \u001b[33m\"\"\u001b[39m):\n",
      "                    \u001b[38;5;66;03m# if role is known then let's inject it into scope\u001b[39;00m\n",
      "                    self._oauth_scope = _OAUTH_DEFAULT_SCOPE.format(role=self._role)\n",
      "                self.auth_class = AuthByOauthCredentials(\n",
      "                    application=self.application,\n",
      "                    client_id=self._oauth_client_id,\n",
      "                    client_secret=self._oauth_client_secret,\n",
      "                    token_request_url=self._oauth_token_request_url.format(\n",
      "                        host=self.host, port=self.port\n",
      "                    ),\n",
      "                    scope=self._oauth_scope,\n",
      "                    credentials_in_body=self._oauth_credentials_in_body,\n",
      "                    connection=self,\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == USR_PWD_MFA_AUTHENTICATOR:\n",
      "                \u001b[38;5;66;03m# Enable MFA token caching\u001b[39;00m\n",
      "                self._session_parameters[PARAMETER_CLIENT_REQUEST_MFA_TOKEN] = (\n",
      "                    self._client_request_mfa_token \u001b[38;5;28;01mif\u001b[39;00m IS_LINUX \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "                )\n",
      "                \u001b[38;5;66;03m# Try to load cached MFA token to skip MFA prompt\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self._session_parameters[PARAMETER_CLIENT_REQUEST_MFA_TOKEN]:\n",
      "                    auth.read_temporary_credentials(\n",
      "                        self.host,\n",
      "                        self.user,\n",
      "                        self._session_parameters,\n",
      "                    )\n",
      "                self.auth_class = AuthByUsrPwdMfa(\n",
      "                    password=self._password,\n",
      "                    mfa_token=self.rest.mfa_token,\n",
      "                    timeout=self.login_timeout,\n",
      "                    backoff_generator=self._backoff_generator,\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == PROGRAMMATIC_ACCESS_TOKEN:\n",
      "                self.auth_class = AuthByPAT(self._token)\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == PAT_WITH_EXTERNAL_SESSION:\n",
      "                \u001b[38;5;66;03m# We don't need to do a POST to /v1/login-request to get session and master tokens at the startup\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# time. PAT with external (Spark) session ID creates a new session when it encounters the unique\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# (PAT, external session ID) combination for the first time and then onwards use the (PAT, external\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# session id) as a key to identify and authenticate the session. So we bypass actual AuthN here.\u001b[39;00m\n",
      "                self.auth_class = AuthNoAuth()\n",
      "                self._rest.set_pat_and_external_session(\n",
      "                    self._token, self._external_session_id\n",
      "                )\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m self._authenticator == WORKLOAD_IDENTITY_AUTHENTICATOR:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m isinstance(self._workload_identity_provider, str):\n",
      "                    self._workload_identity_provider = AttestationProvider.from_string(\n",
      "                        self._workload_identity_provider\n",
      "                    )\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._workload_identity_provider:\n",
      "                    Error.errorhandler_wrapper(\n",
      "                        self,\n",
      "                        \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                        ProgrammingError,\n",
      "                        {\n",
      "                            \u001b[33m\"msg\"\u001b[39m: f\"workload_identity_provider must be set to one of {\u001b[33m','\u001b[39m.join(AttestationProvider.all_string_values())} when authenticator is WORKLOAD_IDENTITY.\",\n",
      "                            \u001b[33m\"errno\"\u001b[39m: ER_INVALID_WIF_SETTINGS,\n",
      "                        },\n",
      "                    )\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                    self._workload_identity_impersonation_path\n",
      "                    \u001b[38;5;28;01mand\u001b[39;00m self._workload_identity_provider\n",
      "                    \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m (\n",
      "                        AttestationProvider.GCP,\n",
      "                        AttestationProvider.AWS,\n",
      "                    )\n",
      "                ):\n",
      "                    Error.errorhandler_wrapper(\n",
      "                        self,\n",
      "                        \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                        ProgrammingError,\n",
      "                        {\n",
      "                            \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"workload_identity_impersonation_path is currently only supported for GCP and AWS.\"\u001b[39m,\n",
      "                            \u001b[33m\"errno\"\u001b[39m: ER_INVALID_WIF_SETTINGS,\n",
      "                        },\n",
      "                    )\n",
      "                self.auth_class = AuthByWorkloadIdentity(\n",
      "                    provider=self._workload_identity_provider,\n",
      "                    token=self._token,\n",
      "                    entra_resource=self._workload_identity_entra_resource,\n",
      "                    impersonation_path=self._workload_identity_impersonation_path,\n",
      "                )\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;66;03m# okta URL, e.g., https://<account>.okta.com/\u001b[39;00m\n",
      "                self.auth_class = AuthByOkta(\n",
      "                    application=self.application,\n",
      "                    timeout=self.login_timeout,\n",
      "                    backoff_generator=self._backoff_generator,\n",
      "                )\n",
      "\n",
      "            self.authenticate_with_retry(self.auth_class)\n",
      "\n",
      "            self._password = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ensure password won't persist\u001b[39;00m\n",
      "            self.auth_class.reset_secrets()\n",
      "\n",
      "        self.initialize_query_context_cache()\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.client_session_keep_alive:\n",
      "            \u001b[38;5;66;03m# This will be called after the heartbeat frequency has actually been set.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# By this point it should have been decided if the heartbeat has to be enabled\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# and what would the heartbeat frequency be\u001b[39;00m\n",
      "            self._add_heartbeat()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __config(self, **kwargs):\n",
      "        \u001b[33m\"\"\"Sets up parameters in the connection object.\"\"\"\u001b[39m\n",
      "        logger.debug(\u001b[33m\"__config\"\u001b[39m)\n",
      "        \u001b[38;5;66;03m# Handle special cases first\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"sequence_counter\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            self.sequence_counter = kwargs[\u001b[33m\"sequence_counter\"\u001b[39m]\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"application\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            value = kwargs[\u001b[33m\"application\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m APPLICATION_RE.match(value):\n",
      "                msg = f\"Invalid application name: {value}\"\n",
      "                \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(msg=msg, errno=\u001b[32m0\u001b[39m)\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                self._application = value\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"validate_default_parameters\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            self._validate_default_parameters = kwargs[\u001b[33m\"validate_default_parameters\"\u001b[39m]\n",
      "        \u001b[38;5;66;03m# Handle rest of arguments\u001b[39;00m\n",
      "        skip_list = [\u001b[33m\"validate_default_parameters\"\u001b[39m, \u001b[33m\"sequence_counter\"\u001b[39m, \u001b[33m\"application\"\u001b[39m]\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;28;01min\u001b[39;00m filter(\u001b[38;5;28;01mlambda\u001b[39;00m e: e[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m skip_list, kwargs.items()):\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self.validate_default_parameters:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m DEFAULT_CONFIGURATION.keys():\n",
      "                    close_matches = get_close_matches(\n",
      "                        name, DEFAULT_CONFIGURATION.keys(), n=\u001b[32m1\u001b[39m, cutoff=\u001b[32m0.8\u001b[39m\n",
      "                    )\n",
      "                    guess = close_matches[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m len(close_matches) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                    warnings.warn(\n",
      "                        \u001b[33m\"'{}' is an unknown connection parameter{}\"\u001b[39m.format(\n",
      "                            name, f\", did you mean '{guess}'?\" \u001b[38;5;28;01mif\u001b[39;00m guess \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\"\u001b[39m\n",
      "                        ),\n",
      "                        \u001b[38;5;66;03m# Raise warning from where class was initiated\u001b[39;00m\n",
      "                        stacklevel=\u001b[32m4\u001b[39m,\n",
      "                    )\n",
      "                \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(value, DEFAULT_CONFIGURATION[name][\u001b[32m1\u001b[39m]):\n",
      "                    accepted_types = DEFAULT_CONFIGURATION[name][\u001b[32m1\u001b[39m]\n",
      "                    warnings.warn(\n",
      "                        \u001b[33m\"'{}' connection parameter should be of type '{}', but is a '{}'\"\u001b[39m.format(\n",
      "                            name,\n",
      "                            (\n",
      "                                str(tuple(e.__name__ \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;28;01min\u001b[39;00m accepted_types)).replace(\n",
      "                                    \u001b[33m\"'\"\u001b[39m, \u001b[33m\"\"\u001b[39m\n",
      "                                )\n",
      "                                \u001b[38;5;28;01mif\u001b[39;00m isinstance(accepted_types, tuple)\n",
      "                                \u001b[38;5;28;01melse\u001b[39;00m accepted_types.__name__\n",
      "                            ),\n",
      "                            type(value).__name__,\n",
      "                        ),\n",
      "                        \u001b[38;5;66;03m# Raise warning from where class was initiated\u001b[39;00m\n",
      "                        stacklevel=\u001b[32m4\u001b[39m,\n",
      "                    )\n",
      "            setattr(self, \u001b[33m\"_\"\u001b[39m + name, value)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._numpy:\n",
      "            \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "                \u001b[38;5;28;01mimport\u001b[39;00m numpy  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "            \u001b[38;5;28;01mexcept\u001b[39;00m ModuleNotFoundError:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "                Error.errorhandler_wrapper(\n",
      "                    self,\n",
      "                    \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                    ProgrammingError,\n",
      "                    {\n",
      "                        \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Numpy module is not installed. Cannot fetch data as numpy\"\u001b[39m,\n",
      "                        \u001b[33m\"errno\"\u001b[39m: ER_NO_NUMPY,\n",
      "                    },\n",
      "                )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._paramstyle \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mimport\u001b[39;00m snowflake.connector\n",
      "\n",
      "            self._paramstyle = snowflake.connector.paramstyle\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self._paramstyle \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m SUPPORTED_PARAMSTYLES:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\n",
      "                msg=\u001b[33m\"Invalid paramstyle is specified\"\u001b[39m, errno=ER_INVALID_VALUE\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._auth_class \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(self._auth_class, AuthByPlugin):\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\u001b[33m\"auth_class must subclass AuthByPlugin\"\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"account\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"host\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "                self._host = construct_hostname(kwargs.get(\u001b[33m\"region\"\u001b[39m), self._account)\n",
      "\n",
      "        logger.info(\n",
      "            f\"Connecting to {_DOMAIN_NAME_MAP.get(extract_top_level_domain_from_hostname(self._host), \u001b[33m'GLOBAL'\u001b[39m)} Snowflake domain\"\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;66;03m# If using a custom auth class, we should set the authenticator\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# type to be the same as the custom auth class\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._auth_class:\n",
      "            self._authenticator = self._auth_class.type_.value\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self._authenticator:\n",
      "            \u001b[38;5;66;03m# Validate authenticator and convert it to uppercase if it is a non-okta link\u001b[39;00m\n",
      "            auth_tmp = self._authenticator.upper()\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m auth_tmp \u001b[38;5;28;01min\u001b[39;00m [\n",
      "                DEFAULT_AUTHENTICATOR,\n",
      "                EXTERNAL_BROWSER_AUTHENTICATOR,\n",
      "                KEY_PAIR_AUTHENTICATOR,\n",
      "                OAUTH_AUTHENTICATOR,\n",
      "                OAUTH_AUTHORIZATION_CODE,\n",
      "                OAUTH_CLIENT_CREDENTIALS,\n",
      "                USR_PWD_MFA_AUTHENTICATOR,\n",
      "                WORKLOAD_IDENTITY_AUTHENTICATOR,\n",
      "                PROGRAMMATIC_ACCESS_TOKEN,\n",
      "                PAT_WITH_EXTERNAL_SESSION,\n",
      "            ]:\n",
      "                self._authenticator = auth_tmp\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m auth_tmp.startswith(\u001b[33m\"HTTPS://\"\u001b[39m):\n",
      "                \u001b[38;5;66;03m# okta authenticator link\u001b[39;00m\n",
      "                \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\n",
      "                    msg=f\"Unknown authenticator: {self._authenticator}\",\n",
      "                    errno=ER_INVALID_VALUE,\n",
      "                )\n",
      "\n",
      "        \u001b[38;5;66;03m# read OAuth token from\u001b[39;00m\n",
      "        token_file_path = kwargs.get(\u001b[33m\"token_file_path\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m token_file_path:\n",
      "            \u001b[38;5;28;01mwith\u001b[39;00m open(token_file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "                self._token = f.read()\n",
      "\n",
      "        \u001b[38;5;66;03m# Set of authenticators allowing empty user.\u001b[39;00m\n",
      "        empty_user_allowed_authenticators = {\n",
      "            OAUTH_AUTHENTICATOR,\n",
      "            NO_AUTH_AUTHENTICATOR,\n",
      "            WORKLOAD_IDENTITY_AUTHENTICATOR,\n",
      "            PROGRAMMATIC_ACCESS_TOKEN,\n",
      "            PAT_WITH_EXTERNAL_SESSION,\n",
      "            OAUTH_AUTHORIZATION_CODE,\n",
      "            OAUTH_CLIENT_CREDENTIALS,\n",
      "        }\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m (self._master_token \u001b[38;5;28;01mand\u001b[39;00m self._session_token):\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                \u001b[38;5;28;01mnot\u001b[39;00m self.user\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m self._authenticator \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m empty_user_allowed_authenticators\n",
      "            ):\n",
      "                \u001b[38;5;66;03m# Some authenticators do not require a username\u001b[39;00m\n",
      "                Error.errorhandler_wrapper(\n",
      "                    self,\n",
      "                    \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                    ProgrammingError,\n",
      "                    {\n",
      "                        \u001b[33m\"msg\"\u001b[39m: f\"User is empty, but it must be provided unless authenticator is one of {\u001b[33m', '\u001b[39m.join(empty_user_allowed_authenticators)}.\",\n",
      "                        \u001b[33m\"errno\"\u001b[39m: ER_NO_USER,\n",
      "                    },\n",
      "                )\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self._private_key \u001b[38;5;28;01mor\u001b[39;00m self._private_key_file:\n",
      "                self._authenticator = KEY_PAIR_AUTHENTICATOR\n",
      "\n",
      "            workload_identity_dependent_options = [\n",
      "                \u001b[33m\"workload_identity_provider\"\u001b[39m,\n",
      "                \u001b[33m\"workload_identity_entra_resource\"\u001b[39m,\n",
      "                \u001b[33m\"workload_identity_impersonation_path\"\u001b[39m,\n",
      "            ]\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m dependent_option \u001b[38;5;28;01min\u001b[39;00m workload_identity_dependent_options:\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                    self.__getattribute__(f\"_{dependent_option}\") \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                    \u001b[38;5;28;01mand\u001b[39;00m self._authenticator != WORKLOAD_IDENTITY_AUTHENTICATOR\n",
      "                ):\n",
      "                    Error.errorhandler_wrapper(\n",
      "                        self,\n",
      "                        \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                        ProgrammingError,\n",
      "                        {\n",
      "                            \u001b[33m\"msg\"\u001b[39m: f\"{dependent_option} was set but authenticator was not set to {WORKLOAD_IDENTITY_AUTHENTICATOR}\",\n",
      "                            \u001b[33m\"errno\"\u001b[39m: ER_INVALID_WIF_SETTINGS,\n",
      "                        },\n",
      "                    )\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "                self.auth_class \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m self._authenticator\n",
      "                \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m (\n",
      "                    EXTERNAL_BROWSER_AUTHENTICATOR,\n",
      "                    OAUTH_AUTHENTICATOR,\n",
      "                    OAUTH_AUTHORIZATION_CODE,\n",
      "                    OAUTH_CLIENT_CREDENTIALS,\n",
      "                    KEY_PAIR_AUTHENTICATOR,\n",
      "                    PROGRAMMATIC_ACCESS_TOKEN,\n",
      "                    WORKLOAD_IDENTITY_AUTHENTICATOR,\n",
      "                    PAT_WITH_EXTERNAL_SESSION,\n",
      "                )\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._password\n",
      "            ):\n",
      "                Error.errorhandler_wrapper(\n",
      "                    self,\n",
      "                    \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                    ProgrammingError,\n",
      "                    {\u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Password is empty\"\u001b[39m, \u001b[33m\"errno\"\u001b[39m: ER_NO_PASSWORD},\n",
      "                )\n",
      "\n",
      "        \u001b[38;5;66;03m# Only AuthNoAuth allows account to be omitted.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._account \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(self.auth_class, AuthNoAuth):\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                ProgrammingError,\n",
      "                {\u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Account must be specified\"\u001b[39m, \u001b[33m\"errno\"\u001b[39m: ER_NO_ACCOUNT_NAME},\n",
      "            )\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._account \u001b[38;5;28;01mand\u001b[39;00m \u001b[33m\".\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m self._account:\n",
      "            self._account = parse_account(self._account)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(self._backoff_policy, Callable) \u001b[38;5;28;01mor\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(\n",
      "            self._backoff_policy(), Iterator\n",
      "        ):\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                ProgrammingError,\n",
      "                {\n",
      "                    \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Backoff policy must be a generator function\"\u001b[39m,\n",
      "                    \u001b[33m\"errno\"\u001b[39m: ER_INVALID_BACKOFF_POLICY,\n",
      "                },\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.ocsp_fail_open:\n",
      "            logger.debug(\n",
      "                \u001b[33m\"This connection is in OCSP Fail Open Mode. \"\u001b[39m\n",
      "                \u001b[33m\"TLS Certificates would be checked for validity \"\u001b[39m\n",
      "                \u001b[33m\"and revocation status. Any other Certificate \"\u001b[39m\n",
      "                \u001b[33m\"Revocation related exceptions or OCSP Responder \"\u001b[39m\n",
      "                \u001b[33m\"failures would be disregarded in favor of \"\u001b[39m\n",
      "                \u001b[33m\"connectivity.\"\u001b[39m\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.disable_ocsp_checks:\n",
      "            logger.debug(\n",
      "                \u001b[33m\"This connection runs with disabled OCSP checks. \"\u001b[39m\n",
      "                \u001b[33m\"Revocation status of the certificate will not be checked against OCSP Responder.\"\u001b[39m\n",
      "            )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m cmd_query(\n",
      "        self,\n",
      "        sql: str,\n",
      "        sequence_counter: int,\n",
      "        request_id: uuid.UUID,\n",
      "        binding_params: \u001b[38;5;28;01mNone\u001b[39;00m | tuple | dict[str, dict[str, str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        binding_stage: str | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        is_file_transfer: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        statement_params: dict[str, str] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        is_internal: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        describe_only: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        _no_results: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        _update_current_object: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        _no_retry: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        timeout: int | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        dataframe_ast: str | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> dict[str, Any]:\n",
      "        \u001b[33m\"\"\"Executes a query with a sequence counter.\"\"\"\u001b[39m\n",
      "        logger.debug(\u001b[33m\"_cmd_query\"\u001b[39m)\n",
      "        data = {\n",
      "            \u001b[33m\"sqlText\"\u001b[39m: sql,\n",
      "            \u001b[33m\"asyncExec\"\u001b[39m: _no_results,\n",
      "            \u001b[33m\"sequenceId\"\u001b[39m: sequence_counter,\n",
      "            \u001b[33m\"querySubmissionTime\"\u001b[39m: get_time_millis(),\n",
      "        }\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m dataframe_ast \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            data[\u001b[33m\"dataframeAst\"\u001b[39m] = dataframe_ast\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m statement_params \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            data[\u001b[33m\"parameters\"\u001b[39m] = statement_params\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m is_internal:\n",
      "            data[\u001b[33m\"isInternal\"\u001b[39m] = is_internal\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m describe_only:\n",
      "            data[\u001b[33m\"describeOnly\"\u001b[39m] = describe_only\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m binding_stage \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# binding stage for bulk array binding\u001b[39;00m\n",
      "            data[\u001b[33m\"bindStage\"\u001b[39m] = binding_stage\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m binding_params \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# binding parameters. This is for qmarks paramstyle.\u001b[39;00m\n",
      "            data[\u001b[33m\"bindings\"\u001b[39m] = binding_params\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m _no_results:\n",
      "            \u001b[38;5;66;03m# not an async query.\u001b[39;00m\n",
      "            queryContext = self.get_query_context()\n",
      "            \u001b[38;5;66;03m#  Here queryContextDTO should be a dict object field, same with `parameters` field\u001b[39;00m\n",
      "            data[\u001b[33m\"queryContextDTO\"\u001b[39m] = queryContext\n",
      "        client = \u001b[33m\"sfsql_file_transfer\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_file_transfer \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"sfsql\"\u001b[39m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m logger.getEffectiveLevel() <= logging.DEBUG:\n",
      "            logger.debug(\n",
      "                \u001b[33m\"sql=[%s], sequence_id=[%s], is_file_transfer=[%s]\"\u001b[39m,\n",
      "                self._format_query_for_log(data[\u001b[33m\"sqlText\"\u001b[39m]),\n",
      "                data[\u001b[33m\"sequenceId\"\u001b[39m],\n",
      "                is_file_transfer,\n",
      "            )\n",
      "\n",
      "        url_parameters = {REQUEST_ID: request_id}\n",
      "\n",
      "        ret = self.rest.request(\n",
      "            \u001b[33m\"/queries/v1/query-request?\"\u001b[39m + urlencode(url_parameters),\n",
      "            data,\n",
      "            client=client,\n",
      "            _no_results=_no_results,\n",
      "            _include_retry_params=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "            _no_retry=_no_retry,\n",
      "            timeout=timeout,\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            ret = {\u001b[33m\"data\"\u001b[39m: {}}\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m ret.get(\u001b[33m\"data\"\u001b[39m) \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            ret[\u001b[33m\"data\"\u001b[39m] = {}\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m _update_current_object:\n",
      "            data = ret[\u001b[33m\"data\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"finalDatabaseName\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m data \u001b[38;5;28;01mand\u001b[39;00m data[\u001b[33m\"finalDatabaseName\"\u001b[39m] \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                self._database = data[\u001b[33m\"finalDatabaseName\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"finalSchemaName\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m data \u001b[38;5;28;01mand\u001b[39;00m data[\u001b[33m\"finalSchemaName\"\u001b[39m] \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                self._schema = data[\u001b[33m\"finalSchemaName\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"finalWarehouseName\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m data \u001b[38;5;28;01mand\u001b[39;00m data[\u001b[33m\"finalWarehouseName\"\u001b[39m] \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                self._warehouse = data[\u001b[33m\"finalWarehouseName\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"finalRoleName\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m data:\n",
      "                self._role = data[\u001b[33m\"finalRoleName\"\u001b[39m]\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"queryContext\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m data \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m _no_results:\n",
      "                \u001b[38;5;66;03m# here the data[\"queryContext\"] field has been automatically converted from JSON into a dict type\u001b[39;00m\n",
      "                self.set_query_context(data[\u001b[33m\"queryContext\"\u001b[39m])\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _reauthenticate(self):\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._auth_class.reauthenticate(conn=self)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m authenticate_with_retry(self, auth_instance) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;66;03m# make some changes if needed before real __authenticate\u001b[39;00m\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            self._authenticate(auth_instance)\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "            \u001b[38;5;66;03m# cached id_token expiration error, we have cleaned id_token and try to authenticate again\u001b[39;00m\n",
      "            logger.debug(\u001b[33m\"ID token expired. Reauthenticating...: %s\"\u001b[39m, ex)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m type(auth_instance) \u001b[38;5;28;01min\u001b[39;00m (\n",
      "                AuthByIdToken,\n",
      "                AuthByOauthCode,\n",
      "                AuthByOauthCredentials,\n",
      "            ):\n",
      "                \u001b[38;5;66;03m# IDToken and OAuth auth need to authenticate through\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# SSO if its credential has expired\u001b[39;00m\n",
      "                self._reauthenticate()\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                self._authenticate(auth_instance)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _authenticate(self, auth_instance: AuthByPlugin):\n",
      "        auth_instance.prepare(\n",
      "            conn=self,\n",
      "            authenticator=self._authenticator,\n",
      "            service_name=self.service_name,\n",
      "            account=self.account,\n",
      "            user=self.user,\n",
      "            password=self._password,\n",
      "        )\n",
      "        self._consent_cache_id_token = getattr(\n",
      "            auth_instance, \u001b[33m\"consent_cache_id_token\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "        )\n",
      "\n",
      "        auth = Auth(self.rest)\n",
      "        \u001b[38;5;66;03m# record start time for computing timeout\u001b[39;00m\n",
      "        auth_instance._retry_ctx.set_start_time()\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            auth.authenticate(\n",
      "                auth_instance=auth_instance,\n",
      "                account=self.account,\n",
      "                user=self.user,\n",
      "                database=self.database,\n",
      "                schema=self.schema,\n",
      "                warehouse=self.warehouse,\n",
      "                role=self.role,\n",
      "                passcode=self._passcode,\n",
      "                passcode_in_password=self._passcode_in_password,\n",
      "                mfa_callback=self._mfa_callback,\n",
      "                password_callback=self._password_callback,\n",
      "                session_parameters=self._session_parameters,\n",
      "            )\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            logger.debug(\n",
      "                \u001b[33m\"Operational Error raised at authentication\"\u001b[39m\n",
      "                f\"for authenticator: {type(auth_instance).__name__}\"\n",
      "            )\n",
      "            \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "                \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "                    auth_instance.handle_timeout(\n",
      "                        authenticator=self._authenticator,\n",
      "                        service_name=self.service_name,\n",
      "                        account=self.account,\n",
      "                        user=self.user,\n",
      "                        password=self._password,\n",
      "                    )\n",
      "                    auth.authenticate(\n",
      "                        auth_instance=auth_instance,\n",
      "                        account=self.account,\n",
      "                        user=self.user,\n",
      "                        database=self.database,\n",
      "                        schema=self.schema,\n",
      "                        warehouse=self.warehouse,\n",
      "                        role=self.role,\n",
      "                        passcode=self._passcode,\n",
      "                        passcode_in_password=self._passcode_in_password,\n",
      "                        mfa_callback=self._mfa_callback,\n",
      "                        password_callback=self._password_callback,\n",
      "                        session_parameters=self._session_parameters,\n",
      "                    )\n",
      "                \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m auth_op:\n",
      "                    \u001b[38;5;28;01mif\u001b[39;00m auth_op.errno == ER_FAILED_TO_CONNECT_TO_DB:\n",
      "                        \u001b[38;5;28;01mif\u001b[39;00m _CONNECTIVITY_ERR_MSG \u001b[38;5;28;01min\u001b[39;00m e.msg:\n",
      "                            auth_op.msg += f\"\\n{_CONNECTIVITY_ERR_MSG}\"\n",
      "                        \u001b[38;5;28;01mraise\u001b[39;00m auth_op \u001b[38;5;28;01mfrom\u001b[39;00m e\n",
      "                    logger.debug(\u001b[33m\"Continuing authenticator specific timeout handling\"\u001b[39m)\n",
      "                    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "                \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _write_params_to_byte_rows(\n",
      "        self, params: list[tuple[Any | tuple]]\n",
      "    ) -> list[bytes]:\n",
      "        \u001b[33m\"\"\"Write csv-format rows of binding values as list of bytes string.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            params: Binding parameters to bulk array insertion query with qmark/numeric format.\u001b[39m\n",
      "\u001b[33m            cursor: SnowflakeCursor.\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns:\u001b[39m\n",
      "\u001b[33m            List of bytes string corresponding to rows\u001b[39m\n",
      "\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        res = []\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;28;01min\u001b[39;00m params:\n",
      "                temp = map(self.converter.to_csv_bindings, row)\n",
      "                res.append((\u001b[33m\",\"\u001b[39m.join(temp) + \u001b[33m\"\\n\"\u001b[39m).encode(\u001b[33m\"utf-8\"\u001b[39m))\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m (ProgrammingError, AttributeError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m BindUploadError \u001b[38;5;28;01mfrom\u001b[39;00m exc\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _get_snowflake_type_and_binding(\n",
      "        self,\n",
      "        cursor: SnowflakeCursor | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        v: tuple[str, Any] | Any,\n",
      "    ) -> TypeAndBinding:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(v, tuple):\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m len(v) != \u001b[32m2\u001b[39m:\n",
      "                Error.errorhandler_wrapper(\n",
      "                    self,\n",
      "                    cursor,\n",
      "                    ProgrammingError,\n",
      "                    {\n",
      "                        \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Binding parameters must be a list \"\u001b[39m\n",
      "                        \u001b[33m\"where one element is a single value or \"\u001b[39m\n",
      "                        \u001b[33m\"a pair of Snowflake datatype and a value\"\u001b[39m,\n",
      "                        \u001b[33m\"errno\"\u001b[39m: ER_FAILED_PROCESSING_QMARK,\n",
      "                    },\n",
      "                )\n",
      "            snowflake_type, v = v\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            snowflake_type = self.converter.snowflake_type(v)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m snowflake_type \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                Error.errorhandler_wrapper(\n",
      "                    self,\n",
      "                    cursor,\n",
      "                    ProgrammingError,\n",
      "                    {\n",
      "                        \u001b[33m\"msg\"\u001b[39m: \u001b[33m\"Python data type [{}] cannot be \"\u001b[39m\n",
      "                        \u001b[33m\"automatically mapped to Snowflake data \"\u001b[39m\n",
      "                        \u001b[33m\"type. Specify the snowflake data type \"\u001b[39m\n",
      "                        \u001b[33m\"explicitly.\"\u001b[39m.format(v.__class__.__name__.lower()),\n",
      "                        \u001b[33m\"errno\"\u001b[39m: ER_NOT_IMPLICITY_SNOWFLAKE_DATATYPE,\n",
      "                    },\n",
      "                )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m TypeAndBinding(\n",
      "            snowflake_type,\n",
      "            self.converter.to_snowflake_bindings(snowflake_type, v),\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;66;03m# TODO we could probably rework this to not make dicts like this: {'1': 'value', '2': '13'}\u001b[39;00m\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _process_params_qmarks(\n",
      "        self,\n",
      "        params: Sequence | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        cursor: SnowflakeCursor | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> dict[str, dict[str, str]] | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m params:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        processed_params = {}\n",
      "\n",
      "        get_type_and_binding = partial(self._get_snowflake_type_and_binding, cursor)\n",
      "\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m idx, v \u001b[38;5;28;01min\u001b[39;00m enumerate(params):\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m isinstance(v, list):\n",
      "                snowflake_type = self.converter.snowflake_type(v)\n",
      "                all_param_data = list(map(get_type_and_binding, v))\n",
      "                first_type = all_param_data[\u001b[32m0\u001b[39m].type\n",
      "                \u001b[38;5;66;03m# if all elements have the same snowflake type, update snowflake_type\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m all(param_data.type == first_type \u001b[38;5;28;01mfor\u001b[39;00m param_data \u001b[38;5;28;01min\u001b[39;00m all_param_data):\n",
      "                    snowflake_type = first_type\n",
      "                processed_params[str(idx + \u001b[32m1\u001b[39m)] = {\n",
      "                    \u001b[33m\"type\"\u001b[39m: snowflake_type,\n",
      "                    \u001b[33m\"value\"\u001b[39m: [param_data.binding \u001b[38;5;28;01mfor\u001b[39;00m param_data \u001b[38;5;28;01min\u001b[39;00m all_param_data],\n",
      "                }\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                snowflake_type, snowflake_binding = get_type_and_binding(v)\n",
      "                processed_params[str(idx + \u001b[32m1\u001b[39m)] = {\n",
      "                    \u001b[33m\"type\"\u001b[39m: snowflake_type,\n",
      "                    \u001b[33m\"value\"\u001b[39m: snowflake_binding,\n",
      "                }\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m logger.getEffectiveLevel() <= logging.DEBUG:\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m processed_params.items():\n",
      "                logger.debug(\u001b[33m\"idx: %s, type: %s\"\u001b[39m, k, v.get(\u001b[33m\"type\"\u001b[39m))\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m processed_params\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _process_params_pyformat(\n",
      "        self,\n",
      "        params: Any | Sequence[Any] | dict[Any, Any] | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        cursor: SnowflakeCursor | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> tuple[Any] | dict[str, Any] | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Process parameters for client-side parameter binding.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            params: Either a sequence, or a dictionary of parameters, if anything else\u001b[39m\n",
      "\u001b[33m                is given then it will be put into a list and processed that way.\u001b[39m\n",
      "\u001b[33m            cursor: The SnowflakeCursor used to report errors if necessary.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m self._interpolate_empty_sequences:\n",
      "                \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(params, dict):\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._process_params_dict(params)\n",
      "\n",
      "        \u001b[38;5;66;03m# TODO: remove this, callers should send in what's in the signature\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(params, (tuple, list)):\n",
      "            params = [\n",
      "                params,\n",
      "            ]\n",
      "\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            res = map(self._process_single_param, params)\n",
      "            ret = tuple(res)\n",
      "            logger.debug(f\"parameters: {ret}\")\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                cursor,\n",
      "                ProgrammingError,\n",
      "                {\n",
      "                    \u001b[33m\"msg\"\u001b[39m: f\"Failed processing pyformat-parameters; {e}\",\n",
      "                    \u001b[33m\"errno\"\u001b[39m: ER_FAILED_PROCESSING_PYFORMAT,\n",
      "                },\n",
      "            )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _process_params_dict(\n",
      "        self, params: dict[Any, Any], cursor: SnowflakeCursor | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    ) -> dict:\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            res = {k: self._process_single_param(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m params.items()}\n",
      "            logger.debug(f\"parameters: {res}\")\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            Error.errorhandler_wrapper(\n",
      "                self,\n",
      "                cursor,\n",
      "                ProgrammingError,\n",
      "                {\n",
      "                    \u001b[33m\"msg\"\u001b[39m: f\"Failed processing pyformat-parameters: {e}\",\n",
      "                    \u001b[33m\"errno\"\u001b[39m: ER_FAILED_PROCESSING_PYFORMAT,\n",
      "                },\n",
      "            )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _process_single_param(self, param: Any) -> Any:\n",
      "        \u001b[33m\"\"\"Process a single parameter to Snowflake understandable form.\u001b[39m\n",
      "\n",
      "\u001b[33m        This is a convenience function to replace repeated multiple calls with a single\u001b[39m\n",
      "\u001b[33m        function call.\u001b[39m\n",
      "\n",
      "\u001b[33m        It calls the following underlying functions in this order:\u001b[39m\n",
      "\u001b[33m            1. self.converter.to_snowflake\u001b[39m\n",
      "\u001b[33m            2. self.converter.escape\u001b[39m\n",
      "\u001b[33m            3. self.converter.quote\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        to_snowflake = self.converter.to_snowflake\n",
      "        escape = self.converter.escape\n",
      "        _quote = self.converter.quote\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m _quote(escape(to_snowflake(param)))\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _cancel_query(self, sql: str, request_id: UUID) -> dict[str, bool | \u001b[38;5;28;01mNone\u001b[39;00m]:\n",
      "        \u001b[33m\"\"\"Cancels the query with the exact SQL query and requestId.\"\"\"\u001b[39m\n",
      "        logger.debug(\u001b[33m\"_cancel_query sql=[%s], request_id=[%s]\"\u001b[39m, sql, request_id)\n",
      "        url_parameters = {REQUEST_ID: str(uuid.uuid4())}\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self.rest.request(\n",
      "            \u001b[33m\"/queries/v1/abort-request?\"\u001b[39m + urlencode(url_parameters),\n",
      "            {\n",
      "                \u001b[33m\"sqlText\"\u001b[39m: sql,\n",
      "                REQUEST_ID: str(request_id),\n",
      "            },\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _next_sequence_counter(self) -> int:\n",
      "        \u001b[33m\"\"\"Gets next sequence counter. Used internally.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m self._lock_sequence_counter:\n",
      "            self.sequence_counter += \u001b[32m1\u001b[39m\n",
      "            logger.debug(\u001b[33m\"sequence counter: %s\"\u001b[39m, self.sequence_counter)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self.sequence_counter\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _log_telemetry(self, telemetry_data) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Logs data to telemetry.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.telemetry_enabled:\n",
      "            self._telemetry.try_add_log_to_batch(telemetry_data)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _add_heartbeat(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Add a periodic heartbeat query in order to keep connection alive.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.heartbeat_thread:\n",
      "            self._validate_client_session_keep_alive_heartbeat_frequency()\n",
      "            heartbeat_wref = weakref.WeakMethod(self._heartbeat_tick)\n",
      "\n",
      "            \u001b[38;5;28;01mdef\u001b[39;00m beat_if_possible() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                heartbeat_fn = heartbeat_wref()\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m heartbeat_fn:\n",
      "                    heartbeat_fn()\n",
      "\n",
      "            self.heartbeat_thread = HeartBeatTimer(\n",
      "                self.client_session_keep_alive_heartbeat_frequency,\n",
      "                beat_if_possible,\n",
      "            )\n",
      "            self.heartbeat_thread.start()\n",
      "            logger.debug(\u001b[33m\"started heartbeat\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _cancel_heartbeat(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Cancel a heartbeat thread.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.heartbeat_thread:\n",
      "            self.heartbeat_thread.cancel()\n",
      "            self.heartbeat_thread.join()\n",
      "            self.heartbeat_thread = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            logger.debug(\u001b[33m\"stopped heartbeat\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _heartbeat_tick(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Execute a heartbeat if connection isn't closed yet.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.is_closed():\n",
      "            logger.debug(\u001b[33m\"heartbeating!\"\u001b[39m)\n",
      "            self.rest._heartbeat()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _validate_client_session_keep_alive_heartbeat_frequency(self) -> int:\n",
      "        \u001b[33m\"\"\"Validate and return heartbeat frequency in seconds.\"\"\"\u001b[39m\n",
      "        real_max = int(self.rest.master_validity_in_seconds / \u001b[32m4\u001b[39m)\n",
      "        real_min = int(real_max / \u001b[32m4\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;66;03m# ensure the type is integer\u001b[39;00m\n",
      "        self._client_session_keep_alive_heartbeat_frequency = int(\n",
      "            self.client_session_keep_alive_heartbeat_frequency\n",
      "        )\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.client_session_keep_alive_heartbeat_frequency \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;66;03m# This is an unlikely scenario but covering it just in case.\u001b[39;00m\n",
      "            self._client_session_keep_alive_heartbeat_frequency = real_min\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.client_session_keep_alive_heartbeat_frequency > real_max:\n",
      "            self._client_session_keep_alive_heartbeat_frequency = real_max\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.client_session_keep_alive_heartbeat_frequency < real_min:\n",
      "            self._client_session_keep_alive_heartbeat_frequency = real_min\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self.client_session_keep_alive_heartbeat_frequency\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _validate_client_prefetch_threads(self) -> int:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.client_prefetch_threads <= \u001b[32m0\u001b[39m:\n",
      "            self._client_prefetch_threads = \u001b[32m1\u001b[39m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.client_prefetch_threads > MAX_CLIENT_PREFETCH_THREADS:\n",
      "            self._client_prefetch_threads = MAX_CLIENT_PREFETCH_THREADS\n",
      "        self._client_prefetch_threads = int(self.client_prefetch_threads)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self.client_prefetch_threads\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _update_parameters(\n",
      "        self,\n",
      "        parameters: dict[str, str | int | bool],\n",
      "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Update session parameters.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m self._lock_converter:\n",
      "            self.converter.set_parameters(parameters)\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;28;01min\u001b[39;00m parameters.items():\n",
      "            self._session_parameters[name] = value\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m PARAMETER_CLIENT_TELEMETRY_ENABLED == name:\n",
      "                self._server_param_telemetry_enabled = value\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m PARAMETER_CLIENT_SESSION_KEEP_ALIVE == name:\n",
      "                \u001b[38;5;66;03m# Only set if the local config is None.\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# Always give preference to user config.\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m self.client_session_keep_alive \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    self.client_session_keep_alive = value\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m (\n",
      "                PARAMETER_CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY == name\n",
      "                \u001b[38;5;28;01mand\u001b[39;00m self.client_session_keep_alive_heartbeat_frequency \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            ):\n",
      "                \u001b[38;5;66;03m# Only set if local value hasn't been set already.\u001b[39;00m\n",
      "                self.client_session_keep_alive_heartbeat_frequency = value\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m PARAMETER_SERVICE_NAME == name:\n",
      "                self.service_name = value\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m PARAMETER_CLIENT_PREFETCH_THREADS == name:\n",
      "                self.client_prefetch_threads = value\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m PARAMETER_ENABLE_STAGE_S3_PRIVATELINK_FOR_US_EAST_1 == name:\n",
      "                self.enable_stage_s3_privatelink_for_us_east_1 = value\n",
      "            \u001b[38;5;28;01melif\u001b[39;00m PARAMETER_QUERY_CONTEXT_CACHE_SIZE == name:\n",
      "                self.query_context_cache_size = value\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _format_query_for_log(self, query: str) -> str:\n",
      "        ret = \u001b[33m\" \"\u001b[39m.join(line.strip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;28;01min\u001b[39;00m query.split(\u001b[33m\"\\n\"\u001b[39m))\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m (\n",
      "            ret\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m len(ret) < self.log_max_query_length\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m ret[\u001b[32m0\u001b[39m : self.log_max_query_length] + \u001b[33m\"...\"\u001b[39m\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __enter__(self) -> SnowflakeConnection:\n",
      "        \u001b[33m\"\"\"Context manager.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __exit__(\n",
      "        self,\n",
      "        exc_type: type[BaseException] | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        exc_val: BaseException | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        exc_tb: TracebackType | \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[33m\"\"\"Context manager with commit or rollback teardown.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._session_parameters.get(\u001b[33m\"AUTOCOMMIT\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "            \u001b[38;5;66;03m# Either AUTOCOMMIT is turned off, or is not set so we default to old behavior\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m exc_tb \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                self.commit()\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                self.rollback()\n",
      "        self.close()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _get_query_status(self, sf_qid: str) -> tuple[QueryStatus, dict[str, Any]]:\n",
      "        \u001b[33m\"\"\"Retrieves the status of query with sf_qid and returns it with the raw response.\u001b[39m\n",
      "\n",
      "\u001b[33m        This is the underlying function used by the public get_status functions.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            sf_qid: Snowflake query id of interest.\u001b[39m\n",
      "\n",
      "\u001b[33m        Raises:\u001b[39m\n",
      "\u001b[33m            ValueError: if sf_qid is not a valid UUID string.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            uuid.UUID(sf_qid)\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m ValueError:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"Invalid UUID: '{sf_qid}'\")\n",
      "        logger.debug(f\"get_query_status sf_qid='{sf_qid}'\")\n",
      "\n",
      "        status = \u001b[33m\"NO_DATA\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.is_closed():\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m QueryStatus.DISCONNECTED, {\u001b[33m\"data\"\u001b[39m: {\u001b[33m\"queries\"\u001b[39m: []}}\n",
      "        status_resp = self.rest.request(\n",
      "            \u001b[33m\"/monitoring/queries/\"\u001b[39m + quote(sf_qid), method=\u001b[33m\"get\"\u001b[39m, client=\u001b[33m\"rest\"\u001b[39m\n",
      "        )\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"queries\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m status_resp[\u001b[33m\"data\"\u001b[39m]:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m QueryStatus.FAILED_WITH_ERROR, status_resp\n",
      "        queries = status_resp[\u001b[33m\"data\"\u001b[39m][\u001b[33m\"queries\"\u001b[39m]\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m len(queries) > \u001b[32m0\u001b[39m:\n",
      "            status = queries[\u001b[32m0\u001b[39m][\u001b[33m\"status\"\u001b[39m]\n",
      "        status_ret = QueryStatus[status]\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m status_ret, status_resp\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _cache_query_status(self, sf_qid: str, status_ret: QueryStatus) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;66;03m# If query was started by us and it has finished let's cache this info\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m sf_qid \u001b[38;5;28;01min\u001b[39;00m self._async_sfqids \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.is_still_running(status_ret):\n",
      "            self._async_sfqids.pop(\n",
      "                sf_qid, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "            )  \u001b[38;5;66;03m# Prevent KeyError when multiple threads try to remove the same query id\u001b[39;00m\n",
      "            self._done_async_sfqids[sf_qid] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _close_at_exit(self):\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m suppress(Exception):\n",
      "            self.close(retry=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _process_error_query_status(\n",
      "        self,\n",
      "        sf_qid: str,\n",
      "        status_resp: dict,\n",
      "        error_message: str = \u001b[33m\"\"\u001b[39m,\n",
      "        error_cls: type[Exception] = ProgrammingError,\n",
      "    ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        status_resp = status_resp \u001b[38;5;28;01mor\u001b[39;00m {}\n",
      "        data = status_resp.get(\u001b[33m\"data\"\u001b[39m, {})\n",
      "        queries = data.get(\u001b[33m\"queries\"\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m sf_qid \u001b[38;5;28;01min\u001b[39;00m self._async_sfqids:\n",
      "            self._async_sfqids.pop(sf_qid, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "        message = status_resp.get(\u001b[33m\"message\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            message = \u001b[33m\"\"\u001b[39m\n",
      "        code = queries[\u001b[32m0\u001b[39m].get(\u001b[33m\"errorCode\"\u001b[39m, -\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m queries \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n",
      "        sql_state = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"data\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m status_resp:\n",
      "            message += queries[\u001b[32m0\u001b[39m].get(\u001b[33m\"errorMessage\"\u001b[39m, \u001b[33m\"\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m queries \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\"\u001b[39m\n",
      "            sql_state = data.get(\u001b[33m\"sqlState\"\u001b[39m)\n",
      "        Error.errorhandler_wrapper(\n",
      "            self,\n",
      "            \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "            error_cls,\n",
      "            {\n",
      "                \u001b[33m\"msg\"\u001b[39m: message \u001b[38;5;28;01mor\u001b[39;00m error_message,\n",
      "                \u001b[33m\"errno\"\u001b[39m: int(code),\n",
      "                \u001b[33m\"sqlstate\"\u001b[39m: sql_state,\n",
      "                \u001b[33m\"sfqid\"\u001b[39m: sf_qid,\n",
      "            },\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m get_query_status(self, sf_qid: str) -> QueryStatus:\n",
      "        \u001b[33m\"\"\"Retrieves the status of query with sf_qid.\u001b[39m\n",
      "\n",
      "\u001b[33m        Query status is returned as a QueryStatus.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            sf_qid: Snowflake query id of interest.\u001b[39m\n",
      "\n",
      "\u001b[33m        Raises:\u001b[39m\n",
      "\u001b[33m            ValueError: if sf_qid is not a valid UUID string.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        status, _ = self._get_query_status(sf_qid)\n",
      "        self._cache_query_status(sf_qid, status)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m status\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m get_query_status_throw_if_error(self, sf_qid: str) -> QueryStatus:\n",
      "        \u001b[33m\"\"\"Retrieves the status of query with sf_qid as a QueryStatus and raises an exception if the query terminated with an error.\u001b[39m\n",
      "\n",
      "\u001b[33m        Query status is returned as a QueryStatus.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            sf_qid: Snowflake query id of interest.\u001b[39m\n",
      "\n",
      "\u001b[33m        Raises:\u001b[39m\n",
      "\u001b[33m            ValueError: if sf_qid is not a valid UUID string.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        status, status_resp = self._get_query_status(sf_qid)\n",
      "        self._cache_query_status(sf_qid, status)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.is_an_error(status):\n",
      "            self._process_error_query_status(sf_qid, status_resp)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m status\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m initialize_query_context_cache(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.is_query_context_cache_disabled:\n",
      "            self.query_context_cache = QueryContextCache(self.query_context_cache_size)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m get_query_context(self) -> dict | \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.is_query_context_cache_disabled:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self.query_context_cache.serialize_to_dict()\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m set_query_context(self, data: dict) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.is_query_context_cache_disabled:\n",
      "            self.query_context_cache.deserialize_json_dict(data)\n",
      "\n",
      "    @staticmethod\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_still_running(status: QueryStatus) -> bool:\n",
      "        \u001b[33m\"\"\"Checks whether given status is currently running.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m status \u001b[38;5;28;01min\u001b[39;00m (\n",
      "            QueryStatus.RUNNING,\n",
      "            QueryStatus.QUEUED,\n",
      "            QueryStatus.RESUMING_WAREHOUSE,\n",
      "            QueryStatus.QUEUED_REPARING_WAREHOUSE,\n",
      "            QueryStatus.BLOCKED,\n",
      "            QueryStatus.NO_DATA,\n",
      "        )\n",
      "\n",
      "    @staticmethod\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_an_error(status: QueryStatus) -> bool:\n",
      "        \u001b[33m\"\"\"Checks whether given status means that there has been an error.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m status \u001b[38;5;28;01min\u001b[39;00m (\n",
      "            QueryStatus.ABORTING,\n",
      "            QueryStatus.FAILED_WITH_ERROR,\n",
      "            QueryStatus.ABORTED,\n",
      "            QueryStatus.FAILED_WITH_INCIDENT,\n",
      "            QueryStatus.DISCONNECTED,\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _all_async_queries_finished(self) -> bool:\n",
      "        \u001b[33m\"\"\"Checks whether all async queries started by this Connection have finished executing.\"\"\"\u001b[39m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._async_sfqids:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "        queries = list(reversed(self._async_sfqids.keys()))\n",
      "\n",
      "        num_workers = min(self.client_prefetch_threads, len(queries))\n",
      "        found_unfinished_query = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "        \u001b[38;5;28;01mdef\u001b[39;00m async_query_check_helper(\n",
      "            sfq_id: str,\n",
      "        ) -> bool:\n",
      "            \u001b[38;5;28;01mnonlocal\u001b[39;00m found_unfinished_query\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m found_unfinished_query \u001b[38;5;28;01mor\u001b[39;00m self.is_still_running(\n",
      "                self.get_query_status(sfq_id)\n",
      "            )\n",
      "\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(\n",
      "            max_workers=num_workers, thread_name_prefix=\u001b[33m\"async_query_check_\"\u001b[39m\n",
      "        ) \u001b[38;5;28;01mas\u001b[39;00m tpe:  \u001b[38;5;66;03m# We should upgrade to using cancel_futures=True once supporting 3.9+\u001b[39;00m\n",
      "            futures = (tpe.submit(async_query_check_helper, sfqid) \u001b[38;5;28;01mfor\u001b[39;00m sfqid \u001b[38;5;28;01min\u001b[39;00m queries)\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;28;01min\u001b[39;00m as_completed(futures):\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m f.result():\n",
      "                    found_unfinished_query = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "                    \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;28;01min\u001b[39;00m futures:\n",
      "                f.cancel()\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m found_unfinished_query\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _log_telemetry_imported_packages(self) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self._log_imported_packages_in_telemetry:\n",
      "            \u001b[38;5;66;03m# filter out duplicates caused by submodules\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# and internal modules with names starting with an underscore\u001b[39;00m\n",
      "            imported_modules = {\n",
      "                k.split(\u001b[33m\".\"\u001b[39m, maxsplit=\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "                \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;28;01min\u001b[39;00m list(sys.modules)\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m k.startswith(\u001b[33m\"_\"\u001b[39m)\n",
      "            }\n",
      "            ts = get_time_millis()\n",
      "            self._log_telemetry(\n",
      "                TelemetryData.from_telemetry_data_dict(\n",
      "                    from_dict={\n",
      "                        TelemetryField.KEY_TYPE.value: TelemetryField.IMPORTED_PACKAGES.value,\n",
      "                        TelemetryField.KEY_VALUE.value: str(imported_modules),\n",
      "                    },\n",
      "                    timestamp=ts,\n",
      "                    connection=self,\n",
      "                )\n",
      "            )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_valid(self) -> bool:\n",
      "        \u001b[33m\"\"\"This function tries to answer the question: Is this connection still good for sending queries?\u001b[39m\n",
      "\u001b[33m        Attempts to validate the connections both on the TCP/IP and Session levels.\"\"\"\u001b[39m\n",
      "        logger.debug(\u001b[33m\"validating connection and session\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.is_closed():\n",
      "            logger.debug(\u001b[33m\"connection is already closed and not valid\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            logger.debug(\u001b[33m\"trying to heartbeat into the session to validate\"\u001b[39m)\n",
      "            hb_result = self.rest._heartbeat()\n",
      "            session_valid = hb_result.get(\u001b[33m\"success\"\u001b[39m)\n",
      "            logger.debug(\u001b[33m\"session still valid? %s\"\u001b[39m, session_valid)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m bool(session_valid)\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            logger.debug(\u001b[33m\"session could not be validated due to exception: %s\"\u001b[39m, e)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "    @staticmethod\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _detect_application() -> \u001b[38;5;28;01mNone\u001b[39;00m | str:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m ENV_VAR_PARTNER \u001b[38;5;28;01min\u001b[39;00m os.environ.keys():\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m os.environ[ENV_VAR_PARTNER]\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"streamlit\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m sys.modules:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"streamlit\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m all(\n",
      "            (jpmod \u001b[38;5;28;01min\u001b[39;00m sys.modules)\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m jpmod \u001b[38;5;28;01min\u001b[39;00m (\u001b[33m\"ipykernel\"\u001b[39m, \u001b[33m\"jupyter_core\"\u001b[39m, \u001b[33m\"jupyter_client\"\u001b[39m)\n",
      "        ):\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"jupyter_notebook\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"snowbooks\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m sys.modules:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"snowflake_notebook\"\u001b[39m\n",
      "\u001b[31mInit docstring:\u001b[39m\n",
      "Create a new SnowflakeConnection.\n",
      "\n",
      "Connections can be loaded from the TOML file located at\n",
      "snowflake.connector.constants.CONNECTIONS_FILE.\n",
      "\n",
      "When connection_name is supplied we will first load that connection\n",
      "and then override any other values supplied.\n",
      "\n",
      "When no arguments are given (other than connection_file_path) the\n",
      "default connection will be loaded first. Note that no overwriting is\n",
      "supported in this case.\n",
      "\n",
      "If overwriting values from the default connection is desirable, supply\n",
      "the name explicitly."
     ]
    }
   ],
   "source": [
    "conn??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute_query\n",
    "To run single SQL query and return data in following format:\n",
    "- `query`: query \n",
    "- `success`: True if successfully executed else False\n",
    "- `data`: `df.to_dict('record')`  if successfully executed else `None`\n",
    "- `error`: `None`  if successfully executed else `str(Exception)`\n",
    "- `row_count`:  len(df) \n",
    "- `query`: time to execution time \n",
    "\n",
    "\n",
    "---\n",
    "1. **Max row limit** - Should you limit results (e.g., max 10,000 rows) to prevent memory issues? : help me to do this\n",
    "2. **Empty query check** - What if someone passes an empty string? : this will bw handled by the try catch block\n",
    "3. **Execution time** - Track how long queries take? (useful for RLM logging) : ok lets add it\n",
    "4. **SQL comments** - Queries like `\"SELECT * FROM x -- ; DROP TABLE y\"` might bypass your check : it is a single query function right?\n",
    "\n",
    "Note: assuming `df` is the result generated after a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS LIMIT 4\")\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id</th>\n",
       "      <th>flight_no</th>\n",
       "      <th>scheduled_departure</th>\n",
       "      <th>scheduled_arrival</th>\n",
       "      <th>departure_airport</th>\n",
       "      <th>arrival_airport</th>\n",
       "      <th>status</th>\n",
       "      <th>aircraft_code</th>\n",
       "      <th>actual_departure</th>\n",
       "      <th>actual_arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185</td>\n",
       "      <td>PG0134</td>\n",
       "      <td>2017-09-10 09:50:00+03</td>\n",
       "      <td>2017-09-10 14:55:00+03</td>\n",
       "      <td>DME</td>\n",
       "      <td>BTK</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>319</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3979</td>\n",
       "      <td>PG0052</td>\n",
       "      <td>2017-08-25 14:50:00+03</td>\n",
       "      <td>2017-08-25 17:35:00+03</td>\n",
       "      <td>VKO</td>\n",
       "      <td>HMA</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>CR2</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4739</td>\n",
       "      <td>PG0561</td>\n",
       "      <td>2017-09-05 12:30:00+03</td>\n",
       "      <td>2017-09-05 14:15:00+03</td>\n",
       "      <td>VKO</td>\n",
       "      <td>AER</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>763</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5502</td>\n",
       "      <td>PG0529</td>\n",
       "      <td>2017-09-12 09:50:00+03</td>\n",
       "      <td>2017-09-12 11:20:00+03</td>\n",
       "      <td>SVO</td>\n",
       "      <td>UFA</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>763</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flight_id flight_no     scheduled_departure       scheduled_arrival  \\\n",
       "0       1185    PG0134  2017-09-10 09:50:00+03  2017-09-10 14:55:00+03   \n",
       "1       3979    PG0052  2017-08-25 14:50:00+03  2017-08-25 17:35:00+03   \n",
       "2       4739    PG0561  2017-09-05 12:30:00+03  2017-09-05 14:15:00+03   \n",
       "3       5502    PG0529  2017-09-12 09:50:00+03  2017-09-12 11:20:00+03   \n",
       "\n",
       "  departure_airport arrival_airport     status aircraft_code actual_departure  \\\n",
       "0               DME             BTK  Scheduled           319               \\N   \n",
       "1               VKO             HMA  Scheduled           CR2               \\N   \n",
       "2               VKO             AER  Scheduled           763               \\N   \n",
       "3               SVO             UFA  Scheduled           763               \\N   \n",
       "\n",
       "  actual_arrival  \n",
       "0             \\N  \n",
       "1             \\N  \n",
       "2             \\N  \n",
       "3             \\N  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results, columns=column_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_df(query: str)->pd.core.frame.DataFrame:\n",
    "    cursor.execute(query)\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    results = cursor.fetchall()\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id</th>\n",
       "      <th>flight_no</th>\n",
       "      <th>scheduled_departure</th>\n",
       "      <th>scheduled_arrival</th>\n",
       "      <th>departure_airport</th>\n",
       "      <th>arrival_airport</th>\n",
       "      <th>status</th>\n",
       "      <th>aircraft_code</th>\n",
       "      <th>actual_departure</th>\n",
       "      <th>actual_arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185</td>\n",
       "      <td>PG0134</td>\n",
       "      <td>2017-09-10 09:50:00+03</td>\n",
       "      <td>2017-09-10 14:55:00+03</td>\n",
       "      <td>DME</td>\n",
       "      <td>BTK</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>319</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flight_id flight_no     scheduled_departure       scheduled_arrival  \\\n",
       "0       1185    PG0134  2017-09-10 09:50:00+03  2017-09-10 14:55:00+03   \n",
       "\n",
       "  departure_airport arrival_airport     status aircraft_code actual_departure  \\\n",
       "0               DME             BTK  Scheduled           319               \\N   \n",
       "\n",
       "  actual_arrival  \n",
       "0             \\N  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sql_df(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS LIMIT 1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should not accept multiple queries. It should only accept a single query. To achive this we will filter out by `;` from the query. \n",
    "Here's the complete better semicolon check logic:\n",
    "\n",
    "1. Count all semicolons in the entire query\n",
    "2. Extract all quoted strings (both single and double quotes)\n",
    "3. Count semicolons that appear inside those quoted strings\n",
    "4. If counts don't match, there's an unquoted semicolon â†’ reject\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'test;data'\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt  = \"SELECT * FROM x WHERE name = 'test;data';\"\n",
    "re.findall(r'\"[^\"]*;[^\"]*\"|\\'[^\\']*;[^\\']*\\'' , txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'test;data'\"]\n",
      "[]\n",
      "[\"'a;b'\", \"'c;d'\"]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "t = [\n",
    "    \"SELECT * FROM x WHERE name = 'test;data'\",\n",
    "    \"SELECT * FROM x; DROP TABLE y\", \n",
    "    \"SELECT * WHERE x = 'a;b' AND y = 'c;d'\",\n",
    "    \"SELECT * FROM x; DROP TABLE y\",\n",
    "    \"\"\"SELECT * FROM x;\n",
    "-- comment\n",
    "DROP TABLE y\n",
    "\"\"\"\n",
    "    ]\n",
    "for i in t:\n",
    "    print(re.findall(r'\"[^\"]*;[^\"]*\"|\\'[^\\']*;[^\\']*\\'' , i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's the complete better semicolon check logic:\n",
    "\n",
    "1. Count all semicolons in the entire query\n",
    "2. Extract all quoted strings (both single and double quotes)\n",
    "3. Count semicolons that appear inside those quoted strings\n",
    "4. If counts don't match, there's an unquoted semicolon â†’ reject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM x WHERE name = 'test;data' 1 1\n",
      "SELECT * FROM x; DROP TABLE y 1 0\n",
      "SELECT * WHERE x = 'a;b' AND y = 'c;d' 2 2\n",
      "SELECT * FROM x; DROP TABLE y 1 0\n",
      "SELECT * FROM x;\n",
      "-- comment\n",
      "DROP TABLE y\n",
      " 1 0\n"
     ]
    }
   ],
   "source": [
    "for i in t:\n",
    "    print(i,  i.count(';'), len(re.findall(r';', ''.join(re.findall(r'\"[^\"]*\"|\\'[^\\']*\\'', i)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    query: str\n",
    "    success: bool\n",
    "    data: Optional[list[dict]] = None\n",
    "    error: Optional[str] = None\n",
    "    row_count: int = 0\n",
    "    execution_time: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import time\n",
    "UNSAFE_QUERIES = ['DROP', 'UPDATE', 'DELETE', 'INSERT', 'TRUNCATE', 'ALTER']\n",
    "\n",
    "class SnowflakeAgent:\n",
    "    def __init__(self):\n",
    "        self.connect_to_snowflake = connect_to_snowflake\n",
    "\n",
    "    def connect(self): \n",
    "        self.conn, self.cursor = self.connect_to_snowflake()\n",
    "        self.check_connection\n",
    "\n",
    "    @property\n",
    "    def check_connection(self):\n",
    "        try:\n",
    "            return not self.cursor.is_closed()\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    def execute_query(\n",
    "        self,                       # SnowflakeAgent instance for toolloop ignore this part\n",
    "        query: str,                 # SQL query with to execute\n",
    "        max_rows: int = 10,         # Maximum rows to fetch\n",
    "        fetch_all: bool = False,    # Fetch all rows if True\n",
    "    ) -> QueryResult:\n",
    "        \"\"\"\n",
    "        Execute a SQL query and return results as a QueryResult object.\n",
    "        \n",
    "        This function provides safe SQL execution with protection against:\n",
    "        - Multiple statements (via semicolon detection)\n",
    "        - Unsafe operations (DROP, DELETE, UPDATE, etc.)\n",
    "        - Memory issues (via row limiting)\n",
    "        \n",
    "        Args:\n",
    "            query (str): The SQL query to execute. Must be a single SELECT statement.\n",
    "            max_rows (int, optional): Maximum number of rows to fetch. Defaults to 10.\n",
    "                Only applies when fetch_all=False.\n",
    "            fetch_all (bool, optional): If True, fetch all rows regardless of max_rows.\n",
    "                Defaults to False for safety.\n",
    "        \n",
    "        Returns:\n",
    "            QueryResult: A Pydantic model containing:\n",
    "                - query: The executed query\n",
    "                - success: Whether execution succeeded\n",
    "                - data: List of dictionaries (rows) if successful, None otherwise\n",
    "                - error: Error message if failed, None otherwise\n",
    "                - row_count: Number of rows returned\n",
    "                - execution_time: Time taken to execute the query in seconds\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If query contains multiple statements or unsafe operations.\n",
    "            Exception: Any database errors are caught and returned in QueryResult.error\n",
    "        \n",
    "        Examples:\n",
    "            >>> result = sql_df(\"SELECT * FROM users\")\n",
    "            >>> result.success\n",
    "            True\n",
    "            >>> result.row_count\n",
    "            10\n",
    "            \n",
    "            >>> result = sql_df(\"SELECT * FROM users\", fetch_all=True)\n",
    "            >>> result.row_count\n",
    "            1000\n",
    "        \"\"\"\n",
    "        if not self.check_connection:\n",
    "            self.connect()\n",
    "\n",
    "        try:\n",
    "            query = query.strip()\n",
    "\n",
    "            # Check for multiple statements via semicolon detection\n",
    "            if ';' in query:\n",
    "                total_semicolons = query.count(';')\n",
    "                safe_semicolons = len(re.findall(r';', ''.join(re.findall(r'\"[^\"]*\"|\\'[^\\']*\\'', query))))\n",
    "                \n",
    "                if total_semicolons != safe_semicolons:\n",
    "                    raise ValueError(\"Multiple statements or unsafe semicolons detected!\")\n",
    "\n",
    "            # Prevent unsafe data modification queries\n",
    "            if any([query.upper().startswith(i) for i in UNSAFE_QUERIES]):\n",
    "                raise ValueError(\"Trying Data Update, Not allowed!!!\")\n",
    "            \n",
    "            # Execute query and measure time\n",
    "            start_time = time.time()\n",
    "            self.cursor.execute(query)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Fetch results\n",
    "            column_names = [desc[0] for desc in self.cursor.description]\n",
    "            if fetch_all:\n",
    "                results = self.cursor.fetchall()\n",
    "            else:\n",
    "                results = self.cursor.fetchmany(max_rows)\n",
    "                \n",
    "            df = pd.DataFrame(results, columns=column_names)\n",
    "            \n",
    "            return QueryResult(\n",
    "                query=query,\n",
    "                success=True,\n",
    "                data=df.to_dict('records'),\n",
    "                error=None,\n",
    "                row_count=len(results),\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return QueryResult(\n",
    "                query=query,\n",
    "                success=False,\n",
    "                data=None,\n",
    "                error=str(e),\n",
    "                row_count=0,\n",
    "                execution_time=0.0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SnowflakeAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS\")\n",
    "assert result.success\n",
    "assert result.row_count > 0\n",
    "assert type(result.execution_time) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id</th>\n",
       "      <th>flight_no</th>\n",
       "      <th>scheduled_departure</th>\n",
       "      <th>scheduled_arrival</th>\n",
       "      <th>departure_airport</th>\n",
       "      <th>arrival_airport</th>\n",
       "      <th>status</th>\n",
       "      <th>aircraft_code</th>\n",
       "      <th>actual_departure</th>\n",
       "      <th>actual_arrival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185</td>\n",
       "      <td>PG0134</td>\n",
       "      <td>2017-09-10 09:50:00+03</td>\n",
       "      <td>2017-09-10 14:55:00+03</td>\n",
       "      <td>DME</td>\n",
       "      <td>BTK</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>319</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3979</td>\n",
       "      <td>PG0052</td>\n",
       "      <td>2017-08-25 14:50:00+03</td>\n",
       "      <td>2017-08-25 17:35:00+03</td>\n",
       "      <td>VKO</td>\n",
       "      <td>HMA</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>CR2</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4739</td>\n",
       "      <td>PG0561</td>\n",
       "      <td>2017-09-05 12:30:00+03</td>\n",
       "      <td>2017-09-05 14:15:00+03</td>\n",
       "      <td>VKO</td>\n",
       "      <td>AER</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>763</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5502</td>\n",
       "      <td>PG0529</td>\n",
       "      <td>2017-09-12 09:50:00+03</td>\n",
       "      <td>2017-09-12 11:20:00+03</td>\n",
       "      <td>SVO</td>\n",
       "      <td>UFA</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>763</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6938</td>\n",
       "      <td>PG0461</td>\n",
       "      <td>2017-09-04 12:25:00+03</td>\n",
       "      <td>2017-09-04 13:20:00+03</td>\n",
       "      <td>SVO</td>\n",
       "      <td>ULV</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>SU9</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7784</td>\n",
       "      <td>PG0667</td>\n",
       "      <td>2017-09-10 15:00:00+03</td>\n",
       "      <td>2017-09-10 17:30:00+03</td>\n",
       "      <td>SVO</td>\n",
       "      <td>KRO</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>CR2</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9478</td>\n",
       "      <td>PG0360</td>\n",
       "      <td>2017-08-28 09:00:00+03</td>\n",
       "      <td>2017-08-28 11:35:00+03</td>\n",
       "      <td>LED</td>\n",
       "      <td>REN</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>CR2</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11085</td>\n",
       "      <td>PG0569</td>\n",
       "      <td>2017-08-24 15:05:00+03</td>\n",
       "      <td>2017-08-24 16:10:00+03</td>\n",
       "      <td>SVX</td>\n",
       "      <td>SCW</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>733</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11847</td>\n",
       "      <td>PG0498</td>\n",
       "      <td>2017-09-12 10:15:00+03</td>\n",
       "      <td>2017-09-12 14:55:00+03</td>\n",
       "      <td>KZN</td>\n",
       "      <td>IKT</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>319</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12012</td>\n",
       "      <td>PG0621</td>\n",
       "      <td>2017-08-26 16:05:00+03</td>\n",
       "      <td>2017-08-26 17:00:00+03</td>\n",
       "      <td>KZN</td>\n",
       "      <td>MQF</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>CR2</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flight_id flight_no     scheduled_departure       scheduled_arrival  \\\n",
       "0       1185    PG0134  2017-09-10 09:50:00+03  2017-09-10 14:55:00+03   \n",
       "1       3979    PG0052  2017-08-25 14:50:00+03  2017-08-25 17:35:00+03   \n",
       "2       4739    PG0561  2017-09-05 12:30:00+03  2017-09-05 14:15:00+03   \n",
       "3       5502    PG0529  2017-09-12 09:50:00+03  2017-09-12 11:20:00+03   \n",
       "4       6938    PG0461  2017-09-04 12:25:00+03  2017-09-04 13:20:00+03   \n",
       "5       7784    PG0667  2017-09-10 15:00:00+03  2017-09-10 17:30:00+03   \n",
       "6       9478    PG0360  2017-08-28 09:00:00+03  2017-08-28 11:35:00+03   \n",
       "7      11085    PG0569  2017-08-24 15:05:00+03  2017-08-24 16:10:00+03   \n",
       "8      11847    PG0498  2017-09-12 10:15:00+03  2017-09-12 14:55:00+03   \n",
       "9      12012    PG0621  2017-08-26 16:05:00+03  2017-08-26 17:00:00+03   \n",
       "\n",
       "  departure_airport arrival_airport     status aircraft_code actual_departure  \\\n",
       "0               DME             BTK  Scheduled           319               \\N   \n",
       "1               VKO             HMA  Scheduled           CR2               \\N   \n",
       "2               VKO             AER  Scheduled           763               \\N   \n",
       "3               SVO             UFA  Scheduled           763               \\N   \n",
       "4               SVO             ULV  Scheduled           SU9               \\N   \n",
       "5               SVO             KRO  Scheduled           CR2               \\N   \n",
       "6               LED             REN  Scheduled           CR2               \\N   \n",
       "7               SVX             SCW  Scheduled           733               \\N   \n",
       "8               KZN             IKT  Scheduled           319               \\N   \n",
       "9               KZN             MQF  Scheduled           CR2               \\N   \n",
       "\n",
       "  actual_arrival  \n",
       "0             \\N  \n",
       "1             \\N  \n",
       "2             \\N  \n",
       "3             \\N  \n",
       "4             \\N  \n",
       "5             \\N  \n",
       "6             \\N  \n",
       "7             \\N  \n",
       "8             \\N  \n",
       "9             \\N  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result.data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) == result.row_count \n",
    "assert not result.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test 1: Successful query with default max_rows\n",
      "Success: True\n",
      "Row count: 10\n",
      "Execution time: 1.8529s\n",
      "First row: {'flight_id': 1185, 'flight_no': 'PG0134', 'scheduled_departure': '2017-09-10 09:50:00+03', 'scheduled_arrival': '2017-09-10 14:55:00+03', 'departure_airport': 'DME', 'arrival_airport': 'BTK', 'status': 'Scheduled', 'aircraft_code': '319', 'actual_departure': '\\\\N', 'actual_arrival': '\\\\N'}\n",
      "\n",
      "==================================================\n",
      "Test 2: Query with fetch_all=True\n",
      "Success: True\n",
      "Row count: 9\n",
      "Data: [{'aircraft_code': '773', 'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}', 'range': 11100}, {'aircraft_code': '763', 'model': '{\"en\": \"Boeing 767-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 767-300\"}', 'range': 7900}]\n",
      "\n",
      "==================================================\n",
      "Test 3: Query with custom max_rows\n",
      "Success: True\n",
      "Row count: 3\n",
      "\n",
      "==================================================\n",
      "Test 4: Unsafe query (DROP)\n",
      "Success: False\n",
      "Error: Trying Data Update, Not allowed!!!\n",
      "\n",
      "==================================================\n",
      "Test 5: Multiple statements\n",
      "Success: False\n",
      "Error: Multiple statements or unsafe semicolons detected!\n",
      "\n",
      "==================================================\n",
      "Test 6: Query with semicolon in string (should pass)\n",
      "Success: False\n",
      "Error: 000904 (42000): SQL compilation error: error line 1 at position 46\n",
      "invalid identifier 'FLIGHT_NO'\n",
      "\n",
      "==================================================\n",
      "Test 7: Invalid table name\n",
      "Success: False\n",
      "Error: 002003 (42S02): SQL compilation error:\n",
      "Object 'AIRLINES.AIRLINES.NONEXISTENT_TABLE' does not exist or not authorized.\n",
      "\n",
      "==================================================\n",
      "Test 8: Invalid column name\n",
      "Success: False\n",
      "Error: 000904 (42000): SQL compilation error: error line 1 at position 7\n",
      "invalid identifier 'NONEXISTENT_COLUMN'\n",
      "\n",
      "==================================================\n",
      "Test 9: Empty result set\n",
      "Success: True\n",
      "Row count: 0\n",
      "Data: []\n",
      "\n",
      "==================================================\n",
      "Test 10: Complex JOIN query\n",
      "Success: False\n",
      "Row count: 0\n",
      "First row: None\n"
     ]
    }
   ],
   "source": [
    "# Test cases for execute_query function\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 1: Successful query with default max_rows\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Row count: {result.row_count}\")\n",
    "print(f\"Execution time: {result.execution_time:.4f}s\")\n",
    "print(f\"First row: {result.data[0] if result.data else None}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 2: Query with fetch_all=True\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.AIRCRAFTS_DATA\", fetch_all=True)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Row count: {result.row_count}\")\n",
    "print(f\"Data: {result.data[:2] if result.data else None}\")  # First 2 rows\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 3: Query with custom max_rows\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.AIRPORTS_DATA\", max_rows=3)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Row count: {result.row_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 4: Unsafe query (DROP)\")\n",
    "result = agent.execute_query(\"DROP TABLE AIRLINES.AIRLINES.FLIGHTS\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 5: Multiple statements\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS; DROP TABLE AIRLINES.AIRLINES.FLIGHTS\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 6: Query with semicolon in string (should pass)\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS WHERE flight_no = 'PG;0134'\", max_rows=5)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 7: Invalid table name\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.NONEXISTENT_TABLE\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 8: Invalid column name\")\n",
    "result = agent.execute_query(\"SELECT nonexistent_column FROM AIRLINES.AIRLINES.FLIGHTS\", max_rows=5)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 9: Empty result set\")\n",
    "result = agent.execute_query(\"SELECT * FROM AIRLINES.AIRLINES.FLIGHTS WHERE 1=0\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Row count: {result.row_count}\")\n",
    "print(f\"Data: {result.data}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 10: Complex JOIN query\")\n",
    "result = agent.execute_query(\"\"\"\n",
    "    SELECT f.flight_no, a.airport_name \n",
    "    FROM AIRLINES.AIRLINES.FLIGHTS f\n",
    "    JOIN AIRLINES.AIRLINES.AIRPORTS_DATA a ON f.departure_airport = a.airport_code\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Row count: {result.row_count}\")\n",
    "print(f\"First row: {result.data[0] if result.data else None}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB context or Metadata Extraction\n",
    "\n",
    "The schema context should be:\n",
    "- **Complete** - all info to generate correct SQL\n",
    "- **Compact** - fit in LLM context window\n",
    "- **Clear** - easy structure to parse\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"database\": \"AIRLINES\",\n",
    "  \"schema\": \"AIRLINES\",\n",
    "  \"tables\": [\n",
    "    {\n",
    "      \"name\": \"FLIGHTS\",\n",
    "      \"row_count\": 33121,\n",
    "      \"columns\": [\n",
    "        {\n",
    "          \"name\": \"flight_id\",\n",
    "          \"type\": \"NUMBER(38,0)\",\n",
    "          \"primary_key\": true\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"aircraft_code\",\n",
    "          \"type\": \"VARCHAR\",\n",
    "          \"foreign_key\": {\n",
    "            \"table\": \"AIRCRAFTS_DATA\",\n",
    "            \"column\": \"aircraft_code\"\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"sample_rows\": [\n",
    "        {\"flight_id\": 1185, \"flight_no\": \"PG0134\", ...}\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- Array of tables (easier to iterate)\n",
    "- Column metadata inline (PK/FK flags)\n",
    "- Sample rows show actual data format\n",
    "- Compact types (no need for full precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# indivisual table\n",
    "class TableAttr(BaseModel):\n",
    "    name: str\n",
    "    column_names: Optional[list[dict]] = None\n",
    "    sample_rows: Optional[list[dict]] = None\n",
    "    row_count : int \n",
    "\n",
    "# complete \n",
    "class ParentSchema(BaseModel):\n",
    "    dialect: str\n",
    "    database: str\n",
    "    Schema: str\n",
    "    tables: list[TableAttr]\n",
    "    relationships: Optional[list[dict]] = None  # For foreign key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use this function `execute_query`  to extract all the tabls and Populate for schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = agent.execute_query(f\"SHOW TABLES IN AIRLINES.AIRLINES\", fetch_all=True)\n",
    "assert r.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>kind</th>\n",
       "      <th>comment</th>\n",
       "      <th>cluster_by</th>\n",
       "      <th>rows</th>\n",
       "      <th>bytes</th>\n",
       "      <th>owner</th>\n",
       "      <th>...</th>\n",
       "      <th>search_optimization_progress</th>\n",
       "      <th>search_optimization_bytes</th>\n",
       "      <th>is_external</th>\n",
       "      <th>enable_schema_evolution</th>\n",
       "      <th>owner_role_type</th>\n",
       "      <th>is_event</th>\n",
       "      <th>is_hybrid</th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>is_dynamic</th>\n",
       "      <th>is_immutable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-24 17:19:09.958000-07:00</td>\n",
       "      <td>AIRCRAFTS_DATA</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>2048</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-24 17:13:56.105000-07:00</td>\n",
       "      <td>AIRPORTS_DATA</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>104</td>\n",
       "      <td>11264</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-24 17:20:59.367000-07:00</td>\n",
       "      <td>BOARDING_PASSES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>579686</td>\n",
       "      <td>3896320</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-24 17:21:23.225000-07:00</td>\n",
       "      <td>BOOKINGS</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>262788</td>\n",
       "      <td>3322880</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-24 17:13:31.466000-07:00</td>\n",
       "      <td>FLIGHTS</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>33121</td>\n",
       "      <td>749568</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-10-24 17:19:56.520000-07:00</td>\n",
       "      <td>SEATS</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1339</td>\n",
       "      <td>4608</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-10-24 17:13:45.103000-07:00</td>\n",
       "      <td>TICKETS</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>366733</td>\n",
       "      <td>6424576</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-10-24 17:20:21.125000-07:00</td>\n",
       "      <td>TICKET_FLIGHTS</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>AIRLINES</td>\n",
       "      <td>TABLE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1045726</td>\n",
       "      <td>5586944</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on             name database_name schema_name  \\\n",
       "0 2024-10-24 17:19:09.958000-07:00   AIRCRAFTS_DATA      AIRLINES    AIRLINES   \n",
       "1 2024-10-24 17:13:56.105000-07:00    AIRPORTS_DATA      AIRLINES    AIRLINES   \n",
       "2 2024-10-24 17:20:59.367000-07:00  BOARDING_PASSES      AIRLINES    AIRLINES   \n",
       "3 2024-10-24 17:21:23.225000-07:00         BOOKINGS      AIRLINES    AIRLINES   \n",
       "4 2024-10-24 17:13:31.466000-07:00          FLIGHTS      AIRLINES    AIRLINES   \n",
       "5 2024-10-24 17:19:56.520000-07:00            SEATS      AIRLINES    AIRLINES   \n",
       "6 2024-10-24 17:13:45.103000-07:00          TICKETS      AIRLINES    AIRLINES   \n",
       "7 2024-10-24 17:20:21.125000-07:00   TICKET_FLIGHTS      AIRLINES    AIRLINES   \n",
       "\n",
       "    kind comment cluster_by     rows    bytes         owner  ...  \\\n",
       "0  TABLE                           9     2048  ACCOUNTADMIN  ...   \n",
       "1  TABLE                         104    11264  ACCOUNTADMIN  ...   \n",
       "2  TABLE                      579686  3896320  ACCOUNTADMIN  ...   \n",
       "3  TABLE                      262788  3322880  ACCOUNTADMIN  ...   \n",
       "4  TABLE                       33121   749568  ACCOUNTADMIN  ...   \n",
       "5  TABLE                        1339     4608  ACCOUNTADMIN  ...   \n",
       "6  TABLE                      366733  6424576  ACCOUNTADMIN  ...   \n",
       "7  TABLE                     1045726  5586944  ACCOUNTADMIN  ...   \n",
       "\n",
       "  search_optimization_progress search_optimization_bytes is_external  \\\n",
       "0                         None                      None           N   \n",
       "1                         None                      None           N   \n",
       "2                         None                      None           N   \n",
       "3                         None                      None           N   \n",
       "4                         None                      None           N   \n",
       "5                         None                      None           N   \n",
       "6                         None                      None           N   \n",
       "7                         None                      None           N   \n",
       "\n",
       "  enable_schema_evolution owner_role_type is_event is_hybrid is_iceberg  \\\n",
       "0                       N            ROLE        N         N          N   \n",
       "1                       N            ROLE        N         N          N   \n",
       "2                       N            ROLE        N         N          N   \n",
       "3                       N            ROLE        N         N          N   \n",
       "4                       N            ROLE        N         N          N   \n",
       "5                       N            ROLE        N         N          N   \n",
       "6                       N            ROLE        N         N          N   \n",
       "7                       N            ROLE        N         N          N   \n",
       "\n",
       "  is_dynamic is_immutable  \n",
       "0          N            N  \n",
       "1          N            N  \n",
       "2          N            N  \n",
       "3          N            N  \n",
       "4          N            N  \n",
       "5          N            N  \n",
       "6          N            N  \n",
       "7          N            N  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(r.data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the schema of a table, we can use the following query like\n",
    "\n",
    "```sql\n",
    "desc table table_name;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = agent.execute_query(f\"DESCRIBE TABLE AIRLINES.AIRLINES.{df['name'][0]}\", fetch_all=True)\n",
    "assert r1.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>kind</th>\n",
       "      <th>null?</th>\n",
       "      <th>default</th>\n",
       "      <th>primary key</th>\n",
       "      <th>unique key</th>\n",
       "      <th>check</th>\n",
       "      <th>expression</th>\n",
       "      <th>comment</th>\n",
       "      <th>policy name</th>\n",
       "      <th>privacy domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aircraft_code</td>\n",
       "      <td>VARCHAR(16777216)</td>\n",
       "      <td>COLUMN</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>VARCHAR(16777216)</td>\n",
       "      <td>COLUMN</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>range</td>\n",
       "      <td>NUMBER(38,0)</td>\n",
       "      <td>COLUMN</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name               type    kind null? default primary key  \\\n",
       "0  aircraft_code  VARCHAR(16777216)  COLUMN     Y    None           N   \n",
       "1          model  VARCHAR(16777216)  COLUMN     Y    None           N   \n",
       "2          range       NUMBER(38,0)  COLUMN     Y    None           N   \n",
       "\n",
       "  unique key check expression comment policy name privacy domain  \n",
       "0          N  None       None    None        None           None  \n",
       "1          N  None       None    None        None           None  \n",
       "2          N  None       None    None        None           None  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(r1.data)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                  aircraft_code\n",
      "type              VARCHAR(16777216)\n",
      "kind                         COLUMN\n",
      "null?                             Y\n",
      "default                        None\n",
      "primary key                       N\n",
      "unique key                        N\n",
      "check                          None\n",
      "expression                     None\n",
      "comment                        None\n",
      "policy name                    None\n",
      "privacy domain                 None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df1.iterrows():\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'kind': 'COLUMN', 'null?': 'Y', 'default': None, 'primary key': 'N', 'unique key': 'N', 'check': None, 'expression': None, 'comment': None, 'policy name': None, 'privacy domain': None}\n"
     ]
    }
   ],
   "source": [
    "print(row.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each field from the DESCRIBE TABLE result:\n",
    "\n",
    "- **name**: Column name\n",
    "- **type**: Data type (e.g., VARCHAR, NUMBER)\n",
    "- **kind**: Always \"COLUMN\" for column descriptions\n",
    "- **null?**: Whether NULL values are allowed ('Y' = yes, 'N' = no)\n",
    "- **default**: Default value if column not specified in INSERT\n",
    "- **primary key**: 'Y' if this column is a primary key\n",
    "- **unique key**: 'Y' if this column must have unique values\n",
    "- **check**: Check constraint expression (validation rule)\n",
    "- **expression**: For computed/virtual columns\n",
    "- **comment**: Description/documentation for the column\n",
    "- **policy name**: Data masking/row access policy name\n",
    "- **privacy domain**: Privacy classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'type', 'kind', 'null?', 'default', 'primary key', 'unique key',\n",
       "       'check', 'expression', 'comment', 'policy name', 'privacy domain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_TO_FILTER = ['name', 'type', 'null?', 'default', 'primary key', 'unique key', 'comment', ] #\"check\", 'expression']\n",
    "{k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v and v != 'N'} # for removing 'primary key': 'N' and 'unique key': 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering non null metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'},\n",
       " {'name': 'model', 'type': 'VARCHAR(16777216)', 'null?': 'Y'},\n",
       " {'name': 'range', 'type': 'NUMBER(38,0)', 'null?': 'Y'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = []\n",
    "for _, row in df1.iterrows():\n",
    "    column_names.append({k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v  and v != 'N'})\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single row data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aircraft_code': '773',\n",
       "  'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}',\n",
       "  'range': 11100}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fectch a single row of data from the table\n",
    "table_data_example = agent.execute_query(f\"select * from AIRLINES.AIRLINES.{df['name'][0]} limit 1\", fetch_all=True)\n",
    "assert table_data_example.success\n",
    "assert len(table_data_example.data) == table_data_example.row_count == 1\n",
    "table_data_example.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'COUNT': 9}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fectch a single row of data from the table\n",
    "row_count = agent.execute_query(f\"select count(*)  as count from AIRLINES.AIRLINES.{df['name'][0]}\", fetch_all=True)\n",
    "assert row_count.success\n",
    "row_count.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TableAttr(name='AIRCRAFTS_DATA', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'model', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'range', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '773', 'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}', 'range': 11100}], row_count=9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TableAttr(\n",
    "    name=df['name'][0],\n",
    "    column_names=column_names,\n",
    "    sample_rows=table_data_example.data,\n",
    "    row_count=row_count.data[0]['COUNT']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:07<00:00,  1.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TableAttr(name='AIRCRAFTS_DATA', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'model', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'range', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '773', 'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}', 'range': 11100}], row_count=9),\n",
       " TableAttr(name='AIRPORTS_DATA', column_names=[{'name': 'airport_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'airport_name', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'city', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'coordinates', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'timezone', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'airport_code': 'YKS', 'airport_name': '{\"en\": \"Yakutsk Airport\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}', 'city': '{\"en\": \"Yakutsk\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}', 'coordinates': '(129.77099609375,62.0932998657226562)', 'timezone': 'Asia/Yakutsk'}], row_count=104),\n",
       " TableAttr(name='BOARDING_PASSES', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'boarding_no', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'seat_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005435212351', 'flight_id': 30625, 'boarding_no': 1, 'seat_no': '2D'}], row_count=579686),\n",
       " TableAttr(name='BOOKINGS', column_names=[{'name': 'book_ref', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'book_date', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'total_amount', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'book_ref': '00000F', 'book_date': '2017-07-05 03:12:00+03', 'total_amount': 265700}], row_count=262788),\n",
       " TableAttr(name='FLIGHTS', column_names=[{'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'flight_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'scheduled_departure', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'scheduled_arrival', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'departure_airport', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'arrival_airport', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'status', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'actual_departure', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'actual_arrival', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'flight_id': 1185, 'flight_no': 'PG0134', 'scheduled_departure': '2017-09-10 09:50:00+03', 'scheduled_arrival': '2017-09-10 14:55:00+03', 'departure_airport': 'DME', 'arrival_airport': 'BTK', 'status': 'Scheduled', 'aircraft_code': '319', 'actual_departure': '\\\\N', 'actual_arrival': '\\\\N'}], row_count=33121),\n",
       " TableAttr(name='SEATS', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'seat_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'fare_conditions', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '319', 'seat_no': '2A', 'fare_conditions': 'Business'}], row_count=1339),\n",
       " TableAttr(name='TICKETS', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'book_ref', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'passenger_id', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005432000987', 'book_ref': '06B046', 'passenger_id': '8149 604011'}], row_count=366733),\n",
       " TableAttr(name='TICKET_FLIGHTS', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'fare_conditions', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'amount', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005432159776', 'flight_id': 30625, 'fare_conditions': 'Business', 'amount': 42100}], row_count=1045726)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_table_atrs = []\n",
    "from tqdm import tqdm\n",
    "\n",
    "for tn in tqdm(df['name']):\n",
    "    # get table info\n",
    "    result = agent.execute_query(f\"DESCRIBE TABLE AIRLINES.AIRLINES.{tn}\", fetch_all=True)\n",
    "    assert result.success\n",
    "\n",
    "    table_df = pd.DataFrame(result.data)\n",
    "\n",
    "    # get column related data\n",
    "    column_names = []\n",
    "    for _, row in table_df.iterrows():\n",
    "        column_names.append({k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v  and v != 'N'})\n",
    "    \n",
    "    # fectch a single row of data from the table\n",
    "    table_data_example = agent.execute_query(f\"select * from AIRLINES.AIRLINES.{tn} limit 1\", fetch_all=True)\n",
    "    assert table_data_example.success\n",
    "\n",
    "    # fectch a single row of data from the table\n",
    "    row_count = agent.execute_query(f\"select count(*)  as count from AIRLINES.AIRLINES.{tn}\", fetch_all=True)\n",
    "    assert row_count.success\n",
    "\n",
    "    all_table_atrs.append(\n",
    "        TableAttr(\n",
    "            name=tn,\n",
    "            column_names=column_names,\n",
    "            sample_rows=table_data_example.data,\n",
    "            row_count=row_count.data[0]['COUNT']\n",
    "            ))\n",
    "all_table_atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect='snowflake' database='AIRLINES' Schema='AIRLINES' tables=[TableAttr(name='AIRCRAFTS_DATA', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'model', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'range', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '773', 'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}', 'range': 11100}], row_count=9), TableAttr(name='AIRPORTS_DATA', column_names=[{'name': 'airport_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'airport_name', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'city', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'coordinates', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'timezone', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'airport_code': 'YKS', 'airport_name': '{\"en\": \"Yakutsk Airport\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}', 'city': '{\"en\": \"Yakutsk\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}', 'coordinates': '(129.77099609375,62.0932998657226562)', 'timezone': 'Asia/Yakutsk'}], row_count=104), TableAttr(name='BOARDING_PASSES', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'boarding_no', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'seat_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005435212351', 'flight_id': 30625, 'boarding_no': 1, 'seat_no': '2D'}], row_count=579686), TableAttr(name='BOOKINGS', column_names=[{'name': 'book_ref', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'book_date', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'total_amount', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'book_ref': '00000F', 'book_date': '2017-07-05 03:12:00+03', 'total_amount': 265700}], row_count=262788), TableAttr(name='FLIGHTS', column_names=[{'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'flight_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'scheduled_departure', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'scheduled_arrival', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'departure_airport', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'arrival_airport', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'status', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'actual_departure', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'actual_arrival', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'flight_id': 1185, 'flight_no': 'PG0134', 'scheduled_departure': '2017-09-10 09:50:00+03', 'scheduled_arrival': '2017-09-10 14:55:00+03', 'departure_airport': 'DME', 'arrival_airport': 'BTK', 'status': 'Scheduled', 'aircraft_code': '319', 'actual_departure': '\\\\N', 'actual_arrival': '\\\\N'}], row_count=33121), TableAttr(name='SEATS', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'seat_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'fare_conditions', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '319', 'seat_no': '2A', 'fare_conditions': 'Business'}], row_count=1339), TableAttr(name='TICKETS', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'book_ref', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'passenger_id', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005432000987', 'book_ref': '06B046', 'passenger_id': '8149 604011'}], row_count=366733), TableAttr(name='TICKET_FLIGHTS', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'fare_conditions', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'amount', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005432159776', 'flight_id': 30625, 'fare_conditions': 'Business', 'amount': 42100}], row_count=1045726)] relationships=None\n"
     ]
    }
   ],
   "source": [
    "schema = ParentSchema(\n",
    "    dialect=\"snowflake\",\n",
    "    database=\"AIRLINES\",\n",
    "    Schema=\"AIRLINES\",\n",
    "    tables=all_table_atrs\n",
    ")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.relationships is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`relationships` for populating foreign key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreign key build up\n",
    "As there is no explicit way to find the FK. We will use LLM to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = agent.execute_query(\"\"\"\n",
    "SELECT COLUMN_NAME, TABLE_NAME\n",
    "FROM AIRLINES.INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE TABLE_SCHEMA = 'AIRLINES'\n",
    "ORDER BY COLUMN_NAME, TABLE_NAME\n",
    "\"\"\", fetch_all=True)\n",
    "\n",
    "assert r.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actual_arrival</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actual_departure</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aircraft_code</td>\n",
       "      <td>AIRCRAFTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aircraft_code</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aircraft_code</td>\n",
       "      <td>SEATS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>airport_code</td>\n",
       "      <td>AIRPORTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>airport_name</td>\n",
       "      <td>AIRPORTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amount</td>\n",
       "      <td>TICKET_FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arrival_airport</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>boarding_no</td>\n",
       "      <td>BOARDING_PASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>book_date</td>\n",
       "      <td>BOOKINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>book_ref</td>\n",
       "      <td>BOOKINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>book_ref</td>\n",
       "      <td>TICKETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>city</td>\n",
       "      <td>AIRPORTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>coordinates</td>\n",
       "      <td>AIRPORTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>departure_airport</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fare_conditions</td>\n",
       "      <td>SEATS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fare_conditions</td>\n",
       "      <td>TICKET_FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>flight_id</td>\n",
       "      <td>BOARDING_PASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>flight_id</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>flight_id</td>\n",
       "      <td>TICKET_FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>flight_no</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model</td>\n",
       "      <td>AIRCRAFTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>passenger_id</td>\n",
       "      <td>TICKETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>range</td>\n",
       "      <td>AIRCRAFTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>scheduled_arrival</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>scheduled_departure</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>seat_no</td>\n",
       "      <td>BOARDING_PASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>seat_no</td>\n",
       "      <td>SEATS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>status</td>\n",
       "      <td>FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ticket_no</td>\n",
       "      <td>BOARDING_PASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ticket_no</td>\n",
       "      <td>TICKETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ticket_no</td>\n",
       "      <td>TICKET_FLIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>timezone</td>\n",
       "      <td>AIRPORTS_DATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>total_amount</td>\n",
       "      <td>BOOKINGS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            COLUMN_NAME       TABLE_NAME\n",
       "0        actual_arrival          FLIGHTS\n",
       "1      actual_departure          FLIGHTS\n",
       "2         aircraft_code   AIRCRAFTS_DATA\n",
       "3         aircraft_code          FLIGHTS\n",
       "4         aircraft_code            SEATS\n",
       "5          airport_code    AIRPORTS_DATA\n",
       "6          airport_name    AIRPORTS_DATA\n",
       "7                amount   TICKET_FLIGHTS\n",
       "8       arrival_airport          FLIGHTS\n",
       "9           boarding_no  BOARDING_PASSES\n",
       "10            book_date         BOOKINGS\n",
       "11             book_ref         BOOKINGS\n",
       "12             book_ref          TICKETS\n",
       "13                 city    AIRPORTS_DATA\n",
       "14          coordinates    AIRPORTS_DATA\n",
       "15    departure_airport          FLIGHTS\n",
       "16      fare_conditions            SEATS\n",
       "17      fare_conditions   TICKET_FLIGHTS\n",
       "18            flight_id  BOARDING_PASSES\n",
       "19            flight_id          FLIGHTS\n",
       "20            flight_id   TICKET_FLIGHTS\n",
       "21            flight_no          FLIGHTS\n",
       "22                model   AIRCRAFTS_DATA\n",
       "23         passenger_id          TICKETS\n",
       "24                range   AIRCRAFTS_DATA\n",
       "25    scheduled_arrival          FLIGHTS\n",
       "26  scheduled_departure          FLIGHTS\n",
       "27              seat_no  BOARDING_PASSES\n",
       "28              seat_no            SEATS\n",
       "29               status          FLIGHTS\n",
       "30            ticket_no  BOARDING_PASSES\n",
       "31            ticket_no          TICKETS\n",
       "32            ticket_no   TICKET_FLIGHTS\n",
       "33             timezone    AIRPORTS_DATA\n",
       "34         total_amount         BOOKINGS"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(r.data)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actual_arrival': ['FLIGHTS'],\n",
       " 'actual_departure': ['FLIGHTS'],\n",
       " 'aircraft_code': ['AIRCRAFTS_DATA', 'FLIGHTS', 'SEATS'],\n",
       " 'airport_code': ['AIRPORTS_DATA'],\n",
       " 'airport_name': ['AIRPORTS_DATA'],\n",
       " 'amount': ['TICKET_FLIGHTS'],\n",
       " 'arrival_airport': ['FLIGHTS'],\n",
       " 'boarding_no': ['BOARDING_PASSES'],\n",
       " 'book_date': ['BOOKINGS'],\n",
       " 'book_ref': ['BOOKINGS', 'TICKETS'],\n",
       " 'city': ['AIRPORTS_DATA'],\n",
       " 'coordinates': ['AIRPORTS_DATA'],\n",
       " 'departure_airport': ['FLIGHTS'],\n",
       " 'fare_conditions': ['SEATS', 'TICKET_FLIGHTS'],\n",
       " 'flight_id': ['BOARDING_PASSES', 'FLIGHTS', 'TICKET_FLIGHTS'],\n",
       " 'flight_no': ['FLIGHTS'],\n",
       " 'model': ['AIRCRAFTS_DATA'],\n",
       " 'passenger_id': ['TICKETS'],\n",
       " 'range': ['AIRCRAFTS_DATA'],\n",
       " 'scheduled_arrival': ['FLIGHTS'],\n",
       " 'scheduled_departure': ['FLIGHTS'],\n",
       " 'seat_no': ['BOARDING_PASSES', 'SEATS'],\n",
       " 'status': ['FLIGHTS'],\n",
       " 'ticket_no': ['BOARDING_PASSES', 'TICKETS', 'TICKET_FLIGHTS'],\n",
       " 'timezone': ['AIRPORTS_DATA'],\n",
       " 'total_amount': ['BOOKINGS']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('COLUMN_NAME')['TABLE_NAME'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table': 'AIRCRAFTS_DATA',\n",
       "  'columns': ['aircraft_code', 'model', 'range'],\n",
       "  'row_count': 9,\n",
       "  'sample': {'aircraft_code': '773',\n",
       "   'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}',\n",
       "   'range': 11100}},\n",
       " {'table': 'AIRPORTS_DATA',\n",
       "  'columns': ['airport_code',\n",
       "   'airport_name',\n",
       "   'city',\n",
       "   'coordinates',\n",
       "   'timezone'],\n",
       "  'row_count': 104,\n",
       "  'sample': {'airport_code': 'YKS',\n",
       "   'airport_name': '{\"en\": \"Yakutsk Airport\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}',\n",
       "   'city': '{\"en\": \"Yakutsk\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}',\n",
       "   'coordinates': '(129.77099609375,62.0932998657226562)',\n",
       "   'timezone': 'Asia/Yakutsk'}},\n",
       " {'table': 'BOARDING_PASSES',\n",
       "  'columns': ['ticket_no', 'flight_id', 'boarding_no', 'seat_no'],\n",
       "  'row_count': 579686,\n",
       "  'sample': {'ticket_no': '0005435212351',\n",
       "   'flight_id': 30625,\n",
       "   'boarding_no': 1,\n",
       "   'seat_no': '2D'}},\n",
       " {'table': 'BOOKINGS',\n",
       "  'columns': ['book_ref', 'book_date', 'total_amount'],\n",
       "  'row_count': 262788,\n",
       "  'sample': {'book_ref': '00000F',\n",
       "   'book_date': '2017-07-05 03:12:00+03',\n",
       "   'total_amount': 265700}},\n",
       " {'table': 'FLIGHTS',\n",
       "  'columns': ['flight_id',\n",
       "   'flight_no',\n",
       "   'scheduled_departure',\n",
       "   'scheduled_arrival',\n",
       "   'departure_airport',\n",
       "   'arrival_airport',\n",
       "   'status',\n",
       "   'aircraft_code',\n",
       "   'actual_departure',\n",
       "   'actual_arrival'],\n",
       "  'row_count': 33121,\n",
       "  'sample': {'flight_id': 1185,\n",
       "   'flight_no': 'PG0134',\n",
       "   'scheduled_departure': '2017-09-10 09:50:00+03',\n",
       "   'scheduled_arrival': '2017-09-10 14:55:00+03',\n",
       "   'departure_airport': 'DME',\n",
       "   'arrival_airport': 'BTK',\n",
       "   'status': 'Scheduled',\n",
       "   'aircraft_code': '319',\n",
       "   'actual_departure': '\\\\N',\n",
       "   'actual_arrival': '\\\\N'}},\n",
       " {'table': 'SEATS',\n",
       "  'columns': ['aircraft_code', 'seat_no', 'fare_conditions'],\n",
       "  'row_count': 1339,\n",
       "  'sample': {'aircraft_code': '319',\n",
       "   'seat_no': '2A',\n",
       "   'fare_conditions': 'Business'}},\n",
       " {'table': 'TICKETS',\n",
       "  'columns': ['ticket_no', 'book_ref', 'passenger_id'],\n",
       "  'row_count': 366733,\n",
       "  'sample': {'ticket_no': '0005432000987',\n",
       "   'book_ref': '06B046',\n",
       "   'passenger_id': '8149 604011'}},\n",
       " {'table': 'TICKET_FLIGHTS',\n",
       "  'columns': ['ticket_no', 'flight_id', 'fare_conditions', 'amount'],\n",
       "  'row_count': 1045726,\n",
       "  'sample': {'ticket_no': '0005432159776',\n",
       "   'flight_id': 30625,\n",
       "   'fare_conditions': 'Business',\n",
       "   'amount': 42100}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_summary = []\n",
    "for table in schema.tables:\n",
    "    cols = [col['name'] for col in table.column_names]\n",
    "    schema_summary.append({\n",
    "        \"table\": table.name,\n",
    "        \"columns\": cols,\n",
    "        \"row_count\": table.row_count,\n",
    "        \"sample\": table.sample_rows[0] if table.sample_rows else {}\n",
    "    })\n",
    "schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given this database schema, identify the foreign key relationships.\n",
      "\n",
      "Schema: [\n",
      "  {\n",
      "    \"table\": \"AIRCRAFTS_DATA\",\n",
      "    \"columns\": [\n",
      "      \"aircraft_code\",\n",
      "      \"model\",\n",
      "      \"range\"\n",
      "    ],\n",
      "    \"row_count\": 9,\n",
      "    \"sample\": {\n",
      "      \"aircraft_code\": \"773\",\n",
      "      \"model\": \"{\\\"en\\\": \\\"Boeing 777-300\\\", \\\"ru\\\": \\\"\\u0411\\u043e\\u0438\\u043d\\u0433 777-300\\\"}\",\n",
      "      \"range\": 11100\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"AIRPORTS_DATA\",\n",
      "    \"columns\": [\n",
      "      \"airport_code\",\n",
      "      \"airport_name\",\n",
      "      \"city\",\n",
      "      \"coordinates\",\n",
      "      \"timezone\"\n",
      "    ],\n",
      "    \"row_count\": 104,\n",
      "    \"sample\": {\n",
      "      \"airport_code\": \"YKS\",\n",
      "      \"airport_name\": \"{\\\"en\\\": \\\"Yakutsk Airport\\\", \\\"ru\\\": \\\"\\u042f\\u043a\\u0443\\u0442\\u0441\\u043a\\\"}\",\n",
      "      \"city\": \"{\\\"en\\\": \\\"Yakutsk\\\", \\\"ru\\\": \\\"\\u042f\\u043a\\u0443\\u0442\\u0441\\u043a\\\"}\",\n",
      "      \"coordinates\": \"(129.77099609375,62.0932998657226562)\",\n",
      "      \"timezone\": \"Asia/Yakutsk\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"BOARDING_PASSES\",\n",
      "    \"columns\": [\n",
      "      \"ticket_no\",\n",
      "      \"flight_id\",\n",
      "      \"boarding_no\",\n",
      "      \"seat_no\"\n",
      "    ],\n",
      "    \"row_count\": 579686,\n",
      "    \"sample\": {\n",
      "      \"ticket_no\": \"0005435212351\",\n",
      "      \"flight_id\": 30625,\n",
      "      \"boarding_no\": 1,\n",
      "      \"seat_no\": \"2D\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"BOOKINGS\",\n",
      "    \"columns\": [\n",
      "      \"book_ref\",\n",
      "      \"book_date\",\n",
      "      \"total_amount\"\n",
      "    ],\n",
      "    \"row_count\": 262788,\n",
      "    \"sample\": {\n",
      "      \"book_ref\": \"00000F\",\n",
      "      \"book_date\": \"2017-07-05 03:12:00+03\",\n",
      "      \"total_amount\": 265700\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"FLIGHTS\",\n",
      "    \"columns\": [\n",
      "      \"flight_id\",\n",
      "      \"flight_no\",\n",
      "      \"scheduled_departure\",\n",
      "      \"scheduled_arrival\",\n",
      "      \"departure_airport\",\n",
      "      \"arrival_airport\",\n",
      "      \"status\",\n",
      "      \"aircraft_code\",\n",
      "      \"actual_departure\",\n",
      "      \"actual_arrival\"\n",
      "    ],\n",
      "    \"row_count\": 33121,\n",
      "    \"sample\": {\n",
      "      \"flight_id\": 1185,\n",
      "      \"flight_no\": \"PG0134\",\n",
      "      \"scheduled_departure\": \"2017-09-10 09:50:00+03\",\n",
      "      \"scheduled_arrival\": \"2017-09-10 14:55:00+03\",\n",
      "      \"departure_airport\": \"DME\",\n",
      "      \"arrival_airport\": \"BTK\",\n",
      "      \"status\": \"Scheduled\",\n",
      "      \"aircraft_code\": \"319\",\n",
      "      \"actual_departure\": \"\\\\N\",\n",
      "      \"actual_arrival\": \"\\\\N\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"SEATS\",\n",
      "    \"columns\": [\n",
      "      \"aircraft_code\",\n",
      "      \"seat_no\",\n",
      "      \"fare_conditions\"\n",
      "    ],\n",
      "    \"row_count\": 1339,\n",
      "    \"sample\": {\n",
      "      \"aircraft_code\": \"319\",\n",
      "      \"seat_no\": \"2A\",\n",
      "      \"fare_conditions\": \"Business\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"TICKETS\",\n",
      "    \"columns\": [\n",
      "      \"ticket_no\",\n",
      "      \"book_ref\",\n",
      "      \"passenger_id\"\n",
      "    ],\n",
      "    \"row_count\": 366733,\n",
      "    \"sample\": {\n",
      "      \"ticket_no\": \"0005432000987\",\n",
      "      \"book_ref\": \"06B046\",\n",
      "      \"passenger_id\": \"8149 604011\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"table\": \"TICKET_FLIGHTS\",\n",
      "    \"columns\": [\n",
      "      \"ticket_no\",\n",
      "      \"flight_id\",\n",
      "      \"fare_conditions\",\n",
      "      \"amount\"\n",
      "    ],\n",
      "    \"row_count\": 1045726,\n",
      "    \"sample\": {\n",
      "      \"ticket_no\": \"0005432159776\",\n",
      "      \"flight_id\": 30625,\n",
      "      \"fare_conditions\": \"Business\",\n",
      "      \"amount\": 42100\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "Foreign Keys relations db column name:\n",
      "{\n",
      "  \"actual_arrival\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"actual_departure\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"aircraft_code\": [\n",
      "    \"AIRCRAFTS_DATA\",\n",
      "    \"FLIGHTS\",\n",
      "    \"SEATS\"\n",
      "  ],\n",
      "  \"airport_code\": [\n",
      "    \"AIRPORTS_DATA\"\n",
      "  ],\n",
      "  \"airport_name\": [\n",
      "    \"AIRPORTS_DATA\"\n",
      "  ],\n",
      "  \"amount\": [\n",
      "    \"TICKET_FLIGHTS\"\n",
      "  ],\n",
      "  \"arrival_airport\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"boarding_no\": [\n",
      "    \"BOARDING_PASSES\"\n",
      "  ],\n",
      "  \"book_date\": [\n",
      "    \"BOOKINGS\"\n",
      "  ],\n",
      "  \"book_ref\": [\n",
      "    \"BOOKINGS\",\n",
      "    \"TICKETS\"\n",
      "  ],\n",
      "  \"city\": [\n",
      "    \"AIRPORTS_DATA\"\n",
      "  ],\n",
      "  \"coordinates\": [\n",
      "    \"AIRPORTS_DATA\"\n",
      "  ],\n",
      "  \"departure_airport\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"fare_conditions\": [\n",
      "    \"SEATS\",\n",
      "    \"TICKET_FLIGHTS\"\n",
      "  ],\n",
      "  \"flight_id\": [\n",
      "    \"BOARDING_PASSES\",\n",
      "    \"FLIGHTS\",\n",
      "    \"TICKET_FLIGHTS\"\n",
      "  ],\n",
      "  \"flight_no\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"model\": [\n",
      "    \"AIRCRAFTS_DATA\"\n",
      "  ],\n",
      "  \"passenger_id\": [\n",
      "    \"TICKETS\"\n",
      "  ],\n",
      "  \"range\": [\n",
      "    \"AIRCRAFTS_DATA\"\n",
      "  ],\n",
      "  \"scheduled_arrival\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"scheduled_departure\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"seat_no\": [\n",
      "    \"BOARDING_PASSES\",\n",
      "    \"SEATS\"\n",
      "  ],\n",
      "  \"status\": [\n",
      "    \"FLIGHTS\"\n",
      "  ],\n",
      "  \"ticket_no\": [\n",
      "    \"BOARDING_PASSES\",\n",
      "    \"TICKETS\",\n",
      "    \"TICKET_FLIGHTS\"\n",
      "  ],\n",
      "  \"timezone\": [\n",
      "    \"AIRPORTS_DATA\"\n",
      "  ],\n",
      "  \"total_amount\": [\n",
      "    \"BOOKINGS\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Return ONLY a JSON array of relationships in this exact format:\n",
      "[\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"aircraft_code\",\n",
      "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
      "    \"to_column\": \"aircraft_code\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Rules:\n",
      "- Only include relationships where a column in one table references a primary key in another\n",
      "- Use row counts as hints (parent tables typically have fewer rows)\n",
      "- Consider naming patterns (e.g., aircraft_code likely references AIRCRAFTS_DATA)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Given this database schema, identify the foreign key relationships.\n",
    "\n",
    "Schema: {json.dumps(schema_summary, indent=2)}\n",
    "\n",
    "Foreign Keys relations db column name:\n",
    "{json.dumps(df2.groupby('COLUMN_NAME')['TABLE_NAME'].apply(list).to_dict(), indent=2)}\n",
    "\n",
    "Return ONLY a JSON array of relationships in this exact format:\n",
    "[\n",
    "  {{\n",
    "    \"from_table\": \"FLIGHTS\",\n",
    "    \"from_column\": \"aircraft_code\",\n",
    "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
    "    \"to_column\": \"aircraft_code\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Rules:\n",
    "- Only include relationships where a column in one table references a primary key in another\n",
    "- Use row counts as hints (parent tables typically have fewer rows)\n",
    "- Consider naming patterns (e.g., aircraft_code likely references AIRCRAFTS_DATA)\n",
    "\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[\n",
       "  {\n",
       "    \"from_table\": \"FLIGHTS\",\n",
       "    \"from_column\": \"aircraft_code\",\n",
       "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
       "    \"to_column\": \"aircraft_code\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"FLIGHTS\",\n",
       "    \"from_column\": \"departure_airport\",\n",
       "    \"to_table\": \"AIRPORTS_DATA\",\n",
       "    \"to_column\": \"airport_code\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"FLIGHTS\",\n",
       "    \"from_column\": \"arrival_airport\",\n",
       "    \"to_table\": \"AIRPORTs_DATA\",\n",
       "    \"to_column\": \"airport_code\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"BOARDING_PASSES\",\n",
       "    \"from_column\": \"ticket_no\",\n",
       "    \"to_table\": \"TICKETS\",\n",
       "    \"to_column\": \"ticket_no\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"BOARDING_PASSES\",\n",
       "    \"from_column\": \"flight_id\",\n",
       "    \"to_table\": \"FLIGHTS\",\n",
       "    \"to_column\": \"flight_id\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"TICKET_FLIGHTS\",\n",
       "    \"from_column\": \"ticket_no\",\n",
       "    \"to_table\": \"TICKETS\",\n",
       "    \"to_column\": \"ticket_no\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"TICKET_FLIGHTS\",\n",
       "    \"from_column\": \"flight_id\",\n",
       "    \"to_table\": \"FLIGHTS\",\n",
       "    \"to_column\": \"flight_id\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"TICKETS\",\n",
       "    \"from_column\": \"book_ref\",\n",
       "    \"to_table\": \"BOOKINGS\",\n",
       "    \"to_column\": \"book_ref\"\n",
       "  },\n",
       "  {\n",
       "    \"from_table\": \"SEATS\",\n",
       "    \"from_column\": \"aircraft_code\",\n",
       "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
       "    \"to_column\": \"aircraft_code\"\n",
       "  }\n",
       "]\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-q2c0sog4gfc6liwb8xwp8`\n",
       "- model: `lm_studio/openai/gpt-oss-20b`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=707, prompt_tokens=1559, total_tokens=2266, completion_tokens_details=None, prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-q2c0sog4gfc6liwb8xwp8', created=1763696070, model='lm_studio/openai/gpt-oss-20b', object='chat.completion', system_fingerprint='openai/gpt-oss-20b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[\\n  {\\n    \"from_table\": \"FLIGHTS\",\\n    \"from_column\": \"aircraft_code\",\\n    \"to_table\": \"AIRCRAFTS_DATA\",\\n    \"to_column\": \"aircraft_code\"\\n  },\\n  {\\n    \"from_table\": \"FLIGHTS\",\\n    \"from_column\": \"departure_airport\",\\n    \"to_table\": \"AIRPORTS_DATA\",\\n    \"to_column\": \"airport_code\"\\n  },\\n  {\\n    \"from_table\": \"FLIGHTS\",\\n    \"from_column\": \"arrival_airport\",\\n    \"to_table\": \"AIRPORTs_DATA\",\\n    \"to_column\": \"airport_code\"\\n  },\\n  {\\n    \"from_table\": \"BOARDING_PASSES\",\\n    \"from_column\": \"ticket_no\",\\n    \"to_table\": \"TICKETS\",\\n    \"to_column\": \"ticket_no\"\\n  },\\n  {\\n    \"from_table\": \"BOARDING_PASSES\",\\n    \"from_column\": \"flight_id\",\\n    \"to_table\": \"FLIGHTS\",\\n    \"to_column\": \"flight_id\"\\n  },\\n  {\\n    \"from_table\": \"TICKET_FLIGHTS\",\\n    \"from_column\": \"ticket_no\",\\n    \"to_table\": \"TICKETS\",\\n    \"to_column\": \"ticket_no\"\\n  },\\n  {\\n    \"from_table\": \"TICKET_FLIGHTS\",\\n    \"from_column\": \"flight_id\",\\n    \"to_table\": \"FLIGHTS\",\\n    \"to_column\": \"flight_id\"\\n  },\\n  {\\n    \"from_table\": \"TICKETS\",\\n    \"from_column\": \"book_ref\",\\n    \"to_table\": \"BOOKINGS\",\\n    \"to_column\": \"book_ref\"\\n  },\\n  {\\n    \"from_table\": \"SEATS\",\\n    \"from_column\": \"aircraft_code\",\\n    \"to_table\": \"AIRCRAFTS_DATA\",\\n    \"to_column\": \"aircraft_code\"\\n  }\\n]', role='assistant', tool_calls=None, function_call=None, reasoning_content='We need to output JSON array of FK relationships. Determine which columns reference PKs. Likely: FLIGHTS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code. FLIGHTS.departure_airport, arrival_airport -> AIRPORTS_DATA.airport_code. BOARDING_PASSES.ticket_no -> TICKETS.ticket_no? Actually ticket_no primary in TICKETS. Also BOARDING_PASSES.flight_id -> FLIGHTS.flight_id. TICKET_FLIGHTS.ticket_no -> TICKETS.ticket_no; flight_id -> FLIGHTS.flight_id. TICKETS.book_ref -> BOOKINGS.book_ref. SEATS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code. SEATS.seat_no maybe PK with aircraft_code. Also fare_conditions not FK. So list relationships:\\n\\n1 FLIGHTS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code\\n2 FLIGHTS.departure_airport -> AIRPORTS_DATA.airport_code\\n3 FLIGHTS.arrival_airport -> AIRPORTS_DATA.airport_code\\n4 BOARDING_PASSES.ticket_no -> TICKETS.ticket_no\\n5 BOARDING_PASSES.flight_id -> FLIGHTS.flight_id\\n6 TICKET_FLIGHTS.ticket_no -> TICKETS.ticket_no\\n7 TICKET_FLIGHTS.flight_id -> FLIGHTS.flight_id\\n8 TICKETS.book_ref -> BOOKINGS.book_ref\\n9 SEATS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code\\n\\nAlso maybe SEATS.seat_no with aircraft_code composite PK but not FK. So output array of these objects.\\n\\n', provider_specific_fields={'refusal': None, 'reasoning': 'We need to output JSON array of FK relationships. Determine which columns reference PKs. Likely: FLIGHTS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code. FLIGHTS.departure_airport, arrival_airport -> AIRPORTS_DATA.airport_code. BOARDING_PASSES.ticket_no -> TICKETS.ticket_no? Actually ticket_no primary in TICKETS. Also BOARDING_PASSES.flight_id -> FLIGHTS.flight_id. TICKET_FLIGHTS.ticket_no -> TICKETS.ticket_no; flight_id -> FLIGHTS.flight_id. TICKETS.book_ref -> BOOKINGS.book_ref. SEATS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code. SEATS.seat_no maybe PK with aircraft_code. Also fare_conditions not FK. So list relationships:\\n\\n1 FLIGHTS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code\\n2 FLIGHTS.departure_airport -> AIRPORTS_DATA.airport_code\\n3 FLIGHTS.arrival_airport -> AIRPORTS_DATA.airport_code\\n4 BOARDING_PASSES.ticket_no -> TICKETS.ticket_no\\n5 BOARDING_PASSES.flight_id -> FLIGHTS.flight_id\\n6 TICKET_FLIGHTS.ticket_no -> TICKETS.ticket_no\\n7 TICKET_FLIGHTS.flight_id -> FLIGHTS.flight_id\\n8 TICKETS.book_ref -> BOOKINGS.book_ref\\n9 SEATS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code\\n\\nAlso maybe SEATS.seat_no with aircraft_code composite PK but not FK. So output array of these objects.\\n\\n', 'reasoning_content': 'We need to output JSON array of FK relationships. Determine which columns reference PKs. Likely: FLIGHTS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code. FLIGHTS.departure_airport, arrival_airport -> AIRPORTS_DATA.airport_code. BOARDING_PASSES.ticket_no -> TICKETS.ticket_no? Actually ticket_no primary in TICKETS. Also BOARDING_PASSES.flight_id -> FLIGHTS.flight_id. TICKET_FLIGHTS.ticket_no -> TICKETS.ticket_no; flight_id -> FLIGHTS.flight_id. TICKETS.book_ref -> BOOKINGS.book_ref. SEATS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code. SEATS.seat_no maybe PK with aircraft_code. Also fare_conditions not FK. So list relationships:\\n\\n1 FLIGHTS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code\\n2 FLIGHTS.departure_airport -> AIRPORTS_DATA.airport_code\\n3 FLIGHTS.arrival_airport -> AIRPORTS_DATA.airport_code\\n4 BOARDING_PASSES.ticket_no -> TICKETS.ticket_no\\n5 BOARDING_PASSES.flight_id -> FLIGHTS.flight_id\\n6 TICKET_FLIGHTS.ticket_no -> TICKETS.ticket_no\\n7 TICKET_FLIGHTS.flight_id -> FLIGHTS.flight_id\\n8 TICKETS.book_ref -> BOOKINGS.book_ref\\n9 SEATS.aircraft_code -> AIRCRAFTS_DATA.aircraft_code\\n\\nAlso maybe SEATS.seat_no with aircraft_code composite PK but not FK. So output array of these objects.\\n\\n'}), provider_specific_fields={})], usage=Usage(completion_tokens=707, prompt_tokens=1559, total_tokens=2266, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, stats={})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model_name)\n",
    "resp = chat(prompt)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"aircraft_code\",\n",
      "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
      "    \"to_column\": \"aircraft_code\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"departure_airport\",\n",
      "    \"to_table\": \"AIRPORTS_DATA\",\n",
      "    \"to_column\": \"airport_code\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"arrival_airport\",\n",
      "    \"to_table\": \"AIRPORTs_DATA\",\n",
      "    \"to_column\": \"airport_code\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"BOARDING_PASSES\",\n",
      "    \"from_column\": \"ticket_no\",\n",
      "    \"to_table\": \"TICKETS\",\n",
      "    \"to_column\": \"ticket_no\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"BOARDING_PASSES\",\n",
      "    \"from_column\": \"flight_id\",\n",
      "    \"to_table\": \"FLIGHTS\",\n",
      "    \"to_column\": \"flight_id\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"TICKET_FLIGHTS\",\n",
      "    \"from_column\": \"ticket_no\",\n",
      "    \"to_table\": \"TICKETS\",\n",
      "    \"to_column\": \"ticket_no\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"TICKET_FLIGHTS\",\n",
      "    \"from_column\": \"flight_id\",\n",
      "    \"to_table\": \"FLIGHTS\",\n",
      "    \"to_column\": \"flight_id\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"TICKETS\",\n",
      "    \"from_column\": \"book_ref\",\n",
      "    \"to_table\": \"BOOKINGS\",\n",
      "    \"to_column\": \"book_ref\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"SEATS\",\n",
      "    \"from_column\": \"aircraft_code\",\n",
      "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
      "    \"to_column\": \"aircraft_code\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"aircraft_code\",\n",
      "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
      "    \"to_column\": \"aircraft_code\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"departure_airport\",\n",
      "    \"to_table\": \"AIRPORTS_DATA\",\n",
      "    \"to_column\": \"airport_code\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"FLIGHTS\",\n",
      "    \"from_column\": \"arrival_airport\",\n",
      "    \"to_table\": \"AIRPORTs_DATA\",\n",
      "    \"to_column\": \"airport_code\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"BOARDING_PASSES\",\n",
      "    \"from_column\": \"ticket_no\",\n",
      "    \"to_table\": \"TICKETS\",\n",
      "    \"to_column\": \"ticket_no\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"BOARDING_PASSES\",\n",
      "    \"from_column\": \"flight_id\",\n",
      "    \"to_table\": \"FLIGHTS\",\n",
      "    \"to_column\": \"flight_id\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"TICKET_FLIGHTS\",\n",
      "    \"from_column\": \"ticket_no\",\n",
      "    \"to_table\": \"TICKETS\",\n",
      "    \"to_column\": \"ticket_no\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"TICKET_FLIGHTS\",\n",
      "    \"from_column\": \"flight_id\",\n",
      "    \"to_table\": \"FLIGHTS\",\n",
      "    \"to_column\": \"flight_id\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"TICKETS\",\n",
      "    \"from_column\": \"book_ref\",\n",
      "    \"to_table\": \"BOOKINGS\",\n",
      "    \"to_column\": \"book_ref\"\n",
      "  },\n",
      "  {\n",
      "    \"from_table\": \"SEATS\",\n",
      "    \"from_column\": \"aircraft_code\",\n",
      "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
      "    \"to_column\": \"aircraft_code\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "msg = resp.choices[0].message.content.replace('```json', '').replace('```', '')\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.relationships = json.loads(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foreign key relationships are not visible in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = agent.execute_query(\"\"\"SELECT \n",
    "  fk.table_name as from_table,\n",
    "  fk.column_name as from_column,\n",
    "  pk.table_name as to_table,\n",
    "  pk.column_name as to_column\n",
    "FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS rc\n",
    "JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE fk \n",
    "  ON rc.constraint_name = fk.constraint_name\n",
    "JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE pk\n",
    "  ON rc.unique_constraint_name = pk.constraint_name\n",
    "WHERE fk.table_schema = 'AIRLINES'\n",
    "\"\"\")\n",
    "r.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f18a4d25310>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_fk(agent, schema: ParentSchema, model_name: str =\"gemini/gemini-2.5-flash\") -> ParentSchema:\n",
    "    try:\n",
    "        # Query for column information\n",
    "        # before any heavy query\n",
    "        agent.cursor.execute(\"ALTER SESSION SET STATEMENT_TIMEOUT_IN_SECONDS = 600\")\n",
    "        r = agent.execute_query(f\"\"\"\n",
    "SELECT COLUMN_NAME, TABLE_NAME\n",
    "FROM {schema.database}.INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE TABLE_SCHEMA = '{schema.Schema}'\n",
    "ORDER BY COLUMN_NAME, TABLE_NAME\n",
    "\"\"\", fetch_all=True)\n",
    "        \n",
    "        if not r.success:\n",
    "            raise Exception(f\"Failed to query schema: {r.error}\")\n",
    "        \n",
    "        df = pd.DataFrame(r.data)\n",
    "        \n",
    "        # Build schema summary\n",
    "        schema_summary = []\n",
    "        for table in schema.tables:\n",
    "            cols = [col['name'] for col in table.column_names]\n",
    "            schema_summary.append({\n",
    "                \"table\": table.name,\n",
    "                \"columns\": cols,\n",
    "                \"row_count\": table.row_count,\n",
    "                \"sample\": table.sample_rows[0] if table.sample_rows else {}\n",
    "            })\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"Given this database schema, identify the foreign key relationships.\n",
    "\n",
    "Schema: {json.dumps(schema_summary, indent=2)}\n",
    "\n",
    "Foreign Keys relations db column name:\n",
    "{json.dumps(df.groupby('COLUMN_NAME')['TABLE_NAME'].apply(list).to_dict(), indent=2)}\n",
    "\n",
    "Return ONLY a JSON array of relationships in this exact format:\n",
    "[\n",
    "  {{\n",
    "    \"from_table\": \"FLIGHTS\",\n",
    "    \"from_column\": \"aircraft_code\",\n",
    "    \"to_table\": \"AIRCRAFTS_DATA\",\n",
    "    \"to_column\": \"aircraft_code\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Rules:\n",
    "- Only include relationships where a column in one table references a primary key in another\n",
    "- Use row counts as hints (parent tables typically have fewer rows)\n",
    "- Consider naming patterns (e.g., aircraft_code likely references AIRCRAFTS_DATA)\n",
    "\"\"\"\n",
    "        \n",
    "        # Call LLM\n",
    "        chat = Chat(model_name)\n",
    "        resp = chat(prompt)\n",
    "        content = resp.choices[0].message.content\n",
    "        \n",
    "        # Extract JSON more robustly\n",
    "        # Try to find JSON array in the response\n",
    "        json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"Could not find JSON array in LLM response\")\n",
    "        \n",
    "        json_str = json_match.group(0)\n",
    "        relationships = json.loads(json_str)\n",
    "        \n",
    "        # Validate it's a list\n",
    "        if not isinstance(relationships, list):\n",
    "            raise ValueError(\"Expected list of relationships from LLM\")\n",
    "        \n",
    "        schema.relationships = relationships\n",
    "        return schema\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        raise Exception(f\"Failed to parse LLM response as JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error inferring foreign keys: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParentSchema(dialect='snowflake', database='AIRLINES', Schema='AIRLINES', tables=[TableAttr(name='AIRCRAFTS_DATA', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'model', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'range', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '773', 'model': '{\"en\": \"Boeing 777-300\", \"ru\": \"Ð‘Ð¾Ð¸Ð½Ð³ 777-300\"}', 'range': 11100}], row_count=9), TableAttr(name='AIRPORTS_DATA', column_names=[{'name': 'airport_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'airport_name', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'city', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'coordinates', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'timezone', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'airport_code': 'YKS', 'airport_name': '{\"en\": \"Yakutsk Airport\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}', 'city': '{\"en\": \"Yakutsk\", \"ru\": \"Ð¯ÐºÑƒÑ‚ÑÐº\"}', 'coordinates': '(129.77099609375,62.0932998657226562)', 'timezone': 'Asia/Yakutsk'}], row_count=104), TableAttr(name='BOARDING_PASSES', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'boarding_no', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'seat_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005435212351', 'flight_id': 30625, 'boarding_no': 1, 'seat_no': '2D'}], row_count=579686), TableAttr(name='BOOKINGS', column_names=[{'name': 'book_ref', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'book_date', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'total_amount', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'book_ref': '00000F', 'book_date': '2017-07-05 03:12:00+03', 'total_amount': 265700}], row_count=262788), TableAttr(name='FLIGHTS', column_names=[{'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'flight_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'scheduled_departure', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'scheduled_arrival', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'departure_airport', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'arrival_airport', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'status', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'actual_departure', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'actual_arrival', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'flight_id': 1185, 'flight_no': 'PG0134', 'scheduled_departure': '2017-09-10 09:50:00+03', 'scheduled_arrival': '2017-09-10 14:55:00+03', 'departure_airport': 'DME', 'arrival_airport': 'BTK', 'status': 'Scheduled', 'aircraft_code': '319', 'actual_departure': '\\\\N', 'actual_arrival': '\\\\N'}], row_count=33121), TableAttr(name='SEATS', column_names=[{'name': 'aircraft_code', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'seat_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'fare_conditions', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'aircraft_code': '319', 'seat_no': '2A', 'fare_conditions': 'Business'}], row_count=1339), TableAttr(name='TICKETS', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'book_ref', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'passenger_id', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005432000987', 'book_ref': '06B046', 'passenger_id': '8149 604011'}], row_count=366733), TableAttr(name='TICKET_FLIGHTS', column_names=[{'name': 'ticket_no', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'flight_id', 'type': 'NUMBER(38,0)', 'null?': 'Y'}, {'name': 'fare_conditions', 'type': 'VARCHAR(16777216)', 'null?': 'Y'}, {'name': 'amount', 'type': 'NUMBER(38,0)', 'null?': 'Y'}], sample_rows=[{'ticket_no': '0005432159776', 'flight_id': 30625, 'fare_conditions': 'Business', 'amount': 42100}], row_count=1045726)], relationships=[{'from_table': 'FLIGHTS', 'from_column': 'aircraft_code', 'to_table': 'AIRCRAFTS_DATA', 'to_column': 'aircraft_code'}, {'from_table': 'FLIGHTS', 'from_column': 'departure_airport', 'to_table': 'AIRPORTS_DATA', 'to_column': 'airport_code'}, {'from_table': 'FLIGHTS', 'from_column': 'arrival_airport', 'to_table': 'AIRPORTs_DATA', 'to_column': 'airport_code'}, {'from_table': 'BOARDING_PASSES', 'from_column': 'ticket_no', 'to_table': 'TICKETS', 'to_column': 'ticket_no'}, {'from_table': 'BOARDING_PASSES', 'from_column': 'flight_id', 'to_table': 'FLIGHTS', 'to_column': 'flight_id'}, {'from_table': 'TICKET_FLIGHTS', 'from_column': 'ticket_no', 'to_table': 'TICKETS', 'to_column': 'ticket_no'}, {'from_table': 'TICKET_FLIGHTS', 'from_column': 'flight_id', 'to_table': 'FLIGHTS', 'to_column': 'flight_id'}, {'from_table': 'TICKETS', 'from_column': 'book_ref', 'to_table': 'BOOKINGS', 'to_column': 'book_ref'}, {'from_table': 'SEATS', 'from_column': 'aircraft_code', 'to_table': 'AIRCRAFTS_DATA', 'to_column': 'aircraft_code'}])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_s = get_fk(agent, schema, model_name)\n",
    "n_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Schema Build UP\n",
    "Wrapping up all the above in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "FIELD_TO_FILTER = ['name', 'type', 'null?', 'default', 'primary key', 'unique key', 'comment', ] #\"check\", 'expression']\n",
    "\n",
    "class SnowMetadata:\n",
    "    def __init__(self, agent, db_name: str, schema_name: str, row_limit: int = 1, model_name:str = \"gemini/gemini-2.5-flash\"):\n",
    "        self.agent = agent\n",
    "        self.db_name = db_name\n",
    "        self.schema_name = schema_name\n",
    "        self.row_limit = row_limit\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def _get_all_tables_metadata(self):\n",
    "        if self.row_limit <= 0: self.row_limit = 1 # if negative row count is provided\n",
    "\n",
    "        result = self.agent.execute_query(f\"SHOW TABLES IN {self.db_name}.{self.schema_name}\", fetch_all=True)\n",
    "        assert result.success, f\"Not able to fetch the `SHOW TABLES IN {self.db_name}.{self.schema_name}`\"\n",
    "\n",
    "        if len(result.data) == 0 :\n",
    "            raise ValueError(\"Empty schema\")\n",
    "\n",
    "        all_tables_df = pd.DataFrame(result.data)\n",
    "\n",
    "        print(\"Reading Tables....\")\n",
    "        all_table_atrs = []\n",
    "\n",
    "        for tn in tqdm(all_tables_df['name']):\n",
    "            # get table info\n",
    "            tn = f\"{self.db_name}.{self.schema_name}.{tn}\"\n",
    "            result = self.agent.execute_query(f\"DESCRIBE TABLE {tn}\", fetch_all=True)\n",
    "            assert result.success, f\"Not able to fetch the `SHOW TABLES IN {tn}`\"\n",
    "\n",
    "            table_df = pd.DataFrame(result.data)\n",
    "\n",
    "            # get column related data\n",
    "            # this needs to be updated for the new db setup ie oracle and others\n",
    "            column_names = []\n",
    "            for _, row in table_df.iterrows():\n",
    "                column_names.append({k:v for k, v in row.to_dict().items() if k in FIELD_TO_FILTER and v  and v != 'N'})\n",
    "            \n",
    "            # fectch a single row of data from the table\n",
    "            table_data_example = self.agent.execute_query(f\"select * from {tn} limit {self.row_limit}\", fetch_all=True)\n",
    "            assert table_data_example.success\n",
    "\n",
    "            # fectch a single row of data from the table\n",
    "            row_count = self.agent.execute_query(f\"select count(*)  as count from {tn}\", fetch_all=True)\n",
    "            assert row_count.success\n",
    "\n",
    "            all_table_atrs.append(\n",
    "                TableAttr(\n",
    "                    name=tn,\n",
    "                    column_names=column_names,\n",
    "                    sample_rows=table_data_example.data,\n",
    "                    row_count=next(iter(row_count.data[0].values())) # for ignoring the key if upper or lower type\n",
    "                    ))\n",
    "\n",
    "        self.schema_metadata =  ParentSchema(\n",
    "            dialect=\"snowflake\", # for now it is fixed to snowflake\n",
    "            database=self.db_name,\n",
    "            Schema=self.schema_name,\n",
    "            tables=all_table_atrs\n",
    "        )\n",
    "\n",
    "    def _get_fk_metadata(self):\n",
    "        print(\"Building up FKs....\")\n",
    "        new_s = get_fk(self.agent, self.schema_metadata, self.model_name)\n",
    "        \n",
    "        \n",
    "    def get_metadata(self):\n",
    "        self._get_all_tables_metadata()\n",
    "        self._get_fk_metadata()\n",
    "        return self.schema_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Tables....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building up FKs....\n"
     ]
    }
   ],
   "source": [
    "M = SnowMetadata(\n",
    "    agent,\n",
    "    \"AIRLINES\", \"AIRLINES\", model_name = model_name)\n",
    "sn_m = M.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sn_m.dialect == \"snowflake\"\n",
    "assert type(sn_m.database) == str\n",
    "assert type(sn_m.Schema) == str\n",
    "assert len(sn_m.tables) > 0\n",
    "assert len(sn_m.relationships) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
